<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>My new cool app</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/4ac7ed34d61ef456.css" as="style"/><link rel="stylesheet" href="/_next/static/css/4ac7ed34d61ef456.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-d38be8d96a62f950.js" defer=""></script><script src="/_next/static/chunks/framework-3b5a00d5d7e8d93b.js" defer=""></script><script src="/_next/static/chunks/main-9f90a364d66949ce.js" defer=""></script><script src="/_next/static/chunks/pages/_app-8cfbd06f8ddd0b49.js" defer=""></script><script src="/_next/static/chunks/675-262430aa11afdf01.js" defer=""></script><script src="/_next/static/chunks/9-9d15d34c7affe00b.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-ba4a3f289f8ef3c5.js" defer=""></script><script src="/_next/static/GAVvi42gDEHMzYTCLyMnv/_buildManifest.js" defer=""></script><script src="/_next/static/GAVvi42gDEHMzYTCLyMnv/_ssgManifest.js" defer=""></script><style data-styled="" data-styled-version="5.3.6">.iBQlkL{background:coral;height:60px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media (max-width:920px){.iBQlkL ul{display:none;}}/*!sc*/
data-styled.g1[id="sc-7295e4c-0"]{content:"iBQlkL,"}/*!sc*/
.bDMZDz{display:none;font-size:22px;margin-left:auto;}/*!sc*/
@media (max-width:920px){.bDMZDz{display:block;}}/*!sc*/
data-styled.g2[id="sc-7295e4c-1"]{content:"bDMZDz,"}/*!sc*/
html,body{height:100%;margin:0;}/*!sc*/
body{background:#fff;}/*!sc*/
html{box-sizing:border-box;font-size:16px;}/*!sc*/
*,*:before,*:after{box-sizing:inherit;}/*!sc*/
body,h1,h2,h3,h4,h5,h6,p,ol,ul{margin:0;padding:0;font-weight:normal;}/*!sc*/
ol,ul{list-style:none;}/*!sc*/
img,video{max-width:100%;height:auto;}/*!sc*/
.lg-container .lg-backdrop.in{opacity:0.75;}/*!sc*/
.lg-container .lg-toolbar.lg-group,.lg-container .lg-outer .lg-thumb-outer{background:rgba(0,0,0,0.45);}/*!sc*/
data-styled.g3[id="sc-global-cqyaiF1"]{content:"sc-global-cqyaiF1,"}/*!sc*/
.cGDPDa{max-width:800px;margin:0 auto;}/*!sc*/
data-styled.g6[id="sc-f0e62130-0"]{content:"cGDPDa,"}/*!sc*/
</style></head><body><div id="__next"><nav class="sc-7295e4c-0 iBQlkL"><ul><a href="/blog">Blog</a></ul><button class="sc-7295e4c-1 bDMZDz">X</button></nav><main><div class="sc-f0e62130-0 cGDPDa"><h1>Visualizing Gradient Descent</h1><div class="lg-react-element "><p><em>This is a guest blog from the Google Summer of Code project.</em></p>
<p>Gradient Descent was implemented as a part of my <a href="https://developers.google.com/open-source/gsoc/">Google Summer of Code</a> project and it is available in the <a href="https://github.com/biolab/orange3-educational">Orange3-Educational</a> add-on. It simulates <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a> for either <a href="https://en.wikipedia.org/wiki/Logistic_regression">Logistic</a> or <a href="https://en.wikipedia.org/wiki/Linear_regression">Linear</a> regression, depending on the type of the input data. Gradient descent is iterative approach to optimize model parameters that minimize the cost function. In machine learning, the cost function corresponds to prediction error when the model is used on the training data set.</p>
<p>Gradient Descent widget takes <em>data</em> on input and outputs the <em>model</em> and its <em>coefficients</em>.</p>
<a href="/blog/2016-08-visualizing-gradient-descent/__webp-images__/gradient-descent-flow.webp" data-gallery="true"><img alt="" srcSet="/blog/2016-08-visualizing-gradient-descent/__webp-images__/gradient-descent-flow.webp 1x, /blog/2016-08-visualizing-gradient-descent/__webp-images__/gradient-descent-flow.webp 2x" src="/blog/2016-08-visualizing-gradient-descent/__webp-images__/gradient-descent-flow.webp" width="463" height="286" decoding="async" data-nimg="1" loading="lazy" style="color:transparent"/></a>
<p>The widget displays the value of the cost function given two parameters of the model. For linear regression, we consider feature from the training set with the parameters being the intercept and the slope. For logistic regression, the widget considers two feature and their associated multiplicative parameters, setting the intercept to zero. Screenshot bellow shows gradient descent on a <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">Iris data set</a>, where we consider petal length and sepal width on the input and predict the probability that iris comes from the family of Iris versicolor.</p>
<a href="/blog/2016-08-visualizing-gradient-descent/__webp-images__/gradient-descent1-stamped.webp" data-gallery="true"><img alt="" srcSet="/blog/2016-08-visualizing-gradient-descent/__webp-images__/gradient-descent1-stamped.webp 1x, /blog/2016-08-visualizing-gradient-descent/__webp-images__/gradient-descent1-stamped.webp 2x" src="/blog/2016-08-visualizing-gradient-descent/__webp-images__/gradient-descent1-stamped.webp" width="963" height="686" decoding="async" data-nimg="1" loading="lazy" style="color:transparent"/></a>
<ol>
<li>The type of the model used (either <em>Logistic regression</em> or <em>Linear regression</em>)</li>
<li>Input features (one for X and one for Y axis) and the target class</li>
<li>Learning rate is the step size of the gradient descent</li>
<li>In a single iteration step, stochastic approach considers only a single data instance (instead of entire training set). Convergence in terms of iterations steps is slower, and we can instruct the widget to display the progress of optimization only after given number of steps (<em>Step size</em>)</li>
<li>Step through the algorithm (steps can be reverted with <em>step back</em> button)</li>
<li>Run optimization until convergence</li>
</ol>
<p>Following shows gradient descent for linear regression using <a href="https://archive.ics.uci.edu/ml/datasets/Housing">The Boston Housing Data Set</a> when trying to predict the median value of a house given its age.</p>
<a href="/blog/2016-08-visualizing-gradient-descent/__webp-images__/gradient-descent-age.webp" data-gallery="true"><img alt="" srcSet="/blog/2016-08-visualizing-gradient-descent/__webp-images__/gradient-descent-age.webp 1x, /blog/2016-08-visualizing-gradient-descent/__webp-images__/gradient-descent-age.webp 2x" src="/blog/2016-08-visualizing-gradient-descent/__webp-images__/gradient-descent-age.webp" width="1950" height="717" decoding="async" data-nimg="1" loading="lazy" style="color:transparent"/></a>
<p>On the left we use the <a href="https://en.wikipedia.org/wiki/Gradient_descent">regular</a> and on the right the <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">stochastic</a> gradient descent. While the regular descent goes straight to the target, the path of stochastic is not as smooth.</p>
<p>We can use the widget to simulate some dangerous, unwanted behavior of gradient descent. The following screenshots show two extreme cases with too high learning rate where optimization function never converges, and a low learning rate where convergence is painfully slow.</p>
<a href="/blog/2016-08-visualizing-gradient-descent/__webp-images__/gradient-descent-extrems.webp" data-gallery="true"><img alt="" srcSet="/blog/2016-08-visualizing-gradient-descent/__webp-images__/gradient-descent-extrems.webp 1x, /blog/2016-08-visualizing-gradient-descent/__webp-images__/gradient-descent-extrems.webp 2x" src="/blog/2016-08-visualizing-gradient-descent/__webp-images__/gradient-descent-extrems.webp" width="1950" height="717" decoding="async" data-nimg="1" loading="lazy" style="color:transparent"/></a>
<p>The two problems as illustrated above are the reason that many implementations of numerical optimization use adaptive learning rates. We can simulate this in the widget by modifying the learning rate for each step of the optimization.</p></div></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"author":"PRIMOZGODEC","date":"2016-08-25 11:31:04+00:00","draft":false,"title":"Visualizing Gradient Descent","type":"blog","blog":["addons","education","gsoc2016","interactive data visualization","orange3"]},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    em: \"em\",\n    a: \"a\",\n    img: \"img\",\n    ol: \"ol\",\n    li: \"li\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"This is a guest blog from the Google Summer of Code project.\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Gradient Descent was implemented as a part of my \", _jsx(_components.a, {\n        href: \"https://developers.google.com/open-source/gsoc/\",\n        children: \"Google Summer of Code\"\n      }), \" project and it is available in the \", _jsx(_components.a, {\n        href: \"https://github.com/biolab/orange3-educational\",\n        children: \"Orange3-Educational\"\n      }), \" add-on. It simulates \", _jsx(_components.a, {\n        href: \"https://en.wikipedia.org/wiki/Gradient_descent\",\n        children: \"gradient descent\"\n      }), \" for either \", _jsx(_components.a, {\n        href: \"https://en.wikipedia.org/wiki/Logistic_regression\",\n        children: \"Logistic\"\n      }), \" or \", _jsx(_components.a, {\n        href: \"https://en.wikipedia.org/wiki/Linear_regression\",\n        children: \"Linear\"\n      }), \" regression, depending on the type of the input data. Gradient descent is iterative approach to optimize model parameters that minimize the cost function. In machine learning, the cost function corresponds to prediction error when the model is used on the training data set.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Gradient Descent widget takes \", _jsx(_components.em, {\n        children: \"data\"\n      }), \" on input and outputs the \", _jsx(_components.em, {\n        children: \"model\"\n      }), \" and its \", _jsx(_components.em, {\n        children: \"coefficients\"\n      }), \".\"]\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/blog/2016-08-visualizing-gradient-descent/__webp-images__/gradient-descent-flow.webp\",\n      alt: \"\",\n      width: \"463\",\n      height: \"286\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"The widget displays the value of the cost function given two parameters of the model. For linear regression, we consider feature from the training set with the parameters being the intercept and the slope. For logistic regression, the widget considers two feature and their associated multiplicative parameters, setting the intercept to zero. Screenshot bellow shows gradient descent on a \", _jsx(_components.a, {\n        href: \"https://en.wikipedia.org/wiki/Iris_flower_data_set\",\n        children: \"Iris data set\"\n      }), \", where we consider petal length and sepal width on the input and predict the probability that iris comes from the family of Iris versicolor.\"]\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/blog/2016-08-visualizing-gradient-descent/__webp-images__/gradient-descent1-stamped.webp\",\n      alt: \"\",\n      width: \"963\",\n      height: \"686\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"The type of the model used (either \", _jsx(_components.em, {\n          children: \"Logistic regression\"\n        }), \" or \", _jsx(_components.em, {\n          children: \"Linear regression\"\n        }), \")\"]\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Input features (one for X and one for Y axis) and the target class\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Learning rate is the step size of the gradient descent\"\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"In a single iteration step, stochastic approach considers only a single data instance (instead of entire training set). Convergence in terms of iterations steps is slower, and we can instruct the widget to display the progress of optimization only after given number of steps (\", _jsx(_components.em, {\n          children: \"Step size\"\n        }), \")\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"Step through the algorithm (steps can be reverted with \", _jsx(_components.em, {\n          children: \"step back\"\n        }), \" button)\"]\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Run optimization until convergence\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Following shows gradient descent for linear regression using \", _jsx(_components.a, {\n        href: \"https://archive.ics.uci.edu/ml/datasets/Housing\",\n        children: \"The Boston Housing Data Set\"\n      }), \" when trying to predict the median value of a house given its age.\"]\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/blog/2016-08-visualizing-gradient-descent/__webp-images__/gradient-descent-age.webp\",\n      alt: \"\",\n      width: \"1950\",\n      height: \"717\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"On the left we use the \", _jsx(_components.a, {\n        href: \"https://en.wikipedia.org/wiki/Gradient_descent\",\n        children: \"regular\"\n      }), \" and on the right the \", _jsx(_components.a, {\n        href: \"https://en.wikipedia.org/wiki/Stochastic_gradient_descent\",\n        children: \"stochastic\"\n      }), \" gradient descent. While the regular descent goes straight to the target, the path of stochastic is not as smooth.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We can use the widget to simulate some dangerous, unwanted behavior of gradient descent. The following screenshots show two extreme cases with too high learning rate where optimization function never converges, and a low learning rate where convergence is painfully slow.\"\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/blog/2016-08-visualizing-gradient-descent/__webp-images__/gradient-descent-extrems.webp\",\n      alt: \"\",\n      width: \"1950\",\n      height: \"717\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The two problems as illustrated above are the reason that many implementations of numerical optimization use adaptive learning rates. We can simulate this in the widget by modifying the learning rate for each step of the optimization.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"visualizing-gradient-descent"},"buildId":"GAVvi42gDEHMzYTCLyMnv","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>