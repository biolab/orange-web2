<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="description" content="Orange Data Mining Toolbox"/><meta property="og:url" content="https://orangedatamining.com"/><meta property="og:site_name" content="Orange Data Mining"/><meta name="author" content="Bioinformatics Laboratory, University of Ljubljana"/><title>Orange Data Mining - Orange Fairness - Adversarial Debiasing</title><meta name="robots" content="index,follow"/><meta property="og:title" content="Orange Data Mining - Orange Fairness - Adversarial Debiasing"/><meta property="og:description" content="Learn how to use the Adversarial Debiasing model in Orange for fairer machine learning."/><meta property="og:type" content="article"/><meta property="article:published_time" content="2023-09-19 01:00:00+00:00"/><meta property="article:author" content="Žan Mervič"/><meta property="article:tag" content="fairness"/><meta property="article:tag" content="adversarial debiasing"/><meta property="og:image" content="/blog/2023-09-fairness-adversarial-debiasing/__webp-images__/2023-09-19-fairness-adversarial-debiasing-thumb.webp"/><meta property="og:image:width" content="470"/><meta property="og:image:height" content="513"/><meta name="next-head-count" content="18"/><link rel="preload" href="/_next/static/media/e7c7dbb62ddcf6fa-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/148a6a0ea3cd708f.css" as="style" crossorigin=""/><link rel="stylesheet" href="/_next/static/css/148a6a0ea3cd708f.css" crossorigin="" data-n-g=""/><link rel="preload" href="/_next/static/css/4ac7ed34d61ef456.css" as="style" crossorigin=""/><link rel="stylesheet" href="/_next/static/css/4ac7ed34d61ef456.css" crossorigin="" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" crossorigin="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-21c828b96ad33382.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/framework-0c7baedefba6b077.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/main-ab94bcaa7cae018a.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/_app-4ca122263f58d1c1.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/110-f463c991b9377c97.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-5bd99f1cb3f5744c.js" defer="" crossorigin=""></script><script src="/_next/static/D1Lici584uC7lMLO-bXT9/_buildManifest.js" defer="" crossorigin=""></script><script src="/_next/static/D1Lici584uC7lMLO-bXT9/_ssgManifest.js" defer="" crossorigin=""></script><style data-styled="" data-styled-version="5.3.6">.iSNtvJ{position:relative;max-width:1296px;margin-right:auto;margin-left:auto;padding-left:72px;padding-right:72px;height:100%;box-sizing:content-box;}/*!sc*/
@media (max-width:1130px){.iSNtvJ{padding-left:30px;padding-right:30px;}}/*!sc*/
@media (max-width:720px){.iSNtvJ{padding-left:15px;padding-right:15px;}}/*!sc*/
.fJLXpS{position:relative;max-width:1296px;margin-right:auto;margin-left:auto;padding-left:72px;padding-right:72px;height:100%;box-sizing:content-box;max-width:714px;}/*!sc*/
@media (max-width:1130px){.fJLXpS{padding-left:30px;padding-right:30px;}}/*!sc*/
@media (max-width:720px){.fJLXpS{padding-left:15px;padding-right:15px;}}/*!sc*/
data-styled.g1[id="sc-ef15f394-0"]{content:"iSNtvJ,fJLXpS,"}/*!sc*/
.jyxOJT{position:relative;display:inline-block;max-width:100%;font-size:20px;line-height:1.25;font-weight:600;-webkit-text-decoration:none;text-decoration:none;color:#fff;padding:10px 15px;border-radius:5px;background-image:linear-gradient(180deg,#FE7A00 74.93%,#F65D18 100%);box-shadow:0px 10px 10px rgba(0,0,0,0.03);background-size:100%;cursor:pointer;z-index:2;}/*!sc*/
@media (max-width:720px){.jyxOJT{font-size:18px;}}/*!sc*/
.jyxOJT:before{content:"";display:block;position:absolute;top:0;left:0;width:100%;height:100%;border-radius:inherit;background-image:linear-gradient(180deg,#ffbc44 100%,#FE7A00 100%);opacity:0;z-index:-100;-webkit-transition:opacity 0.45s;transition:opacity 0.45s;}/*!sc*/
.jyxOJT:hover{color:#fff;}/*!sc*/
.jyxOJT:hover:before{opacity:1;}/*!sc*/
data-styled.g2[id="sc-b6ea565a-0"]{content:"jyxOJT,"}/*!sc*/
.iQtqNS{background:#1F1F1F;padding:38px 0 25px;color:#fff;}/*!sc*/
data-styled.g3[id="sc-c82ca4e2-0"]{content:"iQtqNS,"}/*!sc*/
.iiCJUO{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-column-gap:80px;column-gap:80px;row-gap:40px;margin-bottom:40px;}/*!sc*/
@media (max-width:720px){.iiCJUO{display:grid;grid-template-columns:1fr 1fr;-webkit-column-gap:20px;column-gap:20px;}}/*!sc*/
.iiCJUO h3{font-size:16px;font-weight:600;margin-bottom:12px;}/*!sc*/
.iiCJUO li + li{margin-top:12px;}/*!sc*/
data-styled.g4[id="sc-c82ca4e2-1"]{content:"iiCJUO,"}/*!sc*/
.eWdALT{display:none;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:space-around;-webkit-justify-content:space-around;-ms-flex-pack:space-around;justify-content:space-around;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:44px;height:34px;padding:6px;border-radius:5px;border:none;background:linear-gradient(180deg,#FE7A00 74.93%,#F65D18 100%);box-shadow:0px 10px 10px rgba(0,0,0,0.03);cursor:pointer;margin-left:auto;}/*!sc*/
@media (max-width:920px){.eWdALT{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}/*!sc*/
.eWdALT div{width:22px;height:2px;background:#fff;-webkit-transform-origin:1px;-ms-transform-origin:1px;transform-origin:1px;-webkit-transition:all 0.3s linear;transition:all 0.3s linear;}/*!sc*/
data-styled.g5[id="sc-46d6b9d9-0"]{content:"eWdALT,"}/*!sc*/
.jTcDmc{-webkit-clip:rect(0 0 0 0);clip:rect(0 0 0 0);-webkit-clip-path:inset(50%);clip-path:inset(50%);height:1px;overflow:hidden;position:absolute;white-space:nowrap;width:1px;}/*!sc*/
data-styled.g6[id="sc-cdea7863-0"]{content:"jTcDmc,"}/*!sc*/
.gPgccM{position:fixed;top:0;left:0;width:100%;height:80px;z-index:5;background:#fff;box-shadow:0px 4px 10px 4px rgba(0,0,0,0.04);}/*!sc*/
data-styled.g7[id="sc-6852f5f6-0"]{content:"gPgccM,"}/*!sc*/
.jrLPNu{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:100%;}/*!sc*/
.jrLPNu .img-logo{width:115px;}/*!sc*/
data-styled.g8[id="sc-6852f5f6-1"]{content:"jrLPNu,"}/*!sc*/
.hzGhxs{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media (max-width:920px){.hzGhxs{display:block;position:absolute;top:80px;left:0;width:100%;z-index:1;padding:0 30px 30px;background-color:#fff;opacity:0;z-index:-9999;pointer-events:none;-webkit-transition:opacity 0.3s ease;transition:opacity 0.3s ease;box-shadow:0 4px 10px -1px rgba(0,0,0,0.04);}}/*!sc*/
@media (max-width:720px){.hzGhxs{padding:0 15px 15px;}}/*!sc*/
data-styled.g9[id="sc-6852f5f6-2"]{content:"hzGhxs,"}/*!sc*/
.chSgxF{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media (max-width:920px){.chSgxF{display:block;margin-bottom:15px;}}/*!sc*/
.chSgxF li + li{margin-left:26px;}/*!sc*/
@media (max-width:920px){.chSgxF li + li{margin-left:0;}}/*!sc*/
.chSgxF a{display:inline-block;font-size:1.25rem;line-height:1;color:#000000;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:color 0.3s;transition:color 0.3s;}/*!sc*/
.chSgxF a:hover{color:#FE7A00;}/*!sc*/
@media (max-width:920px){.chSgxF a{padding:8px 0;}}/*!sc*/
data-styled.g10[id="sc-6852f5f6-3"]{content:"chSgxF,"}/*!sc*/
.hTbjmy{position:relative;width:160px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-transition:width 0.3s ease-in-out;transition:width 0.3s ease-in-out;margin-left:26px;}/*!sc*/
@media (max-width:920px){.hTbjmy{width:auto;margin-left:0;}}/*!sc*/
data-styled.g11[id="sc-6852f5f6-4"]{content:"hTbjmy,"}/*!sc*/
.kzOElI{display:inline-block;position:absolute;top:0;right:42px;width:calc(100% - 42px);height:100%;font-size:16px;line-height:1.25;padding:10px 13px;background:#fff;border-radius:5px 0px 0px 5px;border:1px solid #D9D9D9;border-right:none;-webkit-transition:width 0.3s ease-in-out;transition:width 0.3s ease-in-out;}/*!sc*/
@media (max-width:920px){.kzOElI{display:none;}}/*!sc*/
.kzOElI::-webkit-input-placeholder{color:#D9D9D9;}/*!sc*/
.kzOElI::-moz-placeholder{color:#D9D9D9;}/*!sc*/
.kzOElI:-ms-input-placeholder{color:#D9D9D9;}/*!sc*/
.kzOElI::placeholder{color:#D9D9D9;}/*!sc*/
data-styled.g12[id="sc-6852f5f6-5"]{content:"kzOElI,"}/*!sc*/
.dSfmcG{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;color:#fff;margin-left:auto;-webkit-flex:0 0 42px;-ms-flex:0 0 42px;flex:0 0 42px;height:41px;border:1px solid #474747;border-radius:0px 5px 5px 0px;background-color:#474747;}/*!sc*/
@media (max-width:920px){.dSfmcG{margin-left:0;}}/*!sc*/
data-styled.g13[id="sc-6852f5f6-6"]{content:"dSfmcG,"}/*!sc*/
.eBQFeB{min-height:100vh;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
data-styled.g18[id="sc-da66f971-0"]{content:"eBQFeB,"}/*!sc*/
*,*:before,*:after{box-sizing:border-box;-webkit-font-smoothing:antialiased;}/*!sc*/
html,body{height:100%;margin:0;}/*!sc*/
html{font-family:'Source Sans Pro',sans-serif;font-weight:400;color:#000000;}/*!sc*/
body{background:#fff;}/*!sc*/
body,h1,h2,h3,h4,h5,h6,p,ol,ul{margin:0;padding:0;font-weight:normal;}/*!sc*/
main{padding-top:80px;-webkit-flex:1;-ms-flex:1;flex:1;}/*!sc*/
ol,ul{list-style:none;}/*!sc*/
figure{margin:0;}/*!sc*/
img,video{display:block;max-width:100%;height:auto;}/*!sc*/
a{color:unset;-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
a:hover{color:unset;}/*!sc*/
strong{font-weight:600;}/*!sc*/
pre{display:block;padding:10px 14px;margin:0 0 10px;font-size:18px;line-height:1.42857143;word-break:break-all;word-wrap:break-word;color:#000;background-color:#f5f5f5;border-radius:5px;overflow-x:auto;word-wrap:normal;}/*!sc*/
pre code{padding:0;font-size:inherit;color:inherit;white-space:pre-wrap;background-color:transparent;border-radius:0;}/*!sc*/
code{font-family:Menlo,Monaco,Consolas,"Courier New",monospace;padding:2px 4px;font-size:90%;color:#c7254e;background-color:#f9f2f4;white-space:nowrap;border-radius:4px;white-space:pre-wrap;}/*!sc*/
::selection{background-color:#FE7A00;color:#fff;}/*!sc*/
.lg-container .lg-backdrop.in{opacity:0.75;}/*!sc*/
.lg-container .lg-toolbar.lg-group,.lg-container .lg-outer .lg-thumb-outer{background:rgba(0,0,0,0.45);}/*!sc*/
data-styled.g19[id="sc-global-gFJXlm1"]{content:"sc-global-gFJXlm1,"}/*!sc*/
.dFaKbK h1{font-size:44px;line-height:1.13;font-weight:700;color:#1F1F1F;}/*!sc*/
@media (max-width:920px){.dFaKbK h1{font-size:38px;}}/*!sc*/
@media (max-width:720px){.dFaKbK h1{font-size:32px;}}/*!sc*/
.dFaKbK h2{font-size:33px;line-height:1.13;font-weight:600;color:#1F1F1F;}/*!sc*/
@media (max-width:920px){.dFaKbK h2{font-size:30px;}}/*!sc*/
@media (max-width:720px){.dFaKbK h2{font-size:26px;}}/*!sc*/
.dFaKbK h3{font-size:28px;line-height:1.18;font-weight:600;color:#1F1F1F;}/*!sc*/
@media (max-width:920px){.dFaKbK h3{font-size:26px;}}/*!sc*/
@media (max-width:720px){.dFaKbK h3{font-size:24px;}}/*!sc*/
.dFaKbK h4{font-size:22px;line-height:1.18;font-weight:600;color:#1F1F1F;}/*!sc*/
@media (max-width:720px){.dFaKbK h4{font-size:20px;}}/*!sc*/
.dFaKbK p{font-size:22px;line-height:1.36;color:#1F1F1F;}/*!sc*/
@media (max-width:1130px){.dFaKbK p{font-size:20px;}}/*!sc*/
@media (max-width:920px){.dFaKbK p{font-size:18px;}}/*!sc*/
.dFaKbK ul,.dFaKbK ol{padding-left:40px;}/*!sc*/
.dFaKbK ul li,.dFaKbK ol li{font-size:22px;line-height:1.36;color:#1F1F1F;}/*!sc*/
@media (max-width:1130px){.dFaKbK ul li,.dFaKbK ol li{font-size:20px;}}/*!sc*/
@media (max-width:920px){.dFaKbK ul li,.dFaKbK ol li{font-size:18px;}}/*!sc*/
.dFaKbK ul li + li,.dFaKbK ol li + li{margin-top:4px;}/*!sc*/
.dFaKbK ul{list-style:disc;}/*!sc*/
.dFaKbK ol{list-style:decimal;}/*!sc*/
.dFaKbK p a,.dFaKbK li a{color:#FE7A00;}/*!sc*/
.dFaKbK p a:hover,.dFaKbK li a:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.dFaKbK * + *:not(li,a){margin-top:15px;}/*!sc*/
.dFaKbK * + a[data-gallery],.dFaKbK * + video{margin-top:40px;}/*!sc*/
.dFaKbK a[data-gallery]{display:block;margin-bottom:40px;}/*!sc*/
.dFaKbK iframe,.dFaKbK video{margin-bottom:40px;}/*!sc*/
data-styled.g20[id="sc-ecc41586-0"]{content:"dFaKbK,"}/*!sc*/
.fsyOxA{display:block;margin-left:auto;margin-right:auto;}/*!sc*/
data-styled.g21[id="sc-ecc41586-1"]{content:"fsyOxA,"}/*!sc*/
.cXdfZy{font-size:62px;line-height:1.04;font-weight:700;color:#fff;color:#1F1F1F;}/*!sc*/
@media (max-width:920px){.cXdfZy{font-size:50px;}}/*!sc*/
@media (max-width:720px){.cXdfZy{font-size:42px;}}/*!sc*/
data-styled.g23[id="sc-4d34df46-0"]{content:"cXdfZy,"}/*!sc*/
.wRRdf{padding:80px 0;}/*!sc*/
@media (max-width:920px){.wRRdf{padding:60px 0;}}/*!sc*/
@media (max-width:720px){.wRRdf{padding:40px 0;}}/*!sc*/
data-styled.g30[id="sc-2d40882b-0"]{content:"wRRdf,"}/*!sc*/
.bMcRdu{margin-bottom:30px;}/*!sc*/
data-styled.g31[id="sc-2d40882b-1"]{content:"bMcRdu,"}/*!sc*/
.fZuXtn{position:relative;}/*!sc*/
data-styled.g32[id="sc-2d40882b-2"]{content:"fZuXtn,"}/*!sc*/
.hlBQLj{position:absolute;top:0;right:100%;width:200px;margin-right:50px;text-align:right;}/*!sc*/
@media (max-width:1130px){.hlBQLj{position:unset;width:auto;text-align:left;margin:0 0 30px 0;}}/*!sc*/
data-styled.g33[id="sc-2d40882b-3"]{content:"hlBQLj,"}/*!sc*/
.jOGEdN{font-size:20px;line-height:1.25;color:#1F1F1F;text-transform:capitalize;color:#5651EC;}/*!sc*/
@media (max-width:920px){.jOGEdN{font-size:18px;}}/*!sc*/
.GbnzW{font-size:20px;line-height:1.25;color:#1F1F1F;}/*!sc*/
@media (max-width:920px){.GbnzW{font-size:18px;}}/*!sc*/
data-styled.g34[id="sc-2d40882b-4"]{content:"jOGEdN,GbnzW,"}/*!sc*/
</style></head><body><div id="__next"><div class="sc-da66f971-0 eBQFeB __className_667be3"><nav class="sc-6852f5f6-0 gPgccM"><div class="sc-ef15f394-0 iSNtvJ"><div class="sc-6852f5f6-1 jrLPNu"><a href="/"><img alt="Orange Logo" loading="lazy" width="120" height="35" decoding="async" data-nimg="1" class="img-logo" style="color:transparent" src="/_next/static/media/logo-orange.faff1861.svg"/></a><div><button aria-label="Toggle navigation" class="sc-46d6b9d9-0 eWdALT"><div></div><div></div><div></div></button><div class="sc-6852f5f6-2 hzGhxs"><ul class="sc-6852f5f6-3 chSgxF"><li><a href="/examples">Examples</a></li><li><a href="/download">Download</a></li><li><a href="/blog">Blog</a></li><li><a href="/docs">Docs</a></li><li><a href="/workshops">Workshops</a></li></ul><form class="sc-6852f5f6-4 hTbjmy"><div class="sc-6852f5f6-5 kzOElI">Search</div><button class="sc-6852f5f6-6 dSfmcG"><img alt="Icon for search" loading="lazy" width="18" height="17" decoding="async" data-nimg="1" style="color:transparent" src="/_next/static/media/icon-search.459b2665.svg"/><span class="sc-cdea7863-0 jTcDmc">Search through page</span></button></form></div></div></div></div></nav><main><div class="sc-2d40882b-0 wRRdf"><div class="sc-ef15f394-0 fJLXpS"><div class="sc-2d40882b-1 bMcRdu"><p class="sc-2d40882b-4 jOGEdN"><strong>fairness, adversarial debiasing</strong></p><h1 class="sc-4d34df46-0 cXdfZy">Orange Fairness - Adversarial Debiasing</h1></div><div class="sc-2d40882b-2 fZuXtn"><div class="sc-2d40882b-3 hlBQLj"><p class="sc-2d40882b-4 GbnzW"><strong>Žan Mervič</strong></p><p class="sc-2d40882b-4 GbnzW">Sep 19, 2023</p></div><div class="sc-ecc41586-0 dFaKbK"><div class="lg-react-element "><p>In the <a href="/blog/2023-09-19-fairness-reweighing-preprocessor/">previous blog post</a>, we talked about how to use the Reweighing widget as a preprocessor for a model. This blog post will discuss the Adversarial Debiasing model, a bias-aware model. We will also show how to use it in Orange.</p>
<h3>Adversarial Debiasing:</h3>
<p><a href="https://arxiv.org/abs/1801.07593">Adversarial Debiasing</a> is an in-processing type of fairness mitigation algorithm. It is a technique that uses adversarial training to mitigate bias. It involves simultaneous training of a predictor and a discriminator. The goal of the predictor is to predict the target variable accurately. At the same time, the discriminator aims to predict the protected variable (such as gender or race) based on the predictor&#x27;s predictions. The main goal is to maximize the predictor&#x27;s ability to predict the target variable while reducing the discriminator&#x27;s ability to predict the protected variable based on those predictions. Because of the Adversarial Debiasing implementation in the <a href="https://aif360.res.ibm.com/">AIF360</a> package we are using, the algorithm focuses on Disparate Impact and Statistical Parity Difference fairness metrics.</p>
<p>The widget&#x27;s interface is shown in the image below.</p>
<a href="/blog/2023-09-fairness-adversarial-debiasing/__webp-images__/2023-09-19-fairness-adversarial-debiasing.webp" data-gallery="true"><img alt="" loading="lazy" width="242" height="398" decoding="async" data-nimg="1" class="sc-ecc41586-1 fsyOxA" style="color:transparent" src="/blog/2023-09-fairness-adversarial-debiasing/__webp-images__/2023-09-19-fairness-adversarial-debiasing.webp"/></a>
<p>As seen from the image, there are some unique parameters for this widget:</p>
<ul>
<li>Neurons in hidden layers: The number of neurons in each of the hidden layers of the neural network.</li>
<li>Use Debiasing: Whether to use the debiasing or not. The widget will function as a regular neural network model if this option is not selected.</li>
<li>Adversary loss weight: The weight of the adversary loss in the total loss function. The adversary loss is the loss function of the discriminator. The higher the weight, the more the model will try to reduce the discriminator&#x27;s ability to predict the protected variable at the expense of the predictor&#x27;s ability to predict the target variable.</li>
</ul>
<h2>Orange use case</h2>
<p>Now that we know how the Adversarial Debiasing widget works and how to use it, let us look at a real-world example for a classification task.</p>
<p>For this example, we will use the <a href="https://archive.ics.uci.edu/ml/datasets/adult">Adult dataset</a>, which we have used <a href="/blog/2023-08-23-fairness-dataset-bias/">before</a>. The Adult dataset consists of 48824 instances with 15 attributes describing demographic details from the 1994 census. The primary task is to predict if an individual earns more than $50,000 annually. Unlike previously, we will not use the As Fairness widget to select fairness attributes; instead, we will keep the default ones, &quot;sex&quot; for the protected attribute and &quot;male&quot; for the privileged protected attribute value.</p>
<p>We will train two Adversarial Debiasing models, one with and one without debiasing, and compare them to Random Forests.</p>
<a href="/blog/2023-09-fairness-adversarial-debiasing/__webp-images__/2023-09-19-fairness-adversarial-debiasing-use-case.webp" data-gallery="true"><img alt="" loading="lazy" width="822" height="368" decoding="async" data-nimg="1" class="sc-ecc41586-1 fsyOxA" style="color:transparent" src="/blog/2023-09-fairness-adversarial-debiasing/__webp-images__/2023-09-19-fairness-adversarial-debiasing-use-case.webp"/></a>
<a href="/blog/2023-09-fairness-adversarial-debiasing/__webp-images__/2023-09-19-fairness-adversarial-debiasing-scores.webp" data-gallery="true"><img alt="" loading="lazy" width="968" height="531" decoding="async" data-nimg="1" class="sc-ecc41586-1 fsyOxA" style="color:transparent" src="/blog/2023-09-fairness-adversarial-debiasing/__webp-images__/2023-09-19-fairness-adversarial-debiasing-scores.webp"/></a>
<p>Test and Score shows that debiasing improved fairness metrics, particularly Disparate Impact, and Statistical Parity Difference. Without debiasing, the results are similar to that of the Random Forest model. Disparate Impact moved from 0.294 to 1.051, while Statistical Parity Difference went from -0.180 to 0.006, indicating a near-zero bias between these groups regarding favorable outcomes.</p>
<p>However, this does not mean all forms of bias were addressed. Equal Opportunity Difference and Average Odds Difference metrics worsened from -0.097 to 0.358 and -0.087 to 0.197, respectively. This suggests that even though the algorithm has been optimized for specific fairness criteria, other biases have emerged or become more pronounced.</p>
<p>Why is this happening?</p>
<ul>
<li>
<p>Trade-offs in fairness: Addressing one type of fairness sometimes comes at the expense of another. When we optimize for Disparate Impact or Statistical Parity Difference, it may affect how often positive outcomes are correctly predicted for both privileged and unprivileged groups (equal opportunity). Moreover, it could alter the balance between true positive rates and false positive rates for these groups (average odds difference).</p>
</li>
<li>
<p>Adversarial Debiasing implementation: The current algorithm implementation aims to make predictions independent of protected attributes, focusing on balancing outcomes. This approach inherently focuses on balancing outcomes (reflected in the Disparate Impact and Statistical Parity Difference metrics). However, the nuances of other fairness measures might not be as effectively addressed.</p>
</li>
</ul>
<p>When we use debiasing, there can also be a slight decrease in accuracy. This happens because the model is now not only trying to be accurate but also fair. This balance is a necessary trade-off we accept when we want to remove bias. However, it is worth noting that in this example, the accuracy remains comparable to the scenario without debiasing.</p>
<p>Next, let us look at the Box Plot widget. We will use it to show the Disparate Impact and Statistical Parity Difference, much like our previous blog approach.</p>
<a href="/blog/2023-09-fairness-adversarial-debiasing/__webp-images__/2023-09-19-fairness-adversarial-debiasing-box-plot-bias.webp" data-gallery="true"><img alt="" loading="lazy" width="966" height="461" decoding="async" data-nimg="1" class="sc-ecc41586-1 fsyOxA" style="color:transparent" src="/blog/2023-09-fairness-adversarial-debiasing/__webp-images__/2023-09-19-fairness-adversarial-debiasing-box-plot-bias.webp"/></a>
<a href="/blog/2023-09-fairness-adversarial-debiasing/__webp-images__/2023-09-19-fairness-adversarial-debiasing-box-plot-debias.webp" data-gallery="true"><img alt="" loading="lazy" width="967" height="465" decoding="async" data-nimg="1" class="sc-ecc41586-1 fsyOxA" style="color:transparent" src="/blog/2023-09-fairness-adversarial-debiasing/__webp-images__/2023-09-19-fairness-adversarial-debiasing-box-plot-debias.webp"/></a>
<p>From the first box plot we can see that when using the model without debiasing, males (the privileged group) tends to get classified with &quot;&gt;50K&quot; (the favorable class) more often than females (the unprivileged grouo) do. This is reflected in the Disparate Impact and Statistical Parity Difference metrics which are 0.294 and -0.180, respectively, both below their optimal value, indicating bias towards the unprivileged group.</p>
<p>In the second box plot, we can see that when using the model with debiasing, males and females get classified with the favorable class at a very similar rate. This is also indicated by the Disparate Impact and Statistical Parity Difference metrics which are 1.051 and 0.006, respectively, both very close to their optimal value, indicating a negligible amount of bias towards the privileged group.</p></div></div></div></div></div></main><footer class="sc-c82ca4e2-0 iQtqNS"><div class="sc-ef15f394-0 iSNtvJ"><div class="sc-c82ca4e2-1 iiCJUO"><div><h3>Orange</h3><ul><li><a href="/faq">FAQ</a></li><li><a href="/license">License</a></li><li><a href="/privacy">Privacy</a></li><li><a href="/citation">Citation</a></li><li><a href="/contact">Contact</a></li></ul></div><div><h3>Download</h3><ul><li><a href="/download#win">Windows</a></li><li><a href="/download#mac">Mac OS</a></li></ul></div><div><h3>Community</h3><ul><li><a href="https://twitter.com/OrangeDataMiner">Twitter</a></li><li><a href="https://www.facebook.com/orangedatamining">Facebook</a></li><li><a href="https://datascience.stackexchange.com/questions/tagged/orange">Stack Exchange</a></li><li><a href="https://www.youtube.com/channel/UClKKWBe2SCAEyv7ZNGhIe4g">YouTube</a></li><li><a href="https://discord.com/invite/FWrfeXV">Discord</a></li></ul></div><div><h3>Documentation</h3><ul><li><a href="/getting-started">Get started</a></li><li><a href="/widget-catalogue">Widgets</a></li><li><a href="https://orange3.readthedocs.io/projects/orange-data-mining-library/en/latest/">Scripting</a></li></ul></div><div><h3>Developers</h3><ul><li><a href="https://github.com/biolab/orange3">GitHub</a></li><li><a href="http://docs.biolab.si/3/development/">Getting started</a></li></ul></div><div><a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=A76TAX87ZVR3J" target="_blank" rel="noreferrer" class="sc-b6ea565a-0 jyxOJT">Donate to Orange</a></div></div><p>Copyright © University of Ljubljana</p></div></footer></div></div><script id="__NEXT_DATA__" type="application/json" crossorigin="">{"props":{"pageProps":{"frontmatter":{"author":"Žan Mervič","date":"2023-09-19 01:00:00+00:00","draft":false,"title":"Orange Fairness - Adversarial Debiasing","thumbImage":"2023-09-19-fairness-adversarial-debiasing-thumb.png","frontPageImage":"2023-09-19-fairness-adversarial-debiasing-thumb.png","blog":["fairness","adversarial debiasing"],"shortExcerpt":"Learn how to use the Adversarial Debiasing model in Orange for fairer machine learning.","longExcerpt":"This blog post focuses on the Adversarial Debiasing model in Orange, a tool for enhancing fairness in your machine learning algorithms. We will walk through how to use it and explain the trade-offs that come with using fairness algorithms.","oldUrl":"/blog/2023/2023-09-19-fairness-adversarial-debiasing/"},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    a: \"a\",\n    h3: \"h3\",\n    ul: \"ul\",\n    li: \"li\",\n    h2: \"h2\"\n  }, _provideComponents(), props.components), {WindowScreenshot} = _components;\n  if (!WindowScreenshot) _missingMdxReference(\"WindowScreenshot\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.p, {\n      children: [\"In the \", _jsx(_components.a, {\n        href: \"/blog/2023-09-19-fairness-reweighing-preprocessor/\",\n        children: \"previous blog post\"\n      }), \", we talked about how to use the Reweighing widget as a preprocessor for a model. This blog post will discuss the Adversarial Debiasing model, a bias-aware model. We will also show how to use it in Orange.\"]\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"Adversarial Debiasing:\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [_jsx(_components.a, {\n        href: \"https://arxiv.org/abs/1801.07593\",\n        children: \"Adversarial Debiasing\"\n      }), \" is an in-processing type of fairness mitigation algorithm. It is a technique that uses adversarial training to mitigate bias. It involves simultaneous training of a predictor and a discriminator. The goal of the predictor is to predict the target variable accurately. At the same time, the discriminator aims to predict the protected variable (such as gender or race) based on the predictor's predictions. The main goal is to maximize the predictor's ability to predict the target variable while reducing the discriminator's ability to predict the protected variable based on those predictions. Because of the Adversarial Debiasing implementation in the \", _jsx(_components.a, {\n        href: \"https://aif360.res.ibm.com/\",\n        children: \"AIF360\"\n      }), \" package we are using, the algorithm focuses on Disparate Impact and Statistical Parity Difference fairness metrics.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The widget's interface is shown in the image below.\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2023-09-fairness-adversarial-debiasing/2023-09-19-fairness-adversarial-debiasing.png\",\n      width: \"242\",\n      height: \"398\",\n      src: \"/blog/2023-09-fairness-adversarial-debiasing/__webp-images__/2023-09-19-fairness-adversarial-debiasing.webp\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"As seen from the image, there are some unique parameters for this widget:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Neurons in hidden layers: The number of neurons in each of the hidden layers of the neural network.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Use Debiasing: Whether to use the debiasing or not. The widget will function as a regular neural network model if this option is not selected.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Adversary loss weight: The weight of the adversary loss in the total loss function. The adversary loss is the loss function of the discriminator. The higher the weight, the more the model will try to reduce the discriminator's ability to predict the protected variable at the expense of the predictor's ability to predict the target variable.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Orange use case\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now that we know how the Adversarial Debiasing widget works and how to use it, let us look at a real-world example for a classification task.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"For this example, we will use the \", _jsx(_components.a, {\n        href: \"https://archive.ics.uci.edu/ml/datasets/adult\",\n        children: \"Adult dataset\"\n      }), \", which we have used \", _jsx(_components.a, {\n        href: \"/blog/2023-08-23-fairness-dataset-bias/\",\n        children: \"before\"\n      }), \". The Adult dataset consists of 48824 instances with 15 attributes describing demographic details from the 1994 census. The primary task is to predict if an individual earns more than $50,000 annually. Unlike previously, we will not use the As Fairness widget to select fairness attributes; instead, we will keep the default ones, \\\"sex\\\" for the protected attribute and \\\"male\\\" for the privileged protected attribute value.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We will train two Adversarial Debiasing models, one with and one without debiasing, and compare them to Random Forests.\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2023-09-fairness-adversarial-debiasing/2023-09-19-fairness-adversarial-debiasing-use-case.png\",\n      width: \"822\",\n      height: \"368\",\n      src: \"/blog/2023-09-fairness-adversarial-debiasing/__webp-images__/2023-09-19-fairness-adversarial-debiasing-use-case.webp\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2023-09-fairness-adversarial-debiasing/2023-09-19-fairness-adversarial-debiasing-scores.png\",\n      width: \"968\",\n      height: \"531\",\n      src: \"/blog/2023-09-fairness-adversarial-debiasing/__webp-images__/2023-09-19-fairness-adversarial-debiasing-scores.webp\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Test and Score shows that debiasing improved fairness metrics, particularly Disparate Impact, and Statistical Parity Difference. Without debiasing, the results are similar to that of the Random Forest model. Disparate Impact moved from 0.294 to 1.051, while Statistical Parity Difference went from -0.180 to 0.006, indicating a near-zero bias between these groups regarding favorable outcomes.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"However, this does not mean all forms of bias were addressed. Equal Opportunity Difference and Average Odds Difference metrics worsened from -0.097 to 0.358 and -0.087 to 0.197, respectively. This suggests that even though the algorithm has been optimized for specific fairness criteria, other biases have emerged or become more pronounced.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Why is this happening?\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"Trade-offs in fairness: Addressing one type of fairness sometimes comes at the expense of another. When we optimize for Disparate Impact or Statistical Parity Difference, it may affect how often positive outcomes are correctly predicted for both privileged and unprivileged groups (equal opportunity). Moreover, it could alter the balance between true positive rates and false positive rates for these groups (average odds difference).\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"Adversarial Debiasing implementation: The current algorithm implementation aims to make predictions independent of protected attributes, focusing on balancing outcomes. This approach inherently focuses on balancing outcomes (reflected in the Disparate Impact and Statistical Parity Difference metrics). However, the nuances of other fairness measures might not be as effectively addressed.\"\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When we use debiasing, there can also be a slight decrease in accuracy. This happens because the model is now not only trying to be accurate but also fair. This balance is a necessary trade-off we accept when we want to remove bias. However, it is worth noting that in this example, the accuracy remains comparable to the scenario without debiasing.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Next, let us look at the Box Plot widget. We will use it to show the Disparate Impact and Statistical Parity Difference, much like our previous blog approach.\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2023-09-fairness-adversarial-debiasing/2023-09-19-fairness-adversarial-debiasing-box-plot-bias.png\",\n      width: \"966\",\n      height: \"461\",\n      src: \"/blog/2023-09-fairness-adversarial-debiasing/__webp-images__/2023-09-19-fairness-adversarial-debiasing-box-plot-bias.webp\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2023-09-fairness-adversarial-debiasing/2023-09-19-fairness-adversarial-debiasing-box-plot-debias.png\",\n      width: \"967\",\n      height: \"465\",\n      src: \"/blog/2023-09-fairness-adversarial-debiasing/__webp-images__/2023-09-19-fairness-adversarial-debiasing-box-plot-debias.webp\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"From the first box plot we can see that when using the model without debiasing, males (the privileged group) tends to get classified with \\\"\u003e50K\\\" (the favorable class) more often than females (the unprivileged grouo) do. This is reflected in the Disparate Impact and Statistical Parity Difference metrics which are 0.294 and -0.180, respectively, both below their optimal value, indicating bias towards the unprivileged group.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"In the second box plot, we can see that when using the model with debiasing, males and females get classified with the favorable class at a very similar rate. This is also indicated by the Disparate Impact and Statistical Parity Difference metrics which are 1.051 and 0.006, respectively, both very close to their optimal value, indicating a negligible amount of bias towards the privileged group.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{}},"thumbImage":{"width":470,"height":513,"src":"/blog/2023-09-fairness-adversarial-debiasing/__webp-images__/2023-09-19-fairness-adversarial-debiasing-thumb.webp"}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"orange-fairness-adversarial-debiasing"},"buildId":"D1Lici584uC7lMLO-bXT9","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>