<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>My new cool app</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/4ac7ed34d61ef456.css" as="style"/><link rel="stylesheet" href="/_next/static/css/4ac7ed34d61ef456.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-d38be8d96a62f950.js" defer=""></script><script src="/_next/static/chunks/framework-3b5a00d5d7e8d93b.js" defer=""></script><script src="/_next/static/chunks/main-9f90a364d66949ce.js" defer=""></script><script src="/_next/static/chunks/pages/_app-35144eb61b09af2c.js" defer=""></script><script src="/_next/static/chunks/675-262430aa11afdf01.js" defer=""></script><script src="/_next/static/chunks/9-9d15d34c7affe00b.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-fd50a1465d8f452c.js" defer=""></script><script src="/_next/static/Z00IbhcMESsGP8msRS_H6/_buildManifest.js" defer=""></script><script src="/_next/static/Z00IbhcMESsGP8msRS_H6/_ssgManifest.js" defer=""></script><style data-styled="" data-styled-version="5.3.6">.iBQlkL{background:coral;height:60px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media (max-width:920px){.iBQlkL ul{display:none;}}/*!sc*/
data-styled.g1[id="sc-7295e4c-0"]{content:"iBQlkL,"}/*!sc*/
.bDMZDz{display:none;font-size:22px;margin-left:auto;}/*!sc*/
@media (max-width:920px){.bDMZDz{display:block;}}/*!sc*/
data-styled.g2[id="sc-7295e4c-1"]{content:"bDMZDz,"}/*!sc*/
html,body{height:100%;margin:0;}/*!sc*/
body{background:#fff;}/*!sc*/
html{box-sizing:border-box;font-size:16px;}/*!sc*/
*,*:before,*:after{box-sizing:inherit;}/*!sc*/
body,h1,h2,h3,h4,h5,h6,p,ol,ul{margin:0;padding:0;font-weight:normal;}/*!sc*/
ol,ul{list-style:none;}/*!sc*/
img,video{max-width:100%;height:auto;}/*!sc*/
.lg-container .lg-backdrop.in{opacity:0.75;}/*!sc*/
.lg-container .lg-toolbar.lg-group,.lg-container .lg-outer .lg-thumb-outer{background:rgba(0,0,0,0.45);}/*!sc*/
data-styled.g3[id="sc-global-cqyaiF1"]{content:"sc-global-cqyaiF1,"}/*!sc*/
.eZqfqp{display:block;margin-left:auto;margin-right:auto;}/*!sc*/
data-styled.g4[id="sc-242d5c63-0"]{content:"eZqfqp,"}/*!sc*/
.gndEvh{max-width:800px;margin:0 auto;}/*!sc*/
data-styled.g6[id="sc-6329e001-0"]{content:"gndEvh,"}/*!sc*/
</style></head><body><div id="__next"><nav class="sc-7295e4c-0 iBQlkL"><ul><a href="/blog">Blog</a></ul><button class="sc-7295e4c-1 bDMZDz">X</button></nav><main><div class="sc-6329e001-0 gndEvh"><h1>Confusion matrix for regression?</h1><div class="lg-react-element "><p>It is easy to inspect misclassifications in the Confusion Matrix widget when building classification models. One can even click on misclassified instances, output them and observe them in various visualizations. But what about regression? Predicting numeric values doesn&#x27;t even allow connecting Confusion Matrix, nor would it make sense. So how can one inspect prediction error for regression tasks?</p>
<p>Let us take the well-known <em>housing</em> data set from the File widget. The dataset has 506 instances of houses described with 13 variables and a regression target variable MEDV (median value of homes in 1000$).</p>
<p>We can quickly build a simple workflow with Test and Score and Linear Regression, which estimates model accuracy and outputs predictions.</p>
<a href="/blog/2022-05-regression-results/2022-05-20_workflow1.png" data-gallery="true"><img srcSet="/blog/2022-05-regression-results/2022-05-20_workflow1.png 1x, /blog/2022-05-regression-results/2022-05-20_workflow1.png 2x" src="/blog/2022-05-regression-results/2022-05-20_workflow1.png" width="596" height="324" decoding="async" data-nimg="1" loading="lazy" style="color:transparent"/></a>
<p>Looking at predictions in a Data Table, we can see the true value in the grey column (MEDV) and the predicted value in the beige column (Linear Regression). But it is nigh impossible to extract any meaningful information just by looking at true and predicted values in a table. A visualization would be nice.</p>
<a href="/blog/2022-05-regression-results/2022-05-20_data-table.png" data-gallery="true"><img srcSet="/blog/2022-05-regression-results/2022-05-20_data-table.png 1x, /blog/2022-05-regression-results/2022-05-20_data-table.png 2x" src="/blog/2022-05-regression-results/2022-05-20_data-table.png" width="662" height="493" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<p>Let us connect Feature Constructor to Test and Score. We will create two new variables in the Feature Constructor, both numeric. Click on <em>New</em> --&gt; <em>Numeric</em>, give it a name, say <em>Error</em> and define the function for computing the prediction error. We will pass it a simple subtraction <code>Linear_Regression-MEDV</code>, which subtracts the true value from the predicted value. In this way, we will get negative values for underpredicted instances and positive values for overpredicted instances.</p>
<p>For a more in-depth inspection, create a variable <em>Abs Error</em>, computing the absolute error with <code>abs(Linear_Regression-MEDV)</code>.</p>
<a href="/blog/2022-05-regression-results/2022-05-20_feat-const.png" data-gallery="true"><img srcSet="/blog/2022-05-regression-results/2022-05-20_feat-const.png 1x, /blog/2022-05-regression-results/2022-05-20_feat-const.png 2x" src="/blog/2022-05-regression-results/2022-05-20_feat-const.png" width="851" height="396" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<p>Now, we can inspect the predictions in a Scatter Plot. Set <strong>MEDV</strong> for x-axis and <strong>Linear Regression</strong> for y-axis. Color by <strong>Error</strong> and set the size to <strong>Abs Error</strong>. Since we are using simple linear regression, the results are more or less as expected. Everything above the imaginary regression line will be colored yellow (overprediction) and below it blue (underpredictions). The larger the point, the larger the prediction error. We can see some strange predictions in the top right corner of the plot. We can select them and inspect them downstream.</p>
<a href="/blog/2022-05-regression-results/2022-05-20_scatter-plot.png" data-gallery="true"><img srcSet="/blog/2022-05-regression-results/2022-05-20_scatter-plot.png 1x, /blog/2022-05-regression-results/2022-05-20_scatter-plot.png 2x" src="/blog/2022-05-regression-results/2022-05-20_scatter-plot.png" width="1080" height="718" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<p>Alternatively, we can observe the absolute error in the feature space. Let us select <strong>LSTAT</strong> and <strong>RM</strong> variables in the scatter plot. Now, color by <strong>Abs Error</strong> and set the same variable for size. We can observe two correlated variables (LSTAT and RM) and inspect where the failed predictions lie. They are more frequent in the lower and higher LSTAT values.</p>
<a href="/blog/2022-05-regression-results/2022-05-20_scatter-plot2.png" data-gallery="true"><img srcSet="/blog/2022-05-regression-results/2022-05-20_scatter-plot2.png 1x, /blog/2022-05-regression-results/2022-05-20_scatter-plot2.png 2x" src="/blog/2022-05-regression-results/2022-05-20_scatter-plot2.png" width="1085" height="721" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<p>Now you have no more excuses not to check prediction errors for regression tasks! :)</p>
<a href="/blog/2022-05-regression-results/2022-05-20_workflow2.png" data-gallery="true"><img srcSet="/blog/2022-05-regression-results/2022-05-20_workflow2.png 1x, /blog/2022-05-regression-results/2022-05-20_workflow2.png 2x" src="/blog/2022-05-regression-results/2022-05-20_workflow2.png" width="708" height="242" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a></div></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"author":"Ajda Pretnar Å½agar","date":"2022-05-20","draft":false,"title":"Confusion matrix for regression?","type":"blog","thumbImage":"2022-05-20-cf-regression.png","frontPageImage":"2022-05-20-cf-regression.png","blog":["regression","confusion matrix","scatter plot","prediction error"],"shortExcerpt":"How to display regression error in Orange?","longExcerpt":"Confusion matrix shows classification error, but what is a suitable alternative for observing regression errors in Orange?","x2images":true},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    em: \"em\",\n    code: \"code\",\n    strong: \"strong\"\n  }, _provideComponents(), props.components), {Figure, WindowScreenshot} = _components;\n  if (!Figure) _missingMdxReference(\"Figure\", true);\n  if (!WindowScreenshot) _missingMdxReference(\"WindowScreenshot\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"It is easy to inspect misclassifications in the Confusion Matrix widget when building classification models. One can even click on misclassified instances, output them and observe them in various visualizations. But what about regression? Predicting numeric values doesn't even allow connecting Confusion Matrix, nor would it make sense. So how can one inspect prediction error for regression tasks?\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Let us take the well-known \", _jsx(_components.em, {\n        children: \"housing\"\n      }), \" data set from the File widget. The dataset has 506 instances of houses described with 13 variables and a regression target variable MEDV (median value of homes in 1000$).\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We can quickly build a simple workflow with Test and Score and Linear Regression, which estimates model accuracy and outputs predictions.\"\n    }), \"\\n\", _jsx(Figure, {\n      src: \"/blog/2022-05-regression-results/2022-05-20_workflow1.png\",\n      width: \"10%\",\n      width: \"596\",\n      height: \"324\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Looking at predictions in a Data Table, we can see the true value in the grey column (MEDV) and the predicted value in the beige column (Linear Regression). But it is nigh impossible to extract any meaningful information just by looking at true and predicted values in a table. A visualization would be nice.\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2022-05-regression-results/2022-05-20_data-table.png\",\n      width: \"662\",\n      height: \"493\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Let us connect Feature Constructor to Test and Score. We will create two new variables in the Feature Constructor, both numeric. Click on \", _jsx(_components.em, {\n        children: \"New\"\n      }), \" --\u003e \", _jsx(_components.em, {\n        children: \"Numeric\"\n      }), \", give it a name, say \", _jsx(_components.em, {\n        children: \"Error\"\n      }), \" and define the function for computing the prediction error. We will pass it a simple subtraction \", _jsx(_components.code, {\n        children: \"Linear_Regression-MEDV\"\n      }), \", which subtracts the true value from the predicted value. In this way, we will get negative values for underpredicted instances and positive values for overpredicted instances.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"For a more in-depth inspection, create a variable \", _jsx(_components.em, {\n        children: \"Abs Error\"\n      }), \", computing the absolute error with \", _jsx(_components.code, {\n        children: \"abs(Linear_Regression-MEDV)\"\n      }), \".\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2022-05-regression-results/2022-05-20_feat-const.png\",\n      width: \"851\",\n      height: \"396\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Now, we can inspect the predictions in a Scatter Plot. Set \", _jsx(_components.strong, {\n        children: \"MEDV\"\n      }), \" for x-axis and \", _jsx(_components.strong, {\n        children: \"Linear Regression\"\n      }), \" for y-axis. Color by \", _jsx(_components.strong, {\n        children: \"Error\"\n      }), \" and set the size to \", _jsx(_components.strong, {\n        children: \"Abs Error\"\n      }), \". Since we are using simple linear regression, the results are more or less as expected. Everything above the imaginary regression line will be colored yellow (overprediction) and below it blue (underpredictions). The larger the point, the larger the prediction error. We can see some strange predictions in the top right corner of the plot. We can select them and inspect them downstream.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2022-05-regression-results/2022-05-20_scatter-plot.png\",\n      width: \"1080\",\n      height: \"718\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Alternatively, we can observe the absolute error in the feature space. Let us select \", _jsx(_components.strong, {\n        children: \"LSTAT\"\n      }), \" and \", _jsx(_components.strong, {\n        children: \"RM\"\n      }), \" variables in the scatter plot. Now, color by \", _jsx(_components.strong, {\n        children: \"Abs Error\"\n      }), \" and set the same variable for size. We can observe two correlated variables (LSTAT and RM) and inspect where the failed predictions lie. They are more frequent in the lower and higher LSTAT values.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2022-05-regression-results/2022-05-20_scatter-plot2.png\",\n      width: \"1085\",\n      height: \"721\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now you have no more excuses not to check prediction errors for regression tasks! :)\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2022-05-regression-results/2022-05-20_workflow2.png\",\n      width: \"708\",\n      height: \"242\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"confusion-matrix-for-regression"},"buildId":"Z00IbhcMESsGP8msRS_H6","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>