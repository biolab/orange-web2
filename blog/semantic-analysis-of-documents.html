<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>My new cool app</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/4ac7ed34d61ef456.css" as="style"/><link rel="stylesheet" href="/_next/static/css/4ac7ed34d61ef456.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-d38be8d96a62f950.js" defer=""></script><script src="/_next/static/chunks/framework-3b5a00d5d7e8d93b.js" defer=""></script><script src="/_next/static/chunks/main-9f90a364d66949ce.js" defer=""></script><script src="/_next/static/chunks/pages/_app-35144eb61b09af2c.js" defer=""></script><script src="/_next/static/chunks/675-262430aa11afdf01.js" defer=""></script><script src="/_next/static/chunks/9-9d15d34c7affe00b.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-fd50a1465d8f452c.js" defer=""></script><script src="/_next/static/Z00IbhcMESsGP8msRS_H6/_buildManifest.js" defer=""></script><script src="/_next/static/Z00IbhcMESsGP8msRS_H6/_ssgManifest.js" defer=""></script><style data-styled="" data-styled-version="5.3.6">.iBQlkL{background:coral;height:60px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media (max-width:920px){.iBQlkL ul{display:none;}}/*!sc*/
data-styled.g1[id="sc-7295e4c-0"]{content:"iBQlkL,"}/*!sc*/
.bDMZDz{display:none;font-size:22px;margin-left:auto;}/*!sc*/
@media (max-width:920px){.bDMZDz{display:block;}}/*!sc*/
data-styled.g2[id="sc-7295e4c-1"]{content:"bDMZDz,"}/*!sc*/
html,body{height:100%;margin:0;}/*!sc*/
body{background:#fff;}/*!sc*/
html{box-sizing:border-box;font-size:16px;}/*!sc*/
*,*:before,*:after{box-sizing:inherit;}/*!sc*/
body,h1,h2,h3,h4,h5,h6,p,ol,ul{margin:0;padding:0;font-weight:normal;}/*!sc*/
ol,ul{list-style:none;}/*!sc*/
img,video{max-width:100%;height:auto;}/*!sc*/
.lg-container .lg-backdrop.in{opacity:0.75;}/*!sc*/
.lg-container .lg-toolbar.lg-group,.lg-container .lg-outer .lg-thumb-outer{background:rgba(0,0,0,0.45);}/*!sc*/
data-styled.g3[id="sc-global-cqyaiF1"]{content:"sc-global-cqyaiF1,"}/*!sc*/
.eZqfqp{display:block;margin-left:auto;margin-right:auto;}/*!sc*/
data-styled.g4[id="sc-242d5c63-0"]{content:"eZqfqp,"}/*!sc*/
.gndEvh{max-width:800px;margin:0 auto;}/*!sc*/
data-styled.g6[id="sc-6329e001-0"]{content:"gndEvh,"}/*!sc*/
</style></head><body><div id="__next"><nav class="sc-7295e4c-0 iBQlkL"><ul><a href="/blog">Blog</a></ul><button class="sc-7295e4c-1 bDMZDz">X</button></nav><main><div class="sc-6329e001-0 gndEvh"><h1>Semantic Analysis of Documents</h1><div class="lg-react-element "><p>Our recent project with the Ministry of Public Administration comprises building a <a href="https://nio.gov.si/nio/asset/semanticni+analizator+besedil?lang=en">semantic analysis pipeline in Orange</a>, enabling the users to quickly and efficiently explore the content of documents, compare a subset against the corpus, extract keywords, and semantically explore document maps. If this sounds too vague, don&#x27;t worry, here&#x27;s a quick demo on how to perform semantic analysis in Orange.</p>
<p>First, we will use the pre-prepared corpus of <a href="https://predlagam.vladi.si/">proposals to the government</a>, which you can download <a href="http://file.biolab.si/text-semantics/data/predlogi-vladi-1k.tab">here</a>. These are the initiatives which the citizens of Slovenia propose to the current government for consideration. The present corpus contains 1093 such proposals.</p>
<a href="/blog/2021-09-semantic-analysis/2021-09-17-wf1.png" data-gallery="true"><img srcSet="/blog/2021-09-semantic-analysis/2021-09-17-wf1.png 1x, /blog/2021-09-semantic-analysis/2021-09-17-wf1.png 2x" src="/blog/2021-09-semantic-analysis/2021-09-17-wf1.png" width="256" height="113" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<a href="/blog/2021-09-semantic-analysis/2021-09-17-corpus.png" data-gallery="true"><img srcSet="/blog/2021-09-semantic-analysis/2021-09-17-corpus.png 1x, /blog/2021-09-semantic-analysis/2021-09-17-corpus.png 2x" src="/blog/2021-09-semantic-analysis/2021-09-17-corpus.png" width="605" height="431" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<p>Each proposal contains a title, the content of the proposal, the author, the date when it was published, number of upvotes, and so on. But for a thousand proposals, it would take a long time to read all of them and see which policy areas they cover. Instead, we will use two new Orange widgets to determine the content (main keywords) of a subset of documents.</p>
<a href="/blog/2021-09-semantic-analysis/2021-09-17-corpus-viewer.png" data-gallery="true"><img srcSet="/blog/2021-09-semantic-analysis/2021-09-17-corpus-viewer.png 1x, /blog/2021-09-semantic-analysis/2021-09-17-corpus-viewer.png 2x" src="/blog/2021-09-semantic-analysis/2021-09-17-corpus-viewer.png" width="1207" height="628" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<p>As always, we will first preprocess the corpus to create tokens, the core units of our analysis. The preprocessing pipeline is sequential; first, we lowercase the text, then we split the text into words (this is what the regular expression <code>\w+.</code> does), transform the words into lemmas with UDPipe, and finally remove stopwords from the list (such as &quot;in&quot;, &quot;da&quot;, &quot;če&quot;). Since we are working with a Slovenian language text, we have to select the corresponding models and stopword lists.</p>
<a href="/blog/2021-09-semantic-analysis/2021-09-17-wf2.png" data-gallery="true"><img srcSet="/blog/2021-09-semantic-analysis/2021-09-17-wf2.png 1x, /blog/2021-09-semantic-analysis/2021-09-17-wf2.png 2x" src="/blog/2021-09-semantic-analysis/2021-09-17-wf2.png" width="254" height="114" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<a href="/blog/2021-09-semantic-analysis/2021-09-17-preprocess-text.png" data-gallery="true"><img srcSet="/blog/2021-09-semantic-analysis/2021-09-17-preprocess-text.png 1x, /blog/2021-09-semantic-analysis/2021-09-17-preprocess-text.png 2x" src="/blog/2021-09-semantic-analysis/2021-09-17-preprocess-text.png" width="984" height="1030" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<p>After preprocessing, we build a document-term matrix using the <strong>Bag of Words</strong> widget with the Count+IDF setting. Next, we pass the data to <strong>t-SNE</strong> to observe the document map. t-SNE takes the document-term matrix and finds a 2D projection, where similar documents lie close together.</p>
<a href="/blog/2021-09-semantic-analysis/2021-09-17-wf3.png" data-gallery="true"><img srcSet="/blog/2021-09-semantic-analysis/2021-09-17-wf3.png 1x, /blog/2021-09-semantic-analysis/2021-09-17-wf3.png 2x" src="/blog/2021-09-semantic-analysis/2021-09-17-wf3.png" width="282" height="112" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<a href="/blog/2021-09-semantic-analysis/2021-09-17-tsne1.png" data-gallery="true"><img srcSet="/blog/2021-09-semantic-analysis/2021-09-17-tsne1.png 1x" src="/blog/2021-09-semantic-analysis/2021-09-17-tsne1.png" width="2240" height="1456" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<p>Now we will select a small subset of the document, say in the lower right corner, where we have a nice cluster of points. The question is, what are these documents talking about?</p>
<p>We will use <strong>Extract Keywords</strong> widget to find the most significant keywords in the selection. There are several methods we can use, even the popuar <a href="https://repositorio.inesctec.pt/bitstream/123456789/7623/1/P-00N-NF5.pdf">YAKE!</a>, but we will go with a simple TF-IDF method, which takes the words with the highest TF-IDF score. Note that the vectorizer uses the default sklearn&#x27;s <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">TfidfVectorizer</a> settings, that is the tf-idf transform with L2 norm, keeping the passed tokens as they are.</p>
<a href="/blog/2021-09-semantic-analysis/2021-09-17-wf4.png" data-gallery="true"><img srcSet="/blog/2021-09-semantic-analysis/2021-09-17-wf4.png 1x, /blog/2021-09-semantic-analysis/2021-09-17-wf4.png 2x" src="/blog/2021-09-semantic-analysis/2021-09-17-wf4.png" width="288" height="118" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<a href="/blog/2021-09-semantic-analysis/2021-09-17-extract-keywords.png" data-gallery="true"><img srcSet="/blog/2021-09-semantic-analysis/2021-09-17-extract-keywords.png 1x, /blog/2021-09-semantic-analysis/2021-09-17-extract-keywords.png 2x" src="/blog/2021-09-semantic-analysis/2021-09-17-extract-keywords.png" width="643" height="548" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<p>It looks like the top words characterizing the selected subcorpus are &quot;študent&quot; (<em>student</em>), &quot;delati&quot; (<em>to work</em>), and &quot;delo&quot; (<em>the work</em>). Apparently, the documents talk mostly about student work. Let us explore this a bit further. It would be nice to have a certain score attached to the documents, which would correspond to how much a document talks about student work. In other words, we would like to score the documents based on how many of the selected words they contain (and in what proportion).</p>
<p>To achieve this, we will use <strong>Score Documents</strong>. We will pass it the document-term matrix and the list of selected keywords from <strong>Extract Keywords</strong>. The widget again offers several different ways of scoring documents. A simple way to score them is to compute how often selected words appear in each document, which corresponds to the &quot;Word Count&quot; method.</p>
<a href="/blog/2021-09-semantic-analysis/2021-09-17-wf5.png" data-gallery="true"><img srcSet="/blog/2021-09-semantic-analysis/2021-09-17-wf5.png 1x, /blog/2021-09-semantic-analysis/2021-09-17-wf5.png 2x" src="/blog/2021-09-semantic-analysis/2021-09-17-wf5.png" width="566" height="218" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<a href="/blog/2021-09-semantic-analysis/2021-09-17-score-documents.png" data-gallery="true"><img srcSet="/blog/2021-09-semantic-analysis/2021-09-17-score-documents.png 1x, /blog/2021-09-semantic-analysis/2021-09-17-score-documents.png 2x" src="/blog/2021-09-semantic-analysis/2021-09-17-score-documents.png" width="1007" height="634" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<p><strong>Score Documents</strong> returns keyword scores for each document. Let us pass the scored documents to another <strong>t-SNE</strong> widget. If we set the color and the size of the points to &quot;Word Count&quot; variable, t-SNE plot will expose the documents with the highest scores. These documents talk the most about students and work. A great thing is that we can see documents with high scores that were not a part of our selection, which means the general bottom-right area contains documents relating to this topic.</p>
<a href="/blog/2021-09-semantic-analysis/2021-09-17-tsne2.png" data-gallery="true"><img srcSet="/blog/2021-09-semantic-analysis/2021-09-17-tsne2.png 1x, /blog/2021-09-semantic-analysis/2021-09-17-tsne2.png 2x" src="/blog/2021-09-semantic-analysis/2021-09-17-tsne2.png" width="1832" height="1160" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<a href="/blog/2021-09-semantic-analysis/2021-09-17-workflow.png" data-gallery="true"><img srcSet="/blog/2021-09-semantic-analysis/2021-09-17-workflow.png 1x, /blog/2021-09-semantic-analysis/2021-09-17-workflow.png 2x" src="/blog/2021-09-semantic-analysis/2021-09-17-workflow.png" width="1734" height="562" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<p>Now try selecting a different subset yourself and see what the documents are about. You can use any corpus you want, even the ones that come with Orange.</p></div></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"author":"Ajda Pretnar","date":"2021-09-17","draft":false,"title":"Semantic Analysis of Documents","type":"blog","thumbImage":"2021-09-17-seman.png","frontPageImage":"2021-09-17-seman.png","blog":["semantic analysis","text mining","corpus","keywords"],"shortExcerpt":"How to use Text add-on for semantic analysis of documents.","longExcerpt":"How to use Text add-on to extract keywords from documents, score documents on keywords, and display semantic content in a map.","x2images":true},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    a: \"a\",\n    code: \"code\",\n    strong: \"strong\",\n    em: \"em\"\n  }, _provideComponents(), props.components), {WindowScreenshot} = _components;\n  if (!WindowScreenshot) _missingMdxReference(\"WindowScreenshot\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.p, {\n      children: [\"Our recent project with the Ministry of Public Administration comprises building a \", _jsx(_components.a, {\n        href: \"https://nio.gov.si/nio/asset/semanticni+analizator+besedil?lang=en\",\n        children: \"semantic analysis pipeline in Orange\"\n      }), \", enabling the users to quickly and efficiently explore the content of documents, compare a subset against the corpus, extract keywords, and semantically explore document maps. If this sounds too vague, don't worry, here's a quick demo on how to perform semantic analysis in Orange.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"First, we will use the pre-prepared corpus of \", _jsx(_components.a, {\n        href: \"https://predlagam.vladi.si/\",\n        children: \"proposals to the government\"\n      }), \", which you can download \", _jsx(_components.a, {\n        href: \"http://file.biolab.si/text-semantics/data/predlogi-vladi-1k.tab\",\n        children: \"here\"\n      }), \". These are the initiatives which the citizens of Slovenia propose to the current government for consideration. The present corpus contains 1093 such proposals.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-wf1.png\",\n      width: \"256\",\n      height: \"113\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-corpus.png\",\n      width: \"605\",\n      height: \"431\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Each proposal contains a title, the content of the proposal, the author, the date when it was published, number of upvotes, and so on. But for a thousand proposals, it would take a long time to read all of them and see which policy areas they cover. Instead, we will use two new Orange widgets to determine the content (main keywords) of a subset of documents.\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-corpus-viewer.png\",\n      width: \"1207\",\n      height: \"628\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"As always, we will first preprocess the corpus to create tokens, the core units of our analysis. The preprocessing pipeline is sequential; first, we lowercase the text, then we split the text into words (this is what the regular expression \", _jsx(_components.code, {\n        children: \"\\\\w+.\"\n      }), \" does), transform the words into lemmas with UDPipe, and finally remove stopwords from the list (such as \\\"in\\\", \\\"da\\\", \\\"če\\\"). Since we are working with a Slovenian language text, we have to select the corresponding models and stopword lists.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-wf2.png\",\n      width: \"254\",\n      height: \"114\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-preprocess-text.png\",\n      width: \"984\",\n      height: \"1030\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"After preprocessing, we build a document-term matrix using the \", _jsx(_components.strong, {\n        children: \"Bag of Words\"\n      }), \" widget with the Count+IDF setting. Next, we pass the data to \", _jsx(_components.strong, {\n        children: \"t-SNE\"\n      }), \" to observe the document map. t-SNE takes the document-term matrix and finds a 2D projection, where similar documents lie close together.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-wf3.png\",\n      width: \"282\",\n      height: \"112\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-tsne1.png\",\n      width: \"2240\",\n      height: \"1456\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now we will select a small subset of the document, say in the lower right corner, where we have a nice cluster of points. The question is, what are these documents talking about?\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"We will use \", _jsx(_components.strong, {\n        children: \"Extract Keywords\"\n      }), \" widget to find the most significant keywords in the selection. There are several methods we can use, even the popuar \", _jsx(_components.a, {\n        href: \"https://repositorio.inesctec.pt/bitstream/123456789/7623/1/P-00N-NF5.pdf\",\n        children: \"YAKE!\"\n      }), \", but we will go with a simple TF-IDF method, which takes the words with the highest TF-IDF score. Note that the vectorizer uses the default sklearn's \", _jsx(_components.a, {\n        href: \"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\",\n        children: \"TfidfVectorizer\"\n      }), \" settings, that is the tf-idf transform with L2 norm, keeping the passed tokens as they are.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-wf4.png\",\n      width: \"288\",\n      height: \"118\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-extract-keywords.png\",\n      width: \"643\",\n      height: \"548\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"It looks like the top words characterizing the selected subcorpus are \\\"študent\\\" (\", _jsx(_components.em, {\n        children: \"student\"\n      }), \"), \\\"delati\\\" (\", _jsx(_components.em, {\n        children: \"to work\"\n      }), \"), and \\\"delo\\\" (\", _jsx(_components.em, {\n        children: \"the work\"\n      }), \"). Apparently, the documents talk mostly about student work. Let us explore this a bit further. It would be nice to have a certain score attached to the documents, which would correspond to how much a document talks about student work. In other words, we would like to score the documents based on how many of the selected words they contain (and in what proportion).\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"To achieve this, we will use \", _jsx(_components.strong, {\n        children: \"Score Documents\"\n      }), \". We will pass it the document-term matrix and the list of selected keywords from \", _jsx(_components.strong, {\n        children: \"Extract Keywords\"\n      }), \". The widget again offers several different ways of scoring documents. A simple way to score them is to compute how often selected words appear in each document, which corresponds to the \\\"Word Count\\\" method.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-wf5.png\",\n      width: \"566\",\n      height: \"218\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-score-documents.png\",\n      width: \"1007\",\n      height: \"634\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [_jsx(_components.strong, {\n        children: \"Score Documents\"\n      }), \" returns keyword scores for each document. Let us pass the scored documents to another \", _jsx(_components.strong, {\n        children: \"t-SNE\"\n      }), \" widget. If we set the color and the size of the points to \\\"Word Count\\\" variable, t-SNE plot will expose the documents with the highest scores. These documents talk the most about students and work. A great thing is that we can see documents with high scores that were not a part of our selection, which means the general bottom-right area contains documents relating to this topic.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-tsne2.png\",\n      width: \"1832\",\n      height: \"1160\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-workflow.png\",\n      width: \"1734\",\n      height: \"562\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now try selecting a different subset yourself and see what the documents are about. You can use any corpus you want, even the ones that come with Orange.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"semantic-analysis-of-documents"},"buildId":"Z00IbhcMESsGP8msRS_H6","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>