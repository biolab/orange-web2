<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>My new cool app</title><meta name="next-head-count" content="3"/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-45fae5f2ec2ec297.js" defer=""></script><script src="/_next/static/chunks/framework-3b5a00d5d7e8d93b.js" defer=""></script><script src="/_next/static/chunks/main-abafce5311b78c60.js" defer=""></script><script src="/_next/static/chunks/pages/_app-92507f6e7d1f2d92.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-9737114052c0c44f.js" defer=""></script><script src="/_next/static/nUFfkIWVfcFkGuk2uwBzg/_buildManifest.js" defer=""></script><script src="/_next/static/nUFfkIWVfcFkGuk2uwBzg/_ssgManifest.js" defer=""></script><style data-styled="" data-styled-version="5.3.6">.hZTkcw{background:coral;height:60px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media (max-width:920px){.hZTkcw ul{display:none;}}/*!sc*/
data-styled.g1[id="navbar__Nav-sc-q11qw3-0"]{content:"hZTkcw,"}/*!sc*/
.cGMLJw{display:none;font-size:22px;margin-left:auto;}/*!sc*/
@media (max-width:920px){.cGMLJw{display:block;}}/*!sc*/
data-styled.g2[id="navbar__Burger-sc-q11qw3-1"]{content:"cGMLJw,"}/*!sc*/
html,body{height:100%;margin:0;}/*!sc*/
body{background:#fff;}/*!sc*/
html{box-sizing:border-box;font-size:16px;}/*!sc*/
*,*:before,*:after{box-sizing:inherit;}/*!sc*/
body,h1,h2,h3,h4,h5,h6,p,ol,ul{margin:0;padding:0;font-weight:normal;}/*!sc*/
ol,ul{list-style:none;}/*!sc*/
img,video{max-width:100%;height:auto;}/*!sc*/
data-styled.g3[id="sc-global-fozGBl1"]{content:"sc-global-fozGBl1,"}/*!sc*/
.cHTstd{max-width:800px;margin:0 auto;}/*!sc*/
data-styled.g4[id="slug__Wrapper-sc-1xtdpf8-0"]{content:"cHTstd,"}/*!sc*/
.hFKfMl{max-height:500px;display:block;margin-left:auto;margin-right:auto;}/*!sc*/
data-styled.g5[id="slug__WSImage-sc-1xtdpf8-1"]{content:"hFKfMl,"}/*!sc*/
</style></head><body><div id="__next"><nav class="navbar__Nav-sc-q11qw3-0 hZTkcw"><ul><a href="/blog">Blog</a></ul><button class="navbar__Burger-sc-q11qw3-1 cGMLJw">X</button></nav><main><div class="slug__Wrapper-sc-1xtdpf8-0 cHTstd"><h1>Semantic Analysis of Documents</h1><p>Our recent project with the Ministry of Public Administration comprises building a <a href="https://nio.gov.si/nio/asset/semanticni+analizator+besedil?lang=en">semantic analysis pipeline in Orange</a>, enabling the users to quickly and efficiently explore the content of documents, compare a subset against the corpus, extract keywords, and semantically explore document maps. If this sounds too vague, don&#x27;t worry, here&#x27;s a quick demo on how to perform semantic analysis in Orange.</p>
<p>First, we will use the pre-prepared corpus of <a href="https://predlagam.vladi.si/">proposals to the government</a>, which you can download <a href="http://file.biolab.si/text-semantics/data/predlogi-vladi-1k.tab">here</a>. These are the initiatives which the citizens of Slovenia propose to the current government for consideration. The present corpus contains 1093 such proposals.</p>
<img src="/blog/2021-09-semantic-analysis/2021-09-17-wf1.png" class="slug__WSImage-sc-1xtdpf8-1 hFKfMl"/>
<img src="/blog/2021-09-semantic-analysis/2021-09-17-corpus.png" class="slug__WSImage-sc-1xtdpf8-1 hFKfMl"/>
<p>Each proposal contains a title, the content of the proposal, the author, the date when it was published, number of upvotes, and so on. But for a thousand proposals, it would take a long time to read all of them and see which policy areas they cover. Instead, we will use two new Orange widgets to determine the content (main keywords) of a subset of documents.</p>
<img src="/blog/2021-09-semantic-analysis/2021-09-17-corpus-viewer.png" class="slug__WSImage-sc-1xtdpf8-1 hFKfMl"/>
<p>As always, we will first preprocess the corpus to create tokens, the core units of our analysis. The preprocessing pipeline is sequential; first, we lowercase the text, then we split the text into words (this is what the regular expression <code>\w+.</code> does), transform the words into lemmas with UDPipe, and finally remove stopwords from the list (such as &quot;in&quot;, &quot;da&quot;, &quot;če&quot;). Since we are working with a Slovenian language text, we have to select the corresponding models and stopword lists.</p>
<img src="/blog/2021-09-semantic-analysis/2021-09-17-wf2.png" class="slug__WSImage-sc-1xtdpf8-1 hFKfMl"/>
<img src="/blog/2021-09-semantic-analysis/2021-09-17-preprocess-text.png" class="slug__WSImage-sc-1xtdpf8-1 hFKfMl"/>
<p>After preprocessing, we build a document-term matrix using the <strong>Bag of Words</strong> widget with the Count+IDF setting. Next, we pass the data to <strong>t-SNE</strong> to observe the document map. t-SNE takes the document-term matrix and finds a 2D projection, where similar documents lie close together.</p>
<img src="/blog/2021-09-semantic-analysis/2021-09-17-wf3.png" class="slug__WSImage-sc-1xtdpf8-1 hFKfMl"/>
<img src="/blog/2021-09-semantic-analysis/2021-09-17-tsne1.png" class="slug__WSImage-sc-1xtdpf8-1 hFKfMl"/>
<p>Now we will select a small subset of the document, say in the lower right corner, where we have a nice cluster of points. The question is, what are these documents talking about?</p>
<p>We will use <strong>Extract Keywords</strong> widget to find the most significant keywords in the selection. There are several methods we can use, even the popuar <a href="https://repositorio.inesctec.pt/bitstream/123456789/7623/1/P-00N-NF5.pdf">YAKE!</a>, but we will go with a simple TF-IDF method, which takes the words with the highest TF-IDF score. Note that the vectorizer uses the default sklearn&#x27;s <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">TfidfVectorizer</a> settings, that is the tf-idf transform with L2 norm, keeping the passed tokens as they are.</p>
<img src="/blog/2021-09-semantic-analysis/2021-09-17-wf4.png" class="slug__WSImage-sc-1xtdpf8-1 hFKfMl"/>
<img src="/blog/2021-09-semantic-analysis/2021-09-17-extract-keywords.png" class="slug__WSImage-sc-1xtdpf8-1 hFKfMl"/>
<p>It looks like the top words characterizing the selected subcorpus are &quot;študent&quot; (<em>student</em>), &quot;delati&quot; (<em>to work</em>), and &quot;delo&quot; (<em>the work</em>). Apparently, the documents talk mostly about student work. Let us explore this a bit further. It would be nice to have a certain score attached to the documents, which would correspond to how much a document talks about student work. In other words, we would like to score the documents based on how many of the selected words they contain (and in what proportion).</p>
<p>To achieve this, we will use <strong>Score Documents</strong>. We will pass it the document-term matrix and the list of selected keywords from <strong>Extract Keywords</strong>. The widget again offers several different ways of scoring documents. A simple way to score them is to compute how often selected words appear in each document, which corresponds to the &quot;Word Count&quot; method.</p>
<img src="/blog/2021-09-semantic-analysis/2021-09-17-wf5.png" class="slug__WSImage-sc-1xtdpf8-1 hFKfMl"/>
<img src="/blog/2021-09-semantic-analysis/2021-09-17-score-documents.png" class="slug__WSImage-sc-1xtdpf8-1 hFKfMl"/>
<p><strong>Score Documents</strong> returns keyword scores for each document. Let us pass the scored documents to another <strong>t-SNE</strong> widget. If we set the color and the size of the points to &quot;Word Count&quot; variable, t-SNE plot will expose the documents with the highest scores. These documents talk the most about students and work. A great thing is that we can see documents with high scores that were not a part of our selection, which means the general bottom-right area contains documents relating to this topic.</p>
<img src="/blog/2021-09-semantic-analysis/2021-09-17-tsne2.png" class="slug__WSImage-sc-1xtdpf8-1 hFKfMl"/>
<img src="/blog/2021-09-semantic-analysis/2021-09-17-workflow.png" class="slug__WSImage-sc-1xtdpf8-1 hFKfMl"/>
<p>Now try selecting a different subset yourself and see what the documents are about. You can use any corpus you want, even the ones that come with Orange.</p></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"author":"Ajda Pretnar","date":"2021-09-17","draft":false,"title":"Semantic Analysis of Documents","type":"blog","thumbImage":"2021-09-17-seman.png","frontPageImage":"2021-09-17-seman.png","blog":["semantic analysis","text mining","corpus","keywords"],"shortExcerpt":"How to use Text add-on for semantic analysis of documents.","longExcerpt":"How to use Text add-on to extract keywords from documents, score documents on keywords, and display semantic content in a map.","x2images":true},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    a: \"a\",\n    code: \"code\",\n    strong: \"strong\",\n    em: \"em\"\n  }, _provideComponents(), props.components), {WindowScreenshot} = _components;\n  if (!WindowScreenshot) _missingMdxReference(\"WindowScreenshot\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.p, {\n      children: [\"Our recent project with the Ministry of Public Administration comprises building a \", _jsx(_components.a, {\n        href: \"https://nio.gov.si/nio/asset/semanticni+analizator+besedil?lang=en\",\n        children: \"semantic analysis pipeline in Orange\"\n      }), \", enabling the users to quickly and efficiently explore the content of documents, compare a subset against the corpus, extract keywords, and semantically explore document maps. If this sounds too vague, don't worry, here's a quick demo on how to perform semantic analysis in Orange.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"First, we will use the pre-prepared corpus of \", _jsx(_components.a, {\n        href: \"https://predlagam.vladi.si/\",\n        children: \"proposals to the government\"\n      }), \", which you can download \", _jsx(_components.a, {\n        href: \"http://file.biolab.si/text-semantics/data/predlogi-vladi-1k.tab\",\n        children: \"here\"\n      }), \". These are the initiatives which the citizens of Slovenia propose to the current government for consideration. The present corpus contains 1093 such proposals.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-wf1.png\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-corpus.png\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Each proposal contains a title, the content of the proposal, the author, the date when it was published, number of upvotes, and so on. But for a thousand proposals, it would take a long time to read all of them and see which policy areas they cover. Instead, we will use two new Orange widgets to determine the content (main keywords) of a subset of documents.\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-corpus-viewer.png\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"As always, we will first preprocess the corpus to create tokens, the core units of our analysis. The preprocessing pipeline is sequential; first, we lowercase the text, then we split the text into words (this is what the regular expression \", _jsx(_components.code, {\n        children: \"\\\\w+.\"\n      }), \" does), transform the words into lemmas with UDPipe, and finally remove stopwords from the list (such as \\\"in\\\", \\\"da\\\", \\\"če\\\"). Since we are working with a Slovenian language text, we have to select the corresponding models and stopword lists.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-wf2.png\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-preprocess-text.png\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"After preprocessing, we build a document-term matrix using the \", _jsx(_components.strong, {\n        children: \"Bag of Words\"\n      }), \" widget with the Count+IDF setting. Next, we pass the data to \", _jsx(_components.strong, {\n        children: \"t-SNE\"\n      }), \" to observe the document map. t-SNE takes the document-term matrix and finds a 2D projection, where similar documents lie close together.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-wf3.png\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-tsne1.png\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now we will select a small subset of the document, say in the lower right corner, where we have a nice cluster of points. The question is, what are these documents talking about?\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"We will use \", _jsx(_components.strong, {\n        children: \"Extract Keywords\"\n      }), \" widget to find the most significant keywords in the selection. There are several methods we can use, even the popuar \", _jsx(_components.a, {\n        href: \"https://repositorio.inesctec.pt/bitstream/123456789/7623/1/P-00N-NF5.pdf\",\n        children: \"YAKE!\"\n      }), \", but we will go with a simple TF-IDF method, which takes the words with the highest TF-IDF score. Note that the vectorizer uses the default sklearn's \", _jsx(_components.a, {\n        href: \"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\",\n        children: \"TfidfVectorizer\"\n      }), \" settings, that is the tf-idf transform with L2 norm, keeping the passed tokens as they are.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-wf4.png\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-extract-keywords.png\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"It looks like the top words characterizing the selected subcorpus are \\\"študent\\\" (\", _jsx(_components.em, {\n        children: \"student\"\n      }), \"), \\\"delati\\\" (\", _jsx(_components.em, {\n        children: \"to work\"\n      }), \"), and \\\"delo\\\" (\", _jsx(_components.em, {\n        children: \"the work\"\n      }), \"). Apparently, the documents talk mostly about student work. Let us explore this a bit further. It would be nice to have a certain score attached to the documents, which would correspond to how much a document talks about student work. In other words, we would like to score the documents based on how many of the selected words they contain (and in what proportion).\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"To achieve this, we will use \", _jsx(_components.strong, {\n        children: \"Score Documents\"\n      }), \". We will pass it the document-term matrix and the list of selected keywords from \", _jsx(_components.strong, {\n        children: \"Extract Keywords\"\n      }), \". The widget again offers several different ways of scoring documents. A simple way to score them is to compute how often selected words appear in each document, which corresponds to the \\\"Word Count\\\" method.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-wf5.png\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-score-documents.png\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [_jsx(_components.strong, {\n        children: \"Score Documents\"\n      }), \" returns keyword scores for each document. Let us pass the scored documents to another \", _jsx(_components.strong, {\n        children: \"t-SNE\"\n      }), \" widget. If we set the color and the size of the points to \\\"Word Count\\\" variable, t-SNE plot will expose the documents with the highest scores. These documents talk the most about students and work. A great thing is that we can see documents with high scores that were not a part of our selection, which means the general bottom-right area contains documents relating to this topic.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-tsne2.png\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-workflow.png\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now try selecting a different subset yourself and see what the documents are about. You can use any corpus you want, even the ones that come with Orange.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"semantic-analysis-of-documents"},"buildId":"nUFfkIWVfcFkGuk2uwBzg","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>