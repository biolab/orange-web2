<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>My new cool app</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/4ac7ed34d61ef456.css" as="style"/><link rel="stylesheet" href="/_next/static/css/4ac7ed34d61ef456.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-d38be8d96a62f950.js" defer=""></script><script src="/_next/static/chunks/framework-3b5a00d5d7e8d93b.js" defer=""></script><script src="/_next/static/chunks/main-9f90a364d66949ce.js" defer=""></script><script src="/_next/static/chunks/pages/_app-8cfbd06f8ddd0b49.js" defer=""></script><script src="/_next/static/chunks/675-262430aa11afdf01.js" defer=""></script><script src="/_next/static/chunks/9-9d15d34c7affe00b.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-fd50a1465d8f452c.js" defer=""></script><script src="/_next/static/MdneM3OWGZMTOR1krVaKX/_buildManifest.js" defer=""></script><script src="/_next/static/MdneM3OWGZMTOR1krVaKX/_ssgManifest.js" defer=""></script><style data-styled="" data-styled-version="5.3.6">.iBQlkL{background:coral;height:60px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media (max-width:920px){.iBQlkL ul{display:none;}}/*!sc*/
data-styled.g1[id="sc-7295e4c-0"]{content:"iBQlkL,"}/*!sc*/
.bDMZDz{display:none;font-size:22px;margin-left:auto;}/*!sc*/
@media (max-width:920px){.bDMZDz{display:block;}}/*!sc*/
data-styled.g2[id="sc-7295e4c-1"]{content:"bDMZDz,"}/*!sc*/
html,body{height:100%;margin:0;}/*!sc*/
body{background:#fff;}/*!sc*/
html{box-sizing:border-box;font-size:16px;}/*!sc*/
*,*:before,*:after{box-sizing:inherit;}/*!sc*/
body,h1,h2,h3,h4,h5,h6,p,ol,ul{margin:0;padding:0;font-weight:normal;}/*!sc*/
ol,ul{list-style:none;}/*!sc*/
img,video{max-width:100%;height:auto;}/*!sc*/
.lg-container .lg-backdrop.in{opacity:0.75;}/*!sc*/
.lg-container .lg-toolbar.lg-group,.lg-container .lg-outer .lg-thumb-outer{background:rgba(0,0,0,0.45);}/*!sc*/
data-styled.g3[id="sc-global-cqyaiF1"]{content:"sc-global-cqyaiF1,"}/*!sc*/
.gndEvh{max-width:800px;margin:0 auto;}/*!sc*/
data-styled.g6[id="sc-6329e001-0"]{content:"gndEvh,"}/*!sc*/
</style></head><body><div id="__next"><nav class="sc-7295e4c-0 iBQlkL"><ul><a href="/blog">Blog</a></ul><button class="sc-7295e4c-1 bDMZDz">X</button></nav><main><div class="sc-6329e001-0 gndEvh"><h1>Data Preparation for Machine Learning</h1><div class="lg-react-element "><p>We&#x27;ve said it numerous times and we&#x27;re going to say it again. Data preparation is crucial for any data analysis. If your data is messy, there&#x27;s no way you can make sense of it, let alone a computer. Computers are great at handling large, even enormous data sets, speedy computing and recognizing patterns. But they fail miserably if you give them the wrong input. Also some classification methods work better with binary values, other with continuous, so it is important to know how to treat your data properly.</p>
<p>Orange is well equipped for such tasks.</p>
<p><strong>Widget no. 1: Preprocess</strong></p>
<p><img src="/blog/2017-01-data-preparation-for-machine-learning/preprocess.png" alt=""/></p>
<p><a href="/widget-catalog/data/preprocess/">Preprocess</a> is there to handle a big share of your preprocessing tasks.</p>
<p><img src="/blog/2017-01-data-preparation-for-machine-learning/original-data.png" alt=""/></p>
<ul>
<li>
<p>It can normalize numerical data variables. Say we have a fictional data set of people employed in your company. We want to know which employees are more likely to go on holiday, based on the yearly income, years employed in your company and total years of experience in the industry. If you plot this in heat map, you would see a bold yellow line at &#x27;yearly income&#x27;. This obviously happens because yearly income has much higher values than years of experience or years employed by your company. You would naturally like the wage not to overweight the rest of the feature set, so normalization is the way to go. Normalization will transform your values to relative terms, that is, say (depending on the type of normalization) on a scale from 0 to 1. Now <a href="/widget-catalog/visualize/heatmap/">Heat Map</a> neatly shows that people who&#x27;ve been employed longer and have a higher wage more often go on holidays. (Yes, this is a totally fictional data set, but you see the point.)</p>
</li>
</ul>
<table><thead><tr><th align="center"><img src="/blog/2017-01-data-preparation-for-machine-learning/heatmap1.png" alt=""/></th><th align="center"><img src="/blog/2017-01-data-preparation-for-machine-learning/heatmap2.png" alt=""/></th></tr></thead><tbody><tr><td align="center">no normalization</td><td align="center">normalized data</td></tr></tbody></table>
<ul>
<li>It can impute missing values. Average or most frequent missing value imputation might seem as overly simple, but it actually works most of the time. Also, all the learners that require imputation do it implicitly, so the user doesn&#x27;t have to configure yet another widget for that.</li>
<li>If you want to compare your results against a randomly mixed data set, select &#x27;Randomize&#x27; or if you want to select relevant features, this is the widget for it.</li>
</ul>
<p>Preprocessing needs to be used with caution and understanding of your data to avoid losing important information or, worse, overfitting the model. A good example is a case of paramedics, who usually don&#x27;t record pulse if it is normal. Missing values here thus cannot be imputed by an average value or random number, but as a distinct value (normal pulse). Domain knowledge is always crucial for data preparation.</p>
<p><strong>Widget no. 2: Discretize</strong></p>
<p><img src="/blog/2017-01-data-preparation-for-machine-learning/discretize.png" alt=""/></p>
<p>For certain tasks you might want to resort to binning, which is what <a href="/widget-catalog/data/discretize/">Discretize</a> does. It effectively distributes your continuous values into a selected number of bins, thus making the variable discrete-like. You can either discretize all your data variables at once, using selected discretization type, or select a particular discretization method for each attribute. The cool thing is the transformation is already displayed in the widget, so you instantly know what you&#x27;re getting in the end. A good example of discretization would be having a data set of your customers with their age recorded. It would make little sense to segment customers by each particular age, so binning them into 4 age groups (young, young-adult, middle-aged, senior) would be a great solution. Also some visualizations require feature transformation - <a href="/widget-catalog/visualize/sievediagram/">Sieve Diagram</a> is currently one such widget. Mosaic Display, however, has the transformation already implemented internally.</p>
<p><img src="/blog/2017-01-data-preparation-for-machine-learning/discretize1.png" alt=""/>
original data</p>
<p><img src="/blog/2017-01-data-preparation-for-machine-learning/discretize2.png" alt=""/>
Discretized data with &#x27;years employed&#x27; lower or higher then/equal to 8 (same for &#x27;yearly income&#x27; and &#x27;experience in the industry&#x27;.</p>
<p><strong>Widget no. 3: Continuize</strong></p>
<p><img src="/blog/2017-01-data-preparation-for-machine-learning/continuize.png" alt=""/></p>
<p>This widget essentially creates new attributes out of your discrete ones. If you have, for example, an attribute with people&#x27;s eye color, where values can be either blue, brown or green, you would probably want to have three separate attributes &#x27;blue&#x27;, &#x27;green&#x27; and &#x27;brown&#x27; with 0 or 1 if a person has that eye color. Some learners perform much better if data is transformed in such a way. You can also only have attributes where you would presume 0 is a normal condition and would only like to have deviations from the normal state recorded (&#x27;target or first value as base&#x27;) or the normal state would be the most common value (&#x27;most frequent value as base&#x27;). <a href="/widget-catalog/data/continuize/">Continuize</a> widget offers you a lot of room to play. Best thing is to select a small data set with discrete values, connect it to Continuize and then further to <a href="/widget-catalog/data/datatable/">Data Table</a> and change the parameters. This is how you can observe the transformations in real time. It is useful for projecting discrete data points in <a href="/widget-catalog/visualize/linearprojection/">Linear Projection</a>.</p>
<p><img src="/blog/2017-01-data-preparation-for-machine-learning/continuize1.png" alt=""/>
Original data.</p>
<p><img src="/blog/2017-01-data-preparation-for-machine-learning/continuize2.png" alt=""/>
Continuized data with two new columns - attribute &#x27;position&#x27; was replaced by attributes &#x27;position=office worker&#x27; and &#x27;position=technical staff&#x27; (same for &#x27;gender&#x27;).</p>
<p><strong>Widget no. 4: Purge Domain</strong></p>
<p><img src="/blog/2017-01-data-preparation-for-machine-learning/purge.png" alt=""/></p>
<p>Get a broom and sort your data! That&#x27;s what <a href="/widget-catalog/data/purgedomain/">Purge Domain</a> does. If all of the values of some attributes are constant, it will remove these attributes. If you have unused (empty) attributes in your data, it will remove them. Effectively, you will get a nice and comprehensive data set in the end.</p>
<p><img src="/blog/2017-01-data-preparation-for-machine-learning/purge1.png" alt=""/>
Original data.</p>
<p><img src="/blog/2017-01-data-preparation-for-machine-learning/purge2.png" alt=""/>
Empty columns and columns with the same (constant) value were removed.</p>
<p>Of course, don&#x27;t forget to include all these procedures into your report with the &#x27;Report&#x27; button! :)</p></div></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"author":"AJDA","date":"2017-01-13 09:25:58+00:00","draft":false,"title":"Data Preparation for Machine Learning","type":"blog","blog":["analysis","business intelligence","data","feature engineering","preprocessing"]},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    strong: \"strong\",\n    img: \"img\",\n    a: \"a\",\n    ul: \"ul\",\n    li: \"li\",\n    table: \"table\",\n    thead: \"thead\",\n    tr: \"tr\",\n    th: \"th\",\n    tbody: \"tbody\",\n    td: \"td\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"We've said it numerous times and we're going to say it again. Data preparation is crucial for any data analysis. If your data is messy, there's no way you can make sense of it, let alone a computer. Computers are great at handling large, even enormous data sets, speedy computing and recognizing patterns. But they fail miserably if you give them the wrong input. Also some classification methods work better with binary values, other with continuous, so it is important to know how to treat your data properly.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Orange is well equipped for such tasks.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.strong, {\n        children: \"Widget no. 1: Preprocess\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/blog/2017-01-data-preparation-for-machine-learning/preprocess.png\",\n        alt: \"\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [_jsx(_components.a, {\n        href: \"/widget-catalog/data/preprocess/\",\n        children: \"Preprocess\"\n      }), \" is there to handle a big share of your preprocessing tasks.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/blog/2017-01-data-preparation-for-machine-learning/original-data.png\",\n        alt: \"\"\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [\"It can normalize numerical data variables. Say we have a fictional data set of people employed in your company. We want to know which employees are more likely to go on holiday, based on the yearly income, years employed in your company and total years of experience in the industry. If you plot this in heat map, you would see a bold yellow line at 'yearly income'. This obviously happens because yearly income has much higher values than years of experience or years employed by your company. You would naturally like the wage not to overweight the rest of the feature set, so normalization is the way to go. Normalization will transform your values to relative terms, that is, say (depending on the type of normalization) on a scale from 0 to 1. Now \", _jsx(_components.a, {\n            href: \"/widget-catalog/visualize/heatmap/\",\n            children: \"Heat Map\"\n          }), \" neatly shows that people who've been employed longer and have a higher wage more often go on holidays. (Yes, this is a totally fictional data set, but you see the point.)\"]\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.table, {\n      children: [_jsx(_components.thead, {\n        children: _jsxs(_components.tr, {\n          children: [_jsx(_components.th, {\n            align: \"center\",\n            children: _jsx(_components.img, {\n              src: \"/blog/2017-01-data-preparation-for-machine-learning/heatmap1.png\",\n              alt: \"\"\n            })\n          }), _jsx(_components.th, {\n            align: \"center\",\n            children: _jsx(_components.img, {\n              src: \"/blog/2017-01-data-preparation-for-machine-learning/heatmap2.png\",\n              alt: \"\"\n            })\n          })]\n        })\n      }), _jsx(_components.tbody, {\n        children: _jsxs(_components.tr, {\n          children: [_jsx(_components.td, {\n            align: \"center\",\n            children: \"no normalization\"\n          }), _jsx(_components.td, {\n            align: \"center\",\n            children: \"normalized data\"\n          })]\n        })\n      })]\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"It can impute missing values. Average or most frequent missing value imputation might seem as overly simple, but it actually works most of the time. Also, all the learners that require imputation do it implicitly, so the user doesn't have to configure yet another widget for that.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"If you want to compare your results against a randomly mixed data set, select 'Randomize' or if you want to select relevant features, this is the widget for it.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Preprocessing needs to be used with caution and understanding of your data to avoid losing important information or, worse, overfitting the model. A good example is a case of paramedics, who usually don't record pulse if it is normal. Missing values here thus cannot be imputed by an average value or random number, but as a distinct value (normal pulse). Domain knowledge is always crucial for data preparation.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.strong, {\n        children: \"Widget no. 2: Discretize\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/blog/2017-01-data-preparation-for-machine-learning/discretize.png\",\n        alt: \"\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"For certain tasks you might want to resort to binning, which is what \", _jsx(_components.a, {\n        href: \"/widget-catalog/data/discretize/\",\n        children: \"Discretize\"\n      }), \" does. It effectively distributes your continuous values into a selected number of bins, thus making the variable discrete-like. You can either discretize all your data variables at once, using selected discretization type, or select a particular discretization method for each attribute. The cool thing is the transformation is already displayed in the widget, so you instantly know what you're getting in the end. A good example of discretization would be having a data set of your customers with their age recorded. It would make little sense to segment customers by each particular age, so binning them into 4 age groups (young, young-adult, middle-aged, senior) would be a great solution. Also some visualizations require feature transformation - \", _jsx(_components.a, {\n        href: \"/widget-catalog/visualize/sievediagram/\",\n        children: \"Sieve Diagram\"\n      }), \" is currently one such widget. Mosaic Display, however, has the transformation already implemented internally.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [_jsx(_components.img, {\n        src: \"/blog/2017-01-data-preparation-for-machine-learning/discretize1.png\",\n        alt: \"\"\n      }), \"\\noriginal data\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [_jsx(_components.img, {\n        src: \"/blog/2017-01-data-preparation-for-machine-learning/discretize2.png\",\n        alt: \"\"\n      }), \"\\nDiscretized data with 'years employed' lower or higher then/equal to 8 (same for 'yearly income' and 'experience in the industry'.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.strong, {\n        children: \"Widget no. 3: Continuize\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/blog/2017-01-data-preparation-for-machine-learning/continuize.png\",\n        alt: \"\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"This widget essentially creates new attributes out of your discrete ones. If you have, for example, an attribute with people's eye color, where values can be either blue, brown or green, you would probably want to have three separate attributes 'blue', 'green' and 'brown' with 0 or 1 if a person has that eye color. Some learners perform much better if data is transformed in such a way. You can also only have attributes where you would presume 0 is a normal condition and would only like to have deviations from the normal state recorded ('target or first value as base') or the normal state would be the most common value ('most frequent value as base'). \", _jsx(_components.a, {\n        href: \"/widget-catalog/data/continuize/\",\n        children: \"Continuize\"\n      }), \" widget offers you a lot of room to play. Best thing is to select a small data set with discrete values, connect it to Continuize and then further to \", _jsx(_components.a, {\n        href: \"/widget-catalog/data/datatable/\",\n        children: \"Data Table\"\n      }), \" and change the parameters. This is how you can observe the transformations in real time. It is useful for projecting discrete data points in \", _jsx(_components.a, {\n        href: \"/widget-catalog/visualize/linearprojection/\",\n        children: \"Linear Projection\"\n      }), \".\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [_jsx(_components.img, {\n        src: \"/blog/2017-01-data-preparation-for-machine-learning/continuize1.png\",\n        alt: \"\"\n      }), \"\\nOriginal data.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [_jsx(_components.img, {\n        src: \"/blog/2017-01-data-preparation-for-machine-learning/continuize2.png\",\n        alt: \"\"\n      }), \"\\nContinuized data with two new columns - attribute 'position' was replaced by attributes 'position=office worker' and 'position=technical staff' (same for 'gender').\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.strong, {\n        children: \"Widget no. 4: Purge Domain\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/blog/2017-01-data-preparation-for-machine-learning/purge.png\",\n        alt: \"\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Get a broom and sort your data! That's what \", _jsx(_components.a, {\n        href: \"/widget-catalog/data/purgedomain/\",\n        children: \"Purge Domain\"\n      }), \" does. If all of the values of some attributes are constant, it will remove these attributes. If you have unused (empty) attributes in your data, it will remove them. Effectively, you will get a nice and comprehensive data set in the end.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [_jsx(_components.img, {\n        src: \"/blog/2017-01-data-preparation-for-machine-learning/purge1.png\",\n        alt: \"\"\n      }), \"\\nOriginal data.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [_jsx(_components.img, {\n        src: \"/blog/2017-01-data-preparation-for-machine-learning/purge2.png\",\n        alt: \"\"\n      }), \"\\nEmpty columns and columns with the same (constant) value were removed.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Of course, don't forget to include all these procedures into your report with the 'Report' button! :)\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"data-preparation-for-machine-learning"},"buildId":"MdneM3OWGZMTOR1krVaKX","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>