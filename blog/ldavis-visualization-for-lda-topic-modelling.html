<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>My new cool app</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/4ac7ed34d61ef456.css" as="style"/><link rel="stylesheet" href="/_next/static/css/4ac7ed34d61ef456.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-d38be8d96a62f950.js" defer=""></script><script src="/_next/static/chunks/framework-3b5a00d5d7e8d93b.js" defer=""></script><script src="/_next/static/chunks/main-9f90a364d66949ce.js" defer=""></script><script src="/_next/static/chunks/pages/_app-8cfbd06f8ddd0b49.js" defer=""></script><script src="/_next/static/chunks/675-262430aa11afdf01.js" defer=""></script><script src="/_next/static/chunks/9-9d15d34c7affe00b.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-fd50a1465d8f452c.js" defer=""></script><script src="/_next/static/MdneM3OWGZMTOR1krVaKX/_buildManifest.js" defer=""></script><script src="/_next/static/MdneM3OWGZMTOR1krVaKX/_ssgManifest.js" defer=""></script><style data-styled="" data-styled-version="5.3.6">.iBQlkL{background:coral;height:60px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media (max-width:920px){.iBQlkL ul{display:none;}}/*!sc*/
data-styled.g1[id="sc-7295e4c-0"]{content:"iBQlkL,"}/*!sc*/
.bDMZDz{display:none;font-size:22px;margin-left:auto;}/*!sc*/
@media (max-width:920px){.bDMZDz{display:block;}}/*!sc*/
data-styled.g2[id="sc-7295e4c-1"]{content:"bDMZDz,"}/*!sc*/
html,body{height:100%;margin:0;}/*!sc*/
body{background:#fff;}/*!sc*/
html{box-sizing:border-box;font-size:16px;}/*!sc*/
*,*:before,*:after{box-sizing:inherit;}/*!sc*/
body,h1,h2,h3,h4,h5,h6,p,ol,ul{margin:0;padding:0;font-weight:normal;}/*!sc*/
ol,ul{list-style:none;}/*!sc*/
img,video{max-width:100%;height:auto;}/*!sc*/
.lg-container .lg-backdrop.in{opacity:0.75;}/*!sc*/
.lg-container .lg-toolbar.lg-group,.lg-container .lg-outer .lg-thumb-outer{background:rgba(0,0,0,0.45);}/*!sc*/
data-styled.g3[id="sc-global-cqyaiF1"]{content:"sc-global-cqyaiF1,"}/*!sc*/
.eZqfqp{display:block;margin-left:auto;margin-right:auto;}/*!sc*/
data-styled.g4[id="sc-242d5c63-0"]{content:"eZqfqp,"}/*!sc*/
.gndEvh{max-width:800px;margin:0 auto;}/*!sc*/
data-styled.g6[id="sc-6329e001-0"]{content:"gndEvh,"}/*!sc*/
</style></head><body><div id="__next"><nav class="sc-7295e4c-0 iBQlkL"><ul><a href="/blog">Blog</a></ul><button class="sc-7295e4c-1 bDMZDz">X</button></nav><main><div class="sc-6329e001-0 gndEvh"><h1>LDAvis: visualization for LDA topic modelling</h1><div class="lg-react-element "><p>Topic modelling is a great way to uncover hidden topics in a large collection of documents. The method is extremely popular in digital humanities, so it is not just about the performance, but also the explainability.</p>
<p>Among topic modelling methods, many researchers still go with LDA, a generative model that observe word frequencies in the corpus and iteratively constructs a topic model for a given number of topics.</p>
<p>Topic Modelling widget computes the LDA topic model. The widget also shows the top 10 words that describe each topic. We construct the below example of <em>grimm-tales-selected</em>, which we loaded with the <strong>Corpus</strong> widget. Then we preprocess the data with <strong>Preprocess Text</strong>, where we used the default preprocessing. The most important thing is to add <strong>Bag of Words</strong> with the TF-IDF transform. LDA works a lot better when using this transform, as it descreases the importance of overly frequent words.</p>
<a href="/blog/2022-03-ldavis/2022-03-18_topic-modelling.png" data-gallery="true"><img srcSet="/blog/2022-03-ldavis/2022-03-18_topic-modelling.png 1x, /blog/2022-03-ldavis/2022-03-18_topic-modelling.png 2x" src="/blog/2022-03-ldavis/2022-03-18_topic-modelling.png" width="992" height="442" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<p>In <strong>Topic Modelling</strong> we use the LDA method with 10 topics. Ok, the first topic is a about animal tales, even more so the second. The fourth topic is about fish and the tenth topic probably contains the Little Red Riding Hood tale. But how frequent are grandmothers in the corpus overall? Perhaps most tales talk about grandmothers, so the top word for the tenth topic is not really informative.</p>
<p>To explore the relationship between frequent and specific words in the topic, we can use LDAvis (Siever and Shirley 2014). Be careful to connect the right output to the widget. Topic Modelling outputs three things: corpus with topics, the selected topic, and all topics (topic-term matrix). We need last output for LDAvis to work.</p>
<a href="/blog/2022-03-ldavis/2022-03-18_edit-links.png" data-gallery="true"><img srcSet="/blog/2022-03-ldavis/2022-03-18_edit-links.png 1x, /blog/2022-03-ldavis/2022-03-18_edit-links.png 2x" src="/blog/2022-03-ldavis/2022-03-18_edit-links.png" width="688" height="343" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<p>LDAvis shows top ranked words for each topic, weighted by relevance. Generally, topic modelling show the top N words by the word&#x27;s topic probability. In other words, the more frequent is the word in the topic, the higher the word will be on the list.</p>
<p>Relevance defines the relationship between the word&#x27;s topic probability and its lift. Lift is the ratio of the probability of the word in the topic to the probability of the word in the corpus. The more frequent is the word in the corpus, the lower will be its lift. Hence lift will expose words that appear almost exclusively in the topic.</p>
<p>Relevance is set to 0.5 by default, meaning there will be a balance between the probability of the word in the topic and its lift. Feel free to change the value and explore how the word ranking changes.</p>
<a href="/blog/2022-03-ldavis/2022-03-18_ldavis.png" data-gallery="true"><img srcSet="/blog/2022-03-ldavis/2022-03-18_ldavis.png 1x, /blog/2022-03-ldavis/2022-03-18_ldavis.png 2x" src="/blog/2022-03-ldavis/2022-03-18_ldavis.png" width="888" height="634" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<p>Grandmother from the topic 10 actually seems like it appears almost exlusively in this topic. Wolf, for example, is also characteristic of the topic, but it is quite frequent in the corpus in general.</p>
<p>Siever and Shirley&#x27;s LDAvis has another component, which shows marginal topic frequency in an MDS projection. Connect <em>All Topics</em> output from Topic Modelling to <strong>MDS</strong>. The widget will show topics in a 2D projection, where similar topics will lie close together and different topics far apart. Note that topic similarity is based on which words a characteristic of the topic.</p>
<p>Now set the Color and Size attributes to <em>Marginal Topic Probability</em> and Label to <em>Topics</em>. Voila, this is a PCA-initiated MDS projection, where the size of the point corresponds to the marginal topic probability and the position of the point to its similarity with other topics.</p>
<a href="/blog/2022-03-ldavis/2022-03-18_mds.png" data-gallery="true"><img srcSet="/blog/2022-03-ldavis/2022-03-18_mds.png 1x, /blog/2022-03-ldavis/2022-03-18_mds.png 2x" src="/blog/2022-03-ldavis/2022-03-18_mds.png" width="1082" height="785" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<p>Topic 4, which is about the fish, seems to be quite well-represented in the corpus.</p>
<a href="/blog/2022-03-ldavis/2022-03-18_workflow.png" data-gallery="true"><img srcSet="/blog/2022-03-ldavis/2022-03-18_workflow.png 1x, /blog/2022-03-ldavis/2022-03-18_workflow.png 2x" src="/blog/2022-03-ldavis/2022-03-18_workflow.png" width="710" height="254" decoding="async" data-nimg="1" class="sc-242d5c63-0 eZqfqp" loading="lazy" style="color:transparent"/></a>
<p>This is it - a quick way to recreate LDAvis in Orange.</p>
<h4>References</h4>
<p>Sievert, C. and K. E. Shirley. 2014. &quot;LDAvis: A method for visualizing and interpreting topics.&quot; Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pp. 63–70, Baltimore, Maryland, USA.</p></div></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"author":"Ajda Pretnar Žagar","date":"2022-03-18","draft":false,"title":"LDAvis: visualization for LDA topic modelling","type":"blog","thumbImage":"2022-03-18_ldavis_small.png","frontPageImage":"2022-03-18_ldavis_small.png","blog":["text mining","topic modelling","lda","visualization"],"shortExcerpt":"Text add-on now offers a way to explain topics with LDAvis.","longExcerpt":"Text add-on recently got extended with LDAvis widget, a visualization that enables exploring word frequencies in LDA-generated topics. See how to construct LDAvis pipeline in Orange.","x2images":true},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    em: \"em\",\n    strong: \"strong\",\n    h4: \"h4\"\n  }, _provideComponents(), props.components), {WindowScreenshot} = _components;\n  if (!WindowScreenshot) _missingMdxReference(\"WindowScreenshot\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"Topic modelling is a great way to uncover hidden topics in a large collection of documents. The method is extremely popular in digital humanities, so it is not just about the performance, but also the explainability.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Among topic modelling methods, many researchers still go with LDA, a generative model that observe word frequencies in the corpus and iteratively constructs a topic model for a given number of topics.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Topic Modelling widget computes the LDA topic model. The widget also shows the top 10 words that describe each topic. We construct the below example of \", _jsx(_components.em, {\n        children: \"grimm-tales-selected\"\n      }), \", which we loaded with the \", _jsx(_components.strong, {\n        children: \"Corpus\"\n      }), \" widget. Then we preprocess the data with \", _jsx(_components.strong, {\n        children: \"Preprocess Text\"\n      }), \", where we used the default preprocessing. The most important thing is to add \", _jsx(_components.strong, {\n        children: \"Bag of Words\"\n      }), \" with the TF-IDF transform. LDA works a lot better when using this transform, as it descreases the importance of overly frequent words.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2022-03-ldavis/2022-03-18_topic-modelling.png\",\n      width: \"992\",\n      height: \"442\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"In \", _jsx(_components.strong, {\n        children: \"Topic Modelling\"\n      }), \" we use the LDA method with 10 topics. Ok, the first topic is a about animal tales, even more so the second. The fourth topic is about fish and the tenth topic probably contains the Little Red Riding Hood tale. But how frequent are grandmothers in the corpus overall? Perhaps most tales talk about grandmothers, so the top word for the tenth topic is not really informative.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"To explore the relationship between frequent and specific words in the topic, we can use LDAvis (Siever and Shirley 2014). Be careful to connect the right output to the widget. Topic Modelling outputs three things: corpus with topics, the selected topic, and all topics (topic-term matrix). We need last output for LDAvis to work.\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2022-03-ldavis/2022-03-18_edit-links.png\",\n      width: \"688\",\n      height: \"343\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"LDAvis shows top ranked words for each topic, weighted by relevance. Generally, topic modelling show the top N words by the word's topic probability. In other words, the more frequent is the word in the topic, the higher the word will be on the list.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Relevance defines the relationship between the word's topic probability and its lift. Lift is the ratio of the probability of the word in the topic to the probability of the word in the corpus. The more frequent is the word in the corpus, the lower will be its lift. Hence lift will expose words that appear almost exclusively in the topic.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Relevance is set to 0.5 by default, meaning there will be a balance between the probability of the word in the topic and its lift. Feel free to change the value and explore how the word ranking changes.\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2022-03-ldavis/2022-03-18_ldavis.png\",\n      width: \"888\",\n      height: \"634\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Grandmother from the topic 10 actually seems like it appears almost exlusively in this topic. Wolf, for example, is also characteristic of the topic, but it is quite frequent in the corpus in general.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Siever and Shirley's LDAvis has another component, which shows marginal topic frequency in an MDS projection. Connect \", _jsx(_components.em, {\n        children: \"All Topics\"\n      }), \" output from Topic Modelling to \", _jsx(_components.strong, {\n        children: \"MDS\"\n      }), \". The widget will show topics in a 2D projection, where similar topics will lie close together and different topics far apart. Note that topic similarity is based on which words a characteristic of the topic.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Now set the Color and Size attributes to \", _jsx(_components.em, {\n        children: \"Marginal Topic Probability\"\n      }), \" and Label to \", _jsx(_components.em, {\n        children: \"Topics\"\n      }), \". Voila, this is a PCA-initiated MDS projection, where the size of the point corresponds to the marginal topic probability and the position of the point to its similarity with other topics.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2022-03-ldavis/2022-03-18_mds.png\",\n      width: \"1082\",\n      height: \"785\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Topic 4, which is about the fish, seems to be quite well-represented in the corpus.\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2022-03-ldavis/2022-03-18_workflow.png\",\n      width: \"710\",\n      height: \"254\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This is it - a quick way to recreate LDAvis in Orange.\"\n    }), \"\\n\", _jsx(_components.h4, {\n      children: \"References\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Sievert, C. and K. E. Shirley. 2014. \\\"LDAvis: A method for visualizing and interpreting topics.\\\" Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pp. 63–70, Baltimore, Maryland, USA.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"ldavis-visualization-for-lda-topic-modelling"},"buildId":"MdneM3OWGZMTOR1krVaKX","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>