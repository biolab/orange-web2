<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-45fae5f2ec2ec297.js" defer=""></script><script src="/_next/static/chunks/framework-3b5a00d5d7e8d93b.js" defer=""></script><script src="/_next/static/chunks/main-abafce5311b78c60.js" defer=""></script><script src="/_next/static/chunks/pages/_app-463eea64dd46fd78.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-9737114052c0c44f.js" defer=""></script><script src="/_next/static/Hu_fAq4nVJS-wfnPiRc1Q/_buildManifest.js" defer=""></script><script src="/_next/static/Hu_fAq4nVJS-wfnPiRc1Q/_ssgManifest.js" defer=""></script></head><body><div id="__next"><nav class="navbar__Nav-sc-q11qw3-0 hZTkcw"><ul><a href="/blog">Blog</a></ul><button class="navbar__Burger-sc-q11qw3-1 cGMLJw">X</button></nav><main><div class="slug__Wrapper-sc-1xtdpf8-0 cHTstd"><h1>How to Properly Test Models</h1><p>On Monday we finished the second part of the workshop for the <a href="http://www.stat.si/StatWeb/en">Statistical Office</a> of Republic of Slovenia. The crowd was tough - these guys knew their numbers and asked many challenging questions. And we loved it!</p>
<p><img src="/blog/2017-11-how-to-properly-test-models/IMG_20171124_120523.jpg" alt=""/></p>
<p>One thing we discussed was how to properly test your model. Ok, we know never to test on the same data you&#x27;ve built your model with, but even training and testing on separate data is sometimes not enough. Say I&#x27;ve tested Naive Bayes, Logistic Regression and Tree. Sure, I can select the one that gives the best performance, but we could potentially (over)fit our model, too.</p>
<p>To account for this, we would normally split the data to 3 parts:</p>
<ol>
<li>training data for building a model</li>
<li>validation data for testing which parameters and which model to use</li>
<li>test data for estmating the accurracy of the model</li>
</ol>
<p>Let us try this in Orange. Load <em>heart-disease.tab</em> data set from <em>Browse documentation data sets</em> in File widget. We have 303 patients diagnosed with blood vessel narrowing (1) or diagnosed as healthy (0).</p>
<p><img src="/blog/2017-11-how-to-properly-test-models/Screen-Shot-2017-11-29-at-11.19.15.png" alt=""/></p>
<p>Now, we will split the data into two parts, 85% of data for training and 15% for testing. We will send the first 85% onwards to build a model.</p>
<p><img src="/blog/2017-11-how-to-properly-test-models/Screen-Shot-2017-11-29-at-10.37.18.png" alt=""/></p>
<p><img src="/blog/2017-11-how-to-properly-test-models/Screen-Shot-2017-11-29-at-10.37.24.png" alt=""/>
We sampled by a fixed proportion of data and went with 85%, which is 258 out of 303 patients.</p>
<p>We will use Naive Bayes, Logistic Regression and Tree, but you can try other models, too. This is also a place and time to try different parameters. Now we will send the models to Test &amp; Score. We used cross-validation and discovered Logistic Regression scores the highest AUC. Say this is the model and parameters we want to go with.</p>
<p><img src="/blog/2017-11-how-to-properly-test-models/Screen-Shot-2017-11-29-at-11.11.10.png" alt=""/></p>
<p><img src="/blog/2017-11-how-to-properly-test-models/Screen-Shot-2017-11-29-at-11.11.57.png" alt=""/></p>
<p>Now it is time to bring in our test data (the remaining 15%) for testing. Connect Data Sampler to Test &amp; Score once again and set the connection Remaining Data - Test Data.</p>
<p><img src="/blog/2017-11-how-to-properly-test-models/Screen-Shot-2017-11-29-at-10.38.15.png" alt=""/></p>
<p>Test &amp; Score will warn us we have test data present, but unused. Select Test on test data option and observe the results. These are now the proper scores for our models.</p>
<p><img src="/blog/2017-11-how-to-properly-test-models/Screen-Shot-2017-11-29-at-11.12.56.png" alt=""/></p>
<p><img src="/blog/2017-11-how-to-properly-test-models/Screen-Shot-2017-11-29-at-11.00.17.png" alt=""/></p>
<p>Seems like LogReg still performs well. Such procedure would normally be useful when testing a lot of models with different parameters (say +100), which you would not normally do in Orange. But it&#x27;s good to know how to do the scoring properly. Now we&#x27;re off to report on the results in Nature... ;)</p></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"author":"AJDA","date":"2017-11-29 12:26:44+00:00","draft":false,"title":"How to Properly Test Models","type":"blog","blog":["analysis","classification","education","overfitting","predictive  analytics","scoring","workshop"]},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    a: \"a\",\n    img: \"img\",\n    ol: \"ol\",\n    li: \"li\",\n    em: \"em\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.p, {\n      children: [\"On Monday we finished the second part of the workshop for the \", _jsx(_components.a, {\n        href: \"http://www.stat.si/StatWeb/en\",\n        children: \"Statistical Office\"\n      }), \" of Republic of Slovenia. The crowd was tough - these guys knew their numbers and asked many challenging questions. And we loved it!\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/blog/2017-11-how-to-properly-test-models/IMG_20171124_120523.jpg\",\n        alt: \"\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"One thing we discussed was how to properly test your model. Ok, we know never to test on the same data you've built your model with, but even training and testing on separate data is sometimes not enough. Say I've tested Naive Bayes, Logistic Regression and Tree. Sure, I can select the one that gives the best performance, but we could potentially (over)fit our model, too.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"To account for this, we would normally split the data to 3 parts:\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"training data for building a model\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"validation data for testing which parameters and which model to use\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"test data for estmating the accurracy of the model\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Let us try this in Orange. Load \", _jsx(_components.em, {\n        children: \"heart-disease.tab\"\n      }), \" data set from \", _jsx(_components.em, {\n        children: \"Browse documentation data sets\"\n      }), \" in File widget. We have 303 patients diagnosed with blood vessel narrowing (1) or diagnosed as healthy (0).\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/blog/2017-11-how-to-properly-test-models/Screen-Shot-2017-11-29-at-11.19.15.png\",\n        alt: \"\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now, we will split the data into two parts, 85% of data for training and 15% for testing. We will send the first 85% onwards to build a model.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/blog/2017-11-how-to-properly-test-models/Screen-Shot-2017-11-29-at-10.37.18.png\",\n        alt: \"\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [_jsx(_components.img, {\n        src: \"/blog/2017-11-how-to-properly-test-models/Screen-Shot-2017-11-29-at-10.37.24.png\",\n        alt: \"\"\n      }), \"\\nWe sampled by a fixed proportion of data and went with 85%, which is 258 out of 303 patients.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We will use Naive Bayes, Logistic Regression and Tree, but you can try other models, too. This is also a place and time to try different parameters. Now we will send the models to Test \u0026 Score. We used cross-validation and discovered Logistic Regression scores the highest AUC. Say this is the model and parameters we want to go with.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/blog/2017-11-how-to-properly-test-models/Screen-Shot-2017-11-29-at-11.11.10.png\",\n        alt: \"\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/blog/2017-11-how-to-properly-test-models/Screen-Shot-2017-11-29-at-11.11.57.png\",\n        alt: \"\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now it is time to bring in our test data (the remaining 15%) for testing. Connect Data Sampler to Test \u0026 Score once again and set the connection Remaining Data - Test Data.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/blog/2017-11-how-to-properly-test-models/Screen-Shot-2017-11-29-at-10.38.15.png\",\n        alt: \"\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Test \u0026 Score will warn us we have test data present, but unused. Select Test on test data option and observe the results. These are now the proper scores for our models.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/blog/2017-11-how-to-properly-test-models/Screen-Shot-2017-11-29-at-11.12.56.png\",\n        alt: \"\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/blog/2017-11-how-to-properly-test-models/Screen-Shot-2017-11-29-at-11.00.17.png\",\n        alt: \"\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Seems like LogReg still performs well. Such procedure would normally be useful when testing a lot of models with different parameters (say +100), which you would not normally do in Orange. But it's good to know how to do the scoring properly. Now we're off to report on the results in Nature... ;)\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"how-to-properly-test-models"},"buildId":"Hu_fAq4nVJS-wfnPiRc1Q","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>