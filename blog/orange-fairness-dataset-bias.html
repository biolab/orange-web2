<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="description" content="Orange Data Mining Toolbox"/><meta property="og:url" content="https://orangedatamining.com"/><meta property="og:site_name" content="Orange Data Mining"/><meta name="author" content="Bioinformatics Laboratory, University of Ljubljana"/><title>Orange Data Mining - Orange Fairness - Dataset Bias</title><meta name="robots" content="index,follow"/><meta property="og:title" content="Orange Data Mining - Orange Fairness - Dataset Bias"/><meta property="og:description" content="Orange now supports methods for detecting and mitigating bias in machine learning."/><meta property="og:type" content="article"/><meta property="article:published_time" content="2023-09-18"/><meta property="article:author" content="Žan Mervič"/><meta property="article:tag" content="fairness"/><meta property="article:tag" content="dataset bias"/><meta property="og:image" content="/blog/2023-09-fairness-dataset-bias/__webp-images__/2023-09-18-fairness-dataset-bias-thumb.webp"/><meta property="og:image:width" content="562"/><meta property="og:image:height" content="546"/><meta name="next-head-count" content="18"/><link rel="preload" href="/_next/static/media/e7c7dbb62ddcf6fa-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/148a6a0ea3cd708f.css" as="style" crossorigin=""/><link rel="stylesheet" href="/_next/static/css/148a6a0ea3cd708f.css" crossorigin="" data-n-g=""/><link rel="preload" href="/_next/static/css/4ac7ed34d61ef456.css" as="style" crossorigin=""/><link rel="stylesheet" href="/_next/static/css/4ac7ed34d61ef456.css" crossorigin="" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" crossorigin="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-21c828b96ad33382.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/framework-0c7baedefba6b077.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/main-ab94bcaa7cae018a.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/_app-4ca122263f58d1c1.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/110-f463c991b9377c97.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-ae9156694e147b3c.js" defer="" crossorigin=""></script><script src="/_next/static/hb_COGHh8rE5TbZ3YATyS/_buildManifest.js" defer="" crossorigin=""></script><script src="/_next/static/hb_COGHh8rE5TbZ3YATyS/_ssgManifest.js" defer="" crossorigin=""></script><style data-styled="" data-styled-version="5.3.6">.iSNtvJ{position:relative;max-width:1296px;margin-right:auto;margin-left:auto;padding-left:72px;padding-right:72px;height:100%;box-sizing:content-box;}/*!sc*/
@media (max-width:1130px){.iSNtvJ{padding-left:30px;padding-right:30px;}}/*!sc*/
@media (max-width:720px){.iSNtvJ{padding-left:15px;padding-right:15px;}}/*!sc*/
.fJLXpS{position:relative;max-width:1296px;margin-right:auto;margin-left:auto;padding-left:72px;padding-right:72px;height:100%;box-sizing:content-box;max-width:714px;}/*!sc*/
@media (max-width:1130px){.fJLXpS{padding-left:30px;padding-right:30px;}}/*!sc*/
@media (max-width:720px){.fJLXpS{padding-left:15px;padding-right:15px;}}/*!sc*/
data-styled.g1[id="sc-ef15f394-0"]{content:"iSNtvJ,fJLXpS,"}/*!sc*/
.jyxOJT{position:relative;display:inline-block;max-width:100%;font-size:20px;line-height:1.25;font-weight:600;-webkit-text-decoration:none;text-decoration:none;color:#fff;padding:10px 15px;border-radius:5px;background-image:linear-gradient(180deg,#FE7A00 74.93%,#F65D18 100%);box-shadow:0px 10px 10px rgba(0,0,0,0.03);background-size:100%;cursor:pointer;z-index:2;}/*!sc*/
@media (max-width:720px){.jyxOJT{font-size:18px;}}/*!sc*/
.jyxOJT:before{content:"";display:block;position:absolute;top:0;left:0;width:100%;height:100%;border-radius:inherit;background-image:linear-gradient(180deg,#ffbc44 100%,#FE7A00 100%);opacity:0;z-index:-100;-webkit-transition:opacity 0.45s;transition:opacity 0.45s;}/*!sc*/
.jyxOJT:hover{color:#fff;}/*!sc*/
.jyxOJT:hover:before{opacity:1;}/*!sc*/
data-styled.g2[id="sc-b6ea565a-0"]{content:"jyxOJT,"}/*!sc*/
.iQtqNS{background:#1F1F1F;padding:38px 0 25px;color:#fff;}/*!sc*/
data-styled.g3[id="sc-c82ca4e2-0"]{content:"iQtqNS,"}/*!sc*/
.iiCJUO{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-column-gap:80px;column-gap:80px;row-gap:40px;margin-bottom:40px;}/*!sc*/
@media (max-width:720px){.iiCJUO{display:grid;grid-template-columns:1fr 1fr;-webkit-column-gap:20px;column-gap:20px;}}/*!sc*/
.iiCJUO h3{font-size:16px;font-weight:600;margin-bottom:12px;}/*!sc*/
.iiCJUO li + li{margin-top:12px;}/*!sc*/
data-styled.g4[id="sc-c82ca4e2-1"]{content:"iiCJUO,"}/*!sc*/
.eWdALT{display:none;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:space-around;-webkit-justify-content:space-around;-ms-flex-pack:space-around;justify-content:space-around;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:44px;height:34px;padding:6px;border-radius:5px;border:none;background:linear-gradient(180deg,#FE7A00 74.93%,#F65D18 100%);box-shadow:0px 10px 10px rgba(0,0,0,0.03);cursor:pointer;margin-left:auto;}/*!sc*/
@media (max-width:920px){.eWdALT{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}/*!sc*/
.eWdALT div{width:22px;height:2px;background:#fff;-webkit-transform-origin:1px;-ms-transform-origin:1px;transform-origin:1px;-webkit-transition:all 0.3s linear;transition:all 0.3s linear;}/*!sc*/
data-styled.g5[id="sc-46d6b9d9-0"]{content:"eWdALT,"}/*!sc*/
.jTcDmc{-webkit-clip:rect(0 0 0 0);clip:rect(0 0 0 0);-webkit-clip-path:inset(50%);clip-path:inset(50%);height:1px;overflow:hidden;position:absolute;white-space:nowrap;width:1px;}/*!sc*/
data-styled.g6[id="sc-cdea7863-0"]{content:"jTcDmc,"}/*!sc*/
.gPgccM{position:fixed;top:0;left:0;width:100%;height:80px;z-index:5;background:#fff;box-shadow:0px 4px 10px 4px rgba(0,0,0,0.04);}/*!sc*/
data-styled.g7[id="sc-6852f5f6-0"]{content:"gPgccM,"}/*!sc*/
.jrLPNu{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:100%;}/*!sc*/
.jrLPNu .img-logo{width:115px;}/*!sc*/
data-styled.g8[id="sc-6852f5f6-1"]{content:"jrLPNu,"}/*!sc*/
.hzGhxs{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media (max-width:920px){.hzGhxs{display:block;position:absolute;top:80px;left:0;width:100%;z-index:1;padding:0 30px 30px;background-color:#fff;opacity:0;z-index:-9999;pointer-events:none;-webkit-transition:opacity 0.3s ease;transition:opacity 0.3s ease;box-shadow:0 4px 10px -1px rgba(0,0,0,0.04);}}/*!sc*/
@media (max-width:720px){.hzGhxs{padding:0 15px 15px;}}/*!sc*/
data-styled.g9[id="sc-6852f5f6-2"]{content:"hzGhxs,"}/*!sc*/
.chSgxF{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media (max-width:920px){.chSgxF{display:block;margin-bottom:15px;}}/*!sc*/
.chSgxF li + li{margin-left:26px;}/*!sc*/
@media (max-width:920px){.chSgxF li + li{margin-left:0;}}/*!sc*/
.chSgxF a{display:inline-block;font-size:1.25rem;line-height:1;color:#000000;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:color 0.3s;transition:color 0.3s;}/*!sc*/
.chSgxF a:hover{color:#FE7A00;}/*!sc*/
@media (max-width:920px){.chSgxF a{padding:8px 0;}}/*!sc*/
data-styled.g10[id="sc-6852f5f6-3"]{content:"chSgxF,"}/*!sc*/
.hTbjmy{position:relative;width:160px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-transition:width 0.3s ease-in-out;transition:width 0.3s ease-in-out;margin-left:26px;}/*!sc*/
@media (max-width:920px){.hTbjmy{width:auto;margin-left:0;}}/*!sc*/
data-styled.g11[id="sc-6852f5f6-4"]{content:"hTbjmy,"}/*!sc*/
.kzOElI{display:inline-block;position:absolute;top:0;right:42px;width:calc(100% - 42px);height:100%;font-size:16px;line-height:1.25;padding:10px 13px;background:#fff;border-radius:5px 0px 0px 5px;border:1px solid #D9D9D9;border-right:none;-webkit-transition:width 0.3s ease-in-out;transition:width 0.3s ease-in-out;}/*!sc*/
@media (max-width:920px){.kzOElI{display:none;}}/*!sc*/
.kzOElI::-webkit-input-placeholder{color:#D9D9D9;}/*!sc*/
.kzOElI::-moz-placeholder{color:#D9D9D9;}/*!sc*/
.kzOElI:-ms-input-placeholder{color:#D9D9D9;}/*!sc*/
.kzOElI::placeholder{color:#D9D9D9;}/*!sc*/
data-styled.g12[id="sc-6852f5f6-5"]{content:"kzOElI,"}/*!sc*/
.dSfmcG{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;color:#fff;margin-left:auto;-webkit-flex:0 0 42px;-ms-flex:0 0 42px;flex:0 0 42px;height:41px;border:1px solid #474747;border-radius:0px 5px 5px 0px;background-color:#474747;}/*!sc*/
@media (max-width:920px){.dSfmcG{margin-left:0;}}/*!sc*/
data-styled.g13[id="sc-6852f5f6-6"]{content:"dSfmcG,"}/*!sc*/
.eBQFeB{min-height:100vh;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
data-styled.g18[id="sc-da66f971-0"]{content:"eBQFeB,"}/*!sc*/
*,*:before,*:after{box-sizing:border-box;-webkit-font-smoothing:antialiased;}/*!sc*/
html,body{height:100%;margin:0;}/*!sc*/
html{font-family:'Source Sans Pro',sans-serif;font-weight:400;color:#000000;}/*!sc*/
body{background:#fff;}/*!sc*/
body,h1,h2,h3,h4,h5,h6,p,ol,ul{margin:0;padding:0;font-weight:normal;}/*!sc*/
main{padding-top:80px;-webkit-flex:1;-ms-flex:1;flex:1;}/*!sc*/
ol,ul{list-style:none;}/*!sc*/
figure{margin:0;}/*!sc*/
img,video{display:block;max-width:100%;height:auto;}/*!sc*/
a{color:unset;-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
a:hover{color:unset;}/*!sc*/
strong{font-weight:600;}/*!sc*/
pre{display:block;padding:10px 14px;margin:0 0 10px;font-size:18px;line-height:1.42857143;word-break:break-all;word-wrap:break-word;color:#000;background-color:#f5f5f5;border-radius:5px;overflow-x:auto;word-wrap:normal;}/*!sc*/
pre code{padding:0;font-size:inherit;color:inherit;white-space:pre-wrap;background-color:transparent;border-radius:0;}/*!sc*/
code{font-family:Menlo,Monaco,Consolas,"Courier New",monospace;padding:2px 4px;font-size:90%;color:#c7254e;background-color:#f9f2f4;white-space:nowrap;border-radius:4px;white-space:pre-wrap;}/*!sc*/
::selection{background-color:#FE7A00;color:#fff;}/*!sc*/
.lg-container .lg-backdrop.in{opacity:0.75;}/*!sc*/
.lg-container .lg-toolbar.lg-group,.lg-container .lg-outer .lg-thumb-outer{background:rgba(0,0,0,0.45);}/*!sc*/
data-styled.g19[id="sc-global-gFJXlm1"]{content:"sc-global-gFJXlm1,"}/*!sc*/
.cXdfZy{font-size:62px;line-height:1.04;font-weight:700;color:#fff;color:#1F1F1F;}/*!sc*/
@media (max-width:920px){.cXdfZy{font-size:50px;}}/*!sc*/
@media (max-width:720px){.cXdfZy{font-size:42px;}}/*!sc*/
data-styled.g20[id="sc-4d34df46-0"]{content:"cXdfZy,"}/*!sc*/
.htQmXI h1{font-size:44px;line-height:1.13;font-weight:700;color:#1F1F1F;}/*!sc*/
@media (max-width:920px){.htQmXI h1{font-size:38px;}}/*!sc*/
@media (max-width:720px){.htQmXI h1{font-size:32px;}}/*!sc*/
.htQmXI h2{font-size:33px;line-height:1.13;font-weight:600;color:#1F1F1F;}/*!sc*/
@media (max-width:920px){.htQmXI h2{font-size:30px;}}/*!sc*/
@media (max-width:720px){.htQmXI h2{font-size:26px;}}/*!sc*/
.htQmXI h3{font-size:28px;line-height:1.18;font-weight:600;color:#1F1F1F;}/*!sc*/
@media (max-width:920px){.htQmXI h3{font-size:26px;}}/*!sc*/
@media (max-width:720px){.htQmXI h3{font-size:24px;}}/*!sc*/
.htQmXI h4{font-size:22px;line-height:1.18;font-weight:600;color:#1F1F1F;}/*!sc*/
@media (max-width:720px){.htQmXI h4{font-size:20px;}}/*!sc*/
.htQmXI p{font-size:22px;line-height:1.36;color:#1F1F1F;}/*!sc*/
@media (max-width:1130px){.htQmXI p{font-size:20px;}}/*!sc*/
@media (max-width:920px){.htQmXI p{font-size:18px;}}/*!sc*/
.htQmXI ul,.htQmXI ol{padding-left:40px;}/*!sc*/
.htQmXI ul li,.htQmXI ol li{font-size:22px;line-height:1.36;color:#1F1F1F;}/*!sc*/
@media (max-width:1130px){.htQmXI ul li,.htQmXI ol li{font-size:20px;}}/*!sc*/
@media (max-width:920px){.htQmXI ul li,.htQmXI ol li{font-size:18px;}}/*!sc*/
.htQmXI ul li + li,.htQmXI ol li + li{margin-top:4px;}/*!sc*/
.htQmXI ul{list-style:disc;}/*!sc*/
.htQmXI ol{list-style:decimal;}/*!sc*/
.htQmXI p a,.htQmXI li a{color:#FE7A00;}/*!sc*/
.htQmXI p a:hover,.htQmXI li a:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.htQmXI * + *:not(li,a){margin-top:15px;}/*!sc*/
.htQmXI * + a[data-gallery],.htQmXI * + video{margin-top:40px;}/*!sc*/
.htQmXI a[data-gallery]{display:block;margin-bottom:40px;}/*!sc*/
.htQmXI iframe,.htQmXI video{margin-bottom:40px;}/*!sc*/
data-styled.g27[id="sc-b0f3732d-0"]{content:"htQmXI,"}/*!sc*/
.fdCCNC{display:block;margin-left:auto;margin-right:auto;}/*!sc*/
data-styled.g28[id="sc-b0f3732d-1"]{content:"fdCCNC,"}/*!sc*/
.wRRdf{padding:80px 0;}/*!sc*/
@media (max-width:920px){.wRRdf{padding:60px 0;}}/*!sc*/
@media (max-width:720px){.wRRdf{padding:40px 0;}}/*!sc*/
data-styled.g30[id="sc-2d40882b-0"]{content:"wRRdf,"}/*!sc*/
.bMcRdu{margin-bottom:30px;}/*!sc*/
data-styled.g31[id="sc-2d40882b-1"]{content:"bMcRdu,"}/*!sc*/
.fZuXtn{position:relative;}/*!sc*/
data-styled.g32[id="sc-2d40882b-2"]{content:"fZuXtn,"}/*!sc*/
.hlBQLj{position:absolute;top:0;right:100%;width:200px;margin-right:50px;text-align:right;}/*!sc*/
@media (max-width:1130px){.hlBQLj{position:unset;width:auto;text-align:left;margin:0 0 30px 0;}}/*!sc*/
data-styled.g33[id="sc-2d40882b-3"]{content:"hlBQLj,"}/*!sc*/
.jOGEdN{font-size:20px;line-height:1.25;color:#1F1F1F;text-transform:capitalize;color:#5651EC;}/*!sc*/
@media (max-width:920px){.jOGEdN{font-size:18px;}}/*!sc*/
.GbnzW{font-size:20px;line-height:1.25;color:#1F1F1F;}/*!sc*/
@media (max-width:920px){.GbnzW{font-size:18px;}}/*!sc*/
data-styled.g34[id="sc-2d40882b-4"]{content:"jOGEdN,GbnzW,"}/*!sc*/
</style></head><body><div id="__next"><div class="sc-da66f971-0 eBQFeB __className_667be3"><nav class="sc-6852f5f6-0 gPgccM"><div class="sc-ef15f394-0 iSNtvJ"><div class="sc-6852f5f6-1 jrLPNu"><a href="/"><img alt="Orange Logo" loading="lazy" width="120" height="35" decoding="async" data-nimg="1" class="img-logo" style="color:transparent" src="/_next/static/media/logo-orange.faff1861.svg"/></a><div><button aria-label="Toggle navigation" class="sc-46d6b9d9-0 eWdALT"><div></div><div></div><div></div></button><div class="sc-6852f5f6-2 hzGhxs"><ul class="sc-6852f5f6-3 chSgxF"><li><a href="/examples">Examples</a></li><li><a href="/download">Download</a></li><li><a href="/blog">Blog</a></li><li><a href="/docs">Docs</a></li><li><a href="/workshops">Workshops</a></li></ul><form class="sc-6852f5f6-4 hTbjmy"><div class="sc-6852f5f6-5 kzOElI">Search</div><button class="sc-6852f5f6-6 dSfmcG"><img alt="Icon for search" loading="lazy" width="18" height="17" decoding="async" data-nimg="1" style="color:transparent" src="/_next/static/media/icon-search.459b2665.svg"/><span class="sc-cdea7863-0 jTcDmc">Search through page</span></button></form></div></div></div></div></nav><main><div class="sc-2d40882b-0 wRRdf"><div class="sc-ef15f394-0 fJLXpS"><div class="sc-2d40882b-1 bMcRdu"><p class="sc-2d40882b-4 jOGEdN"><strong>fairness, dataset bias</strong></p><h1 class="sc-4d34df46-0 cXdfZy">Orange Fairness - Dataset Bias</h1></div><div class="sc-2d40882b-2 fZuXtn"><div class="sc-2d40882b-3 hlBQLj"><p class="sc-2d40882b-4 GbnzW"><strong>Žan Mervič</strong></p><p class="sc-2d40882b-4 GbnzW">Sep 18, 2023</p></div><div class="sc-b0f3732d-0 htQmXI"><div class="lg-react-element "><h2>Fairness in Machine Learning</h2>
<p>Artificial intelligence and machine learning are increasingly used in everyday decisions that deeply impact individuals. This includes areas like employment, court sentencing, and credit application approvals. It&#x27;s crucial that these decision-making tools don&#x27;t favor certain demographic groups over others.</p>
<p>Bias in machine learning can stem from multiple sources. It might arise from skewed training data or the model&#x27;s design. This bias can result in unequal model outcomes: unprivileged groups might receive lower salaries despite having similar qualifications as privileged ones or face longer prison sentences for identical crimes. Such biases can significantly affect individuals&#x27; lives.</p>
<p>For example, in the &quot;Adult&quot; dataset, a notable bias exists between male and female genders. When predicting if a person&#x27;s salary is &quot;&gt; 50K&quot; or &quot;≤ 50K&quot;, females tend to be classified as &quot;≤ 50K&quot; more frequently than their male counterparts with similar attributes. This disparity showcases the dataset&#x27;s bias, which can lead to biased and unfair predictions.</p>
<h3>But why is it a problem to have models that reflect real-world bias?</h3>
<p>In essence, machine learning models that merely mimic real-world biases perpetuate those biases, resulting in a vicious cycle. For instance, if we were to train a model on a dataset reflecting gender wage disparities and then use this model to automate salary decisions, we would reinforce the same disparity. This is not about denying the reality of the dataset but recognizing that our goal in AI and machine learning should be to achieve fairer, more equitable outcomes. If models are only used to replicate existing societal conditions, then we miss the opportunity for technology to be a force for positive change.</p>
<p>Furthermore, creating models that actively seek to reduce or eliminate bias is essential for maintaining trust in AI and machine learning systems. When people see these systems merely replicating past biases, they are less likely to trust and adopt them. In contrast, fairness-aware models can be trusted to make decisions that prioritize justice and equity over merely echoing the patterns in the data.</p>
<p>Recognizing these challenges, there has been a surge in efforts to mitigate this bias. The outcome has been the development of models that aim for balanced outcomes for all demographic groups or maintain consistent true positive rates across them. Mitigating bias might mean a slight dip in overall accuracy, but it is a necessary trade-off to ensure fairness in our models. Using fairness algorithms is ethical and highly beneficial to ensure people&#x27;s trust in machine learning and AI.</p>
<h2>Introducing the Fairness add-on in Orange</h2>
<p>Hopefully, the above has emphasized the importance of fairness in machine learning. With this in mind, let us discuss the new Orange fairness add-on. It aims to empower users with tools to detect and mitigate bias in machine-learning workflows. At the time of writing, the add-on includes seven widgets, four of which are used for bias detection and mitigation. The remaining three are used to support the other widgets and provide additional functionality.</p>
<p>In this blog and the following ones, we will familiarize ourselves with the add-on widgets and their use cases. We will also demonstrate how to use them in real-world scenarios and explain relevant concepts. Let us take a look at the first two widgets in more detail.</p>
<h2>Exploring the Widgets: Dataset Bias and As Fairness</h2>
<p>This blog will introduce the first two fairness widgets: As Fairness Data and Dataset Bias. Both aim to address the different bias identification and correction stages in machine learning workflows.</p>
<h4>1. As Fairness Widget:</h4>
<p>The As Fairness Widget does not do anything about fairness on its own. Instead, it allows users to designate fairness attributes in the dataset, which are essential for other fairness algorithms to operate.</p>
<p>When inputting a dataset into the widget, options for three attributes will appear:</p>
<ul>
<li>
<p>Favorable Class Value: Define which class value is viewed as favorable.</p>
</li>
<li>
<p>Protected Attribute: Select the dataset feature representing or containing potentially biased groups or those for which fair predictions are desired, such as race, gender, or age.</p>
</li>
<li>
<p>Privileged Protected Attribute Values: Specify values within the protected attribute deemed privileged.</p>
</li>
</ul>
<a href="/blog/2023-09-fairness-dataset-bias/__webp-images__/2023-09-18-fairness-dataset-bias-as-fairness-data.webp" data-gallery="true"><img alt="" loading="lazy" width="408" height="357" decoding="async" data-nimg="1" class="sc-b0f3732d-1 fdCCNC" style="color:transparent" src="/blog/2023-09-fairness-dataset-bias/__webp-images__/2023-09-18-fairness-dataset-bias-as-fairness-data.webp"/></a>
<h4>2. Dataset Bias Widget:</h4>
<p>Before training a model, it is crucial to understand if the data itself is skewed. The Dataset Bias widget aids in precisely that. After giving it a dataset as input, the widget will calculate two fairness metrics according to the dataset:</p>
<ul>
<li>Disparate Impact (DI): Measures the ratio of favorable outcomes for an unprivileged group to that of the privileged group. An ideal value of 1.0 means the ratio of favorable outcomes is the same for both groups.<!-- -->
<ul>
<li>DI &lt; 1.0: The privileged group receives favorable outcomes at a higher rate.</li>
<li>DI &gt; 1.0: The privileged group receives favorable outcomes at a lower rate.</li>
</ul>
</li>
<li>Statistical Parity Difference (SPD): This is very similar to disparate impact. Instead of the ratio, it measures the difference in favorable outcomes between the unprivileged and the privileged groups. An ideal value for this metric is 0.<!-- -->
<ul>
<li>SPD &lt; 0: The privileged group has a higher rate of favorable outcomes.</li>
<li>SPD &gt; 0: The privileged group has a lower rate of favorable outcomes.</li>
</ul>
</li>
</ul>
<a href="/blog/2023-09-fairness-dataset-bias/__webp-images__/2023-09-18-fairness-dataset-bias.webp" data-gallery="true"><img alt="" loading="lazy" width="303" height="167" decoding="async" data-nimg="1" class="sc-b0f3732d-1 fdCCNC" style="color:transparent" src="/blog/2023-09-fairness-dataset-bias/__webp-images__/2023-09-18-fairness-dataset-bias.webp"/></a>
<p>For anyone interested in learning more about these metrics, other fairness metrics and ai fairness in general, we recommend reading the article <a href="https://arxiv.org/pdf/1908.09635.pdf">A Survey on Bias and Fairness in Machine Learning</a>.</p>
<h2>Orange use case</h2>
<p>Now that we understand the functions of the As Fairness and Dataset Bias widgets, let us examine their application in a real-world scenario.</p>
<p>For this example, we will utilize the <a href="https://archive.ics.uci.edu/ml/datasets/adult">Adult dataset</a>, which is among the most popular in the machine learning community. It consists of 48824 instances with 15 attributes describing demographic details from the 1994 census. These attributes include six numerical and seven categorical factors such as age, employment type, education, marital status, occupation, race, gender, capital gain, capital loss, weekly working hours, and native country. The primary task is to predict if an individual earns more than $50,000 annually. This dataset&#x27;s most commonly protected attributes are gender, age, and race.</p>
<p>Using the As Fairness Data widget, we will define &quot;sex&quot; as the protected attribute and &quot;male&quot; as the privileged, protected attribute value. Subsequently, we will employ the Dataset Bias widget to compute the fairness metrics and a Box Plot for visualization.</p>
<a href="/blog/2023-09-fairness-dataset-bias/__webp-images__/2023-09-18-fairness-dataset-bias-use-case.webp" data-gallery="true"><img alt="" loading="lazy" width="947" height="291" decoding="async" data-nimg="1" class="sc-b0f3732d-1 fdCCNC" style="color:transparent" src="/blog/2023-09-fairness-dataset-bias/__webp-images__/2023-09-18-fairness-dataset-bias-use-case.webp"/></a>
<p>The results show that the dataset exhibits bias: the disparate impact is 0.358, and the statistical parity difference stands at -0.196. In an ideal scenario signifying no bias, these values would be 1 and 0, respectively. We can further visualize the dataset&#x27;s bias using the Box Plot.</p>
<a href="/blog/2023-09-fairness-dataset-bias/__webp-images__/2023-09-18-fairness-dataset-bias-box-plot.webp" data-gallery="true"><img alt="" loading="lazy" width="1038" height="473" decoding="async" data-nimg="1" class="sc-b0f3732d-1 fdCCNC" style="color:transparent" src="/blog/2023-09-fairness-dataset-bias/__webp-images__/2023-09-18-fairness-dataset-bias-box-plot.webp"/></a>
<p>The Box Plot illustrates the distribution of the target variable across both privileged and unprivileged groups. Hovering over the plot reveals that only 10.95% of the females in the dataset belong to the favorable class, in contrast to the 30.57% of males. By dividing the percentage of the favorable class for the unprivileged group by that of the privileged group, we arrive at a disparate impact value of 0.358—precisely what the Dataset Bias widget determined. We can do a similar calculation for the statistical parity difference.</p></div></div></div></div></div></main><footer class="sc-c82ca4e2-0 iQtqNS"><div class="sc-ef15f394-0 iSNtvJ"><div class="sc-c82ca4e2-1 iiCJUO"><div><h3>Orange</h3><ul><li><a href="/faq">FAQ</a></li><li><a href="/license">License</a></li><li><a href="/privacy">Privacy</a></li><li><a href="/citation">Citation</a></li><li><a href="/contact">Contact</a></li></ul></div><div><h3>Download</h3><ul><li><a href="/download#win">Windows</a></li><li><a href="/download#mac">Mac OS</a></li></ul></div><div><h3>Community</h3><ul><li><a href="https://twitter.com/OrangeDataMiner">Twitter</a></li><li><a href="https://www.facebook.com/orangedatamining">Facebook</a></li><li><a href="https://datascience.stackexchange.com/questions/tagged/orange">Stack Exchange</a></li><li><a href="https://www.youtube.com/channel/UClKKWBe2SCAEyv7ZNGhIe4g">YouTube</a></li><li><a href="https://discord.com/invite/FWrfeXV">Discord</a></li></ul></div><div><h3>Documentation</h3><ul><li><a href="/getting-started">Get started</a></li><li><a href="/widget-catalogue">Widgets</a></li><li><a href="https://orange3.readthedocs.io/projects/orange-data-mining-library/en/latest/">Scripting</a></li></ul></div><div><h3>Developers</h3><ul><li><a href="https://github.com/biolab/orange3">GitHub</a></li><li><a href="http://docs.biolab.si/3/development/">Getting started</a></li></ul></div><div><a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=A76TAX87ZVR3J" target="_blank" rel="noreferrer" class="sc-b6ea565a-0 jyxOJT">Donate to Orange</a></div></div><p>Copyright © University of Ljubljana</p></div></footer></div></div><script id="__NEXT_DATA__" type="application/json" crossorigin="">{"props":{"pageProps":{"frontmatter":{"author":"Žan Mervič","date":"2023-09-18","draft":false,"title":"Orange Fairness - Dataset Bias","thumbImage":"2023-09-18-fairness-dataset-bias-thumb.png","frontPageImage":"2023-09-18-fairness-dataset-bias-thumb.png","blog":["fairness","dataset bias"],"shortExcerpt":"Orange now supports methods for detecting and mitigating bias in machine learning.","longExcerpt":"In an era where AI drives decisions impacting real lives, fairness in machine learning is paramount. Take the `Adult` dataset, which shows discrepancies in salary predictions based on gender. Addressing such concerns, Orange introduces a fairness add-on. Using new widgets, users can identify and mitigate biases in their datasets or model predictions.","oldUrl":"/blog/2023/2023-09-18-fairness-dataset-bias/"},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    h2: \"h2\",\n    p: \"p\",\n    h3: \"h3\",\n    h4: \"h4\",\n    ul: \"ul\",\n    li: \"li\",\n    a: \"a\"\n  }, _provideComponents(), props.components), {WindowScreenshot} = _components;\n  if (!WindowScreenshot) _missingMdxReference(\"WindowScreenshot\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.h2, {\n      children: \"Fairness in Machine Learning\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Artificial intelligence and machine learning are increasingly used in everyday decisions that deeply impact individuals. This includes areas like employment, court sentencing, and credit application approvals. It's crucial that these decision-making tools don't favor certain demographic groups over others.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Bias in machine learning can stem from multiple sources. It might arise from skewed training data or the model's design. This bias can result in unequal model outcomes: unprivileged groups might receive lower salaries despite having similar qualifications as privileged ones or face longer prison sentences for identical crimes. Such biases can significantly affect individuals' lives.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"For example, in the \\\"Adult\\\" dataset, a notable bias exists between male and female genders. When predicting if a person's salary is \\\"\u003e 50K\\\" or \\\"≤ 50K\\\", females tend to be classified as \\\"≤ 50K\\\" more frequently than their male counterparts with similar attributes. This disparity showcases the dataset's bias, which can lead to biased and unfair predictions.\"\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"But why is it a problem to have models that reflect real-world bias?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"In essence, machine learning models that merely mimic real-world biases perpetuate those biases, resulting in a vicious cycle. For instance, if we were to train a model on a dataset reflecting gender wage disparities and then use this model to automate salary decisions, we would reinforce the same disparity. This is not about denying the reality of the dataset but recognizing that our goal in AI and machine learning should be to achieve fairer, more equitable outcomes. If models are only used to replicate existing societal conditions, then we miss the opportunity for technology to be a force for positive change.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Furthermore, creating models that actively seek to reduce or eliminate bias is essential for maintaining trust in AI and machine learning systems. When people see these systems merely replicating past biases, they are less likely to trust and adopt them. In contrast, fairness-aware models can be trusted to make decisions that prioritize justice and equity over merely echoing the patterns in the data.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Recognizing these challenges, there has been a surge in efforts to mitigate this bias. The outcome has been the development of models that aim for balanced outcomes for all demographic groups or maintain consistent true positive rates across them. Mitigating bias might mean a slight dip in overall accuracy, but it is a necessary trade-off to ensure fairness in our models. Using fairness algorithms is ethical and highly beneficial to ensure people's trust in machine learning and AI.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Introducing the Fairness add-on in Orange\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Hopefully, the above has emphasized the importance of fairness in machine learning. With this in mind, let us discuss the new Orange fairness add-on. It aims to empower users with tools to detect and mitigate bias in machine-learning workflows. At the time of writing, the add-on includes seven widgets, four of which are used for bias detection and mitigation. The remaining three are used to support the other widgets and provide additional functionality.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"In this blog and the following ones, we will familiarize ourselves with the add-on widgets and their use cases. We will also demonstrate how to use them in real-world scenarios and explain relevant concepts. Let us take a look at the first two widgets in more detail.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Exploring the Widgets: Dataset Bias and As Fairness\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This blog will introduce the first two fairness widgets: As Fairness Data and Dataset Bias. Both aim to address the different bias identification and correction stages in machine learning workflows.\"\n    }), \"\\n\", _jsx(_components.h4, {\n      children: \"1. As Fairness Widget:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The As Fairness Widget does not do anything about fairness on its own. Instead, it allows users to designate fairness attributes in the dataset, which are essential for other fairness algorithms to operate.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When inputting a dataset into the widget, options for three attributes will appear:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"Favorable Class Value: Define which class value is viewed as favorable.\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"Protected Attribute: Select the dataset feature representing or containing potentially biased groups or those for which fair predictions are desired, such as race, gender, or age.\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"Privileged Protected Attribute Values: Specify values within the protected attribute deemed privileged.\"\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2023-09-fairness-dataset-bias/2023-09-18-fairness-dataset-bias-as-fairness-data.png\",\n      width: \"408\",\n      height: \"357\",\n      src: \"/blog/2023-09-fairness-dataset-bias/__webp-images__/2023-09-18-fairness-dataset-bias-as-fairness-data.webp\"\n    }), \"\\n\", _jsx(_components.h4, {\n      children: \"2. Dataset Bias Widget:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Before training a model, it is crucial to understand if the data itself is skewed. The Dataset Bias widget aids in precisely that. After giving it a dataset as input, the widget will calculate two fairness metrics according to the dataset:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"Disparate Impact (DI): Measures the ratio of favorable outcomes for an unprivileged group to that of the privileged group. An ideal value of 1.0 means the ratio of favorable outcomes is the same for both groups.\", \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsx(_components.li, {\n            children: \"DI \u003c 1.0: The privileged group receives favorable outcomes at a higher rate.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"DI \u003e 1.0: The privileged group receives favorable outcomes at a lower rate.\"\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"Statistical Parity Difference (SPD): This is very similar to disparate impact. Instead of the ratio, it measures the difference in favorable outcomes between the unprivileged and the privileged groups. An ideal value for this metric is 0.\", \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsx(_components.li, {\n            children: \"SPD \u003c 0: The privileged group has a higher rate of favorable outcomes.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"SPD \u003e 0: The privileged group has a lower rate of favorable outcomes.\"\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2023-09-fairness-dataset-bias/2023-09-18-fairness-dataset-bias.png\",\n      width: \"303\",\n      height: \"167\",\n      src: \"/blog/2023-09-fairness-dataset-bias/__webp-images__/2023-09-18-fairness-dataset-bias.webp\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"For anyone interested in learning more about these metrics, other fairness metrics and ai fairness in general, we recommend reading the article \", _jsx(_components.a, {\n        href: \"https://arxiv.org/pdf/1908.09635.pdf\",\n        children: \"A Survey on Bias and Fairness in Machine Learning\"\n      }), \".\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Orange use case\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now that we understand the functions of the As Fairness and Dataset Bias widgets, let us examine their application in a real-world scenario.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"For this example, we will utilize the \", _jsx(_components.a, {\n        href: \"https://archive.ics.uci.edu/ml/datasets/adult\",\n        children: \"Adult dataset\"\n      }), \", which is among the most popular in the machine learning community. It consists of 48824 instances with 15 attributes describing demographic details from the 1994 census. These attributes include six numerical and seven categorical factors such as age, employment type, education, marital status, occupation, race, gender, capital gain, capital loss, weekly working hours, and native country. The primary task is to predict if an individual earns more than $50,000 annually. This dataset's most commonly protected attributes are gender, age, and race.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Using the As Fairness Data widget, we will define \\\"sex\\\" as the protected attribute and \\\"male\\\" as the privileged, protected attribute value. Subsequently, we will employ the Dataset Bias widget to compute the fairness metrics and a Box Plot for visualization.\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2023-09-fairness-dataset-bias/2023-09-18-fairness-dataset-bias-use-case.png\",\n      width: \"947\",\n      height: \"291\",\n      src: \"/blog/2023-09-fairness-dataset-bias/__webp-images__/2023-09-18-fairness-dataset-bias-use-case.webp\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The results show that the dataset exhibits bias: the disparate impact is 0.358, and the statistical parity difference stands at -0.196. In an ideal scenario signifying no bias, these values would be 1 and 0, respectively. We can further visualize the dataset's bias using the Box Plot.\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2023-09-fairness-dataset-bias/2023-09-18-fairness-dataset-bias-box-plot.png\",\n      width: \"1038\",\n      height: \"473\",\n      src: \"/blog/2023-09-fairness-dataset-bias/__webp-images__/2023-09-18-fairness-dataset-bias-box-plot.webp\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The Box Plot illustrates the distribution of the target variable across both privileged and unprivileged groups. Hovering over the plot reveals that only 10.95% of the females in the dataset belong to the favorable class, in contrast to the 30.57% of males. By dividing the percentage of the favorable class for the unprivileged group by that of the privileged group, we arrive at a disparate impact value of 0.358—precisely what the Dataset Bias widget determined. We can do a similar calculation for the statistical parity difference.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{}},"thumbImage":{"width":562,"height":546,"src":"/blog/2023-09-fairness-dataset-bias/__webp-images__/2023-09-18-fairness-dataset-bias-thumb.webp"}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"orange-fairness-dataset-bias"},"buildId":"hb_COGHh8rE5TbZ3YATyS","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>