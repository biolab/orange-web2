<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>My new cool app</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/4ac7ed34d61ef456.css" as="style"/><link rel="stylesheet" href="/_next/static/css/4ac7ed34d61ef456.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-d38be8d96a62f950.js" defer=""></script><script src="/_next/static/chunks/framework-3b5a00d5d7e8d93b.js" defer=""></script><script src="/_next/static/chunks/main-9f90a364d66949ce.js" defer=""></script><script src="/_next/static/chunks/pages/_app-8cfbd06f8ddd0b49.js" defer=""></script><script src="/_next/static/chunks/675-262430aa11afdf01.js" defer=""></script><script src="/_next/static/chunks/9-9d15d34c7affe00b.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-ba4a3f289f8ef3c5.js" defer=""></script><script src="/_next/static/t3FkTJiCx_jp9wzVJgeAY/_buildManifest.js" defer=""></script><script src="/_next/static/t3FkTJiCx_jp9wzVJgeAY/_ssgManifest.js" defer=""></script><style data-styled="" data-styled-version="5.3.6">.iBQlkL{background:coral;height:60px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media (max-width:920px){.iBQlkL ul{display:none;}}/*!sc*/
data-styled.g1[id="sc-7295e4c-0"]{content:"iBQlkL,"}/*!sc*/
.bDMZDz{display:none;font-size:22px;margin-left:auto;}/*!sc*/
@media (max-width:920px){.bDMZDz{display:block;}}/*!sc*/
data-styled.g2[id="sc-7295e4c-1"]{content:"bDMZDz,"}/*!sc*/
html,body{height:100%;margin:0;}/*!sc*/
body{background:#fff;}/*!sc*/
html{box-sizing:border-box;font-size:16px;}/*!sc*/
*,*:before,*:after{box-sizing:inherit;}/*!sc*/
body,h1,h2,h3,h4,h5,h6,p,ol,ul{margin:0;padding:0;font-weight:normal;}/*!sc*/
ol,ul{list-style:none;}/*!sc*/
img,video{max-width:100%;height:auto;}/*!sc*/
.lg-container .lg-backdrop.in{opacity:0.75;}/*!sc*/
.lg-container .lg-toolbar.lg-group,.lg-container .lg-outer .lg-thumb-outer{background:rgba(0,0,0,0.45);}/*!sc*/
data-styled.g3[id="sc-global-cqyaiF1"]{content:"sc-global-cqyaiF1,"}/*!sc*/
.cGDPDa{max-width:800px;margin:0 auto;}/*!sc*/
data-styled.g6[id="sc-f0e62130-0"]{content:"cGDPDa,"}/*!sc*/
</style></head><body><div id="__next"><nav class="sc-7295e4c-0 iBQlkL"><ul><a href="/blog">Blog</a></ul><button class="sc-7295e4c-1 bDMZDz">X</button></nav><main><div class="sc-f0e62130-0 cGDPDa"><h1>Text Mining: version 0.2.0</h1><div class="lg-react-element "><p><a href="https://github.com/biolab/orange3-text">Orange3-Text</a> has just recently been polished, updated and enhanced! Our GSoC student Alexey has <a href="/blog/2016/07/05/rehaul-of-text-mining-add-on/">helped us greatly</a> to achieve another milestone in Orange development and release the latest 0.2.0 version of our text mining add-on. The new release, which is already available on PyPi, includes Wikipedia and SimHash widgets and a rehaul of Bag of Words, Topic Modeling and Corpus Viewer.</p>
<p>Wikipedia widget allows retrieving sources from Wikipedia API and can handle multiple queries. It serves as an easy data gathering source and it&#x27;s great for exploring text mining techniques. Here we&#x27;ve simply queried Wikipedia for articles on Slovenia and Germany and displayed them in Corpus Viewer.</p>
<a href="/blog/2016-09-text-mining-version-0-2-0/__webp-images__/wiki1-1.webp" data-gallery="true"><img alt="" srcSet="/blog/2016-09-text-mining-version-0-2-0/__webp-images__/wiki1-1.webp 1x, /blog/2016-09-text-mining-version-0-2-0/__webp-images__/wiki1-1.webp 2x" src="/blog/2016-09-text-mining-version-0-2-0/__webp-images__/wiki1-1.webp" width="1341" height="726" decoding="async" data-nimg="1" loading="lazy" style="color:transparent"/></a>
<p>Query Wikipedia by entering your query word list in the widget. Put each query on a separate line and run Search.</p>
<p><a href="http://matpalm.com/resemblance/simhash/">Similarity Hashing</a> widget computes similarity hashes for the given corpus, allowing the user to find duplicates, plagiarism or textual borrowing in the corpus. Here&#x27;s an example from Wikipedia, which has a pre-defined structure of articles, making our corpus quite similar. We&#x27;ve used Wikipedia widget and retrieved 10 articles for the query &#x27;Slovenia&#x27;. Then we&#x27;ve used Similarity Hashing to compute hashes for our text. What we got on the output is a table of 64 binary features (predefined in the SimHash widget), which denote a 64-bit hash size. Then we computed similarities in text by sending Similarity Hashing to Distances. Here we&#x27;ve selected cosine row distances and sent the output to Hierarchical Clustering. We can see that we have some similar documents, so we can select and inspect them in Corpus Viewer.</p>
<a href="/blog/2016-09-text-mining-version-0-2-0/__webp-images__/SimHash1.webp" data-gallery="true"><img alt="" srcSet="/blog/2016-09-text-mining-version-0-2-0/__webp-images__/SimHash1.webp 1x, /blog/2016-09-text-mining-version-0-2-0/__webp-images__/SimHash1.webp 2x" src="/blog/2016-09-text-mining-version-0-2-0/__webp-images__/SimHash1.webp" width="956" height="406" decoding="async" data-nimg="1" loading="lazy" style="color:transparent"/></a>
<p>Output of Similarity Hashing widget.</p>
<a href="/blog/2016-09-text-mining-version-0-2-0/__webp-images__/SimHash.webp" data-gallery="true"><img alt="" srcSet="/blog/2016-09-text-mining-version-0-2-0/__webp-images__/SimHash.webp 1x, /blog/2016-09-text-mining-version-0-2-0/__webp-images__/SimHash.webp 2x" src="/blog/2016-09-text-mining-version-0-2-0/__webp-images__/SimHash.webp" width="1616" height="912" decoding="async" data-nimg="1" loading="lazy" style="color:transparent"/></a>
<p>We&#x27;ve selected the two most similar documents in Hierarchical Clustering and displayed them in Corpus Viewer.</p>
<p>Topic Modeling now includes three modeling algorithms, namely <a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">Latent Semantic Indexing (LSP)</a>, <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Latent Dirichlet Allocation (LDA)</a>, and <a href="https://en.wikipedia.org/wiki/Hierarchical_Dirichlet_process">Hierarchical Dirichlet Process (HDP)</a>. Let&#x27;s query Twitter for the latest tweets from Hillary Clinton and Donald Trump. First we preprocess the data and send the output to Topic Modeling. The widget suggests 10 topics, with the most significant words denoting each topic, and outputs topic probabilities for each document.</p>
<p>We can inspect distances between the topics with Distances (cosine) and Hierarchical Clustering. Seems like topics are not extremely author specific, since Hierarchical Clustering often puts Trump and Clinton in the same cluster. We&#x27;ve used Average linkage, but you can play around with different linkages and see if you can get better results.</p>
<a href="/blog/2016-09-text-mining-version-0-2-0/__webp-images__/Topic-Modelling.webp" data-gallery="true"><img alt="" srcSet="/blog/2016-09-text-mining-version-0-2-0/__webp-images__/Topic-Modelling.webp 1x, /blog/2016-09-text-mining-version-0-2-0/__webp-images__/Topic-Modelling.webp 2x" src="/blog/2016-09-text-mining-version-0-2-0/__webp-images__/Topic-Modelling.webp" width="1609" height="906" decoding="async" data-nimg="1" loading="lazy" style="color:transparent"/></a>
<p>Example of comparing text by topics.</p>
<p>Now we connect Corpus Viewer to Preprocess Text. This is nothing new, but Corpus Viewer now displays also tokens and POS tags. Enable POS Tagger in Preprocess Text. Now open Corpus Viewer and tick the checkbox Show Tokens &amp; Tags. This will display tagged token at the bottom of each document.</p>
<a href="/blog/2016-09-text-mining-version-0-2-0/__webp-images__/CorpusViewer.webp" data-gallery="true"><img alt="" srcSet="/blog/2016-09-text-mining-version-0-2-0/__webp-images__/CorpusViewer.webp 1x, /blog/2016-09-text-mining-version-0-2-0/__webp-images__/CorpusViewer.webp 2x" src="/blog/2016-09-text-mining-version-0-2-0/__webp-images__/CorpusViewer.webp" width="869" height="542" decoding="async" data-nimg="1" loading="lazy" style="color:transparent"/></a>
<p>Corpus Viewer can now display tokens and POS tags below each document.</p>
<p>This is just a brief overview of what one can do with the new Orange text mining functionalities. Of course, these are just exemplary workflows. If you did textual analysis with great results using any of these widgets, feel free to share it with us! :)</p></div></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"author":"AJDA","date":"2016-09-23 14:50:38+00:00","draft":false,"title":"Text Mining: version 0.2.0","type":"blog","blog":["addons","clustering","examples","orange3","preprocessing","release","text mining","widget"]},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    a: \"a\",\n    img: \"img\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.p, {\n      children: [_jsx(_components.a, {\n        href: \"https://github.com/biolab/orange3-text\",\n        children: \"Orange3-Text\"\n      }), \" has just recently been polished, updated and enhanced! Our GSoC student Alexey has \", _jsx(_components.a, {\n        href: \"/blog/2016/07/05/rehaul-of-text-mining-add-on/\",\n        children: \"helped us greatly\"\n      }), \" to achieve another milestone in Orange development and release the latest 0.2.0 version of our text mining add-on. The new release, which is already available on PyPi, includes Wikipedia and SimHash widgets and a rehaul of Bag of Words, Topic Modeling and Corpus Viewer.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Wikipedia widget allows retrieving sources from Wikipedia API and can handle multiple queries. It serves as an easy data gathering source and it's great for exploring text mining techniques. Here we've simply queried Wikipedia for articles on Slovenia and Germany and displayed them in Corpus Viewer.\"\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/blog/2016-09-text-mining-version-0-2-0/__webp-images__/wiki1-1.webp\",\n      alt: \"\",\n      width: \"1341\",\n      height: \"726\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Query Wikipedia by entering your query word list in the widget. Put each query on a separate line and run Search.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [_jsx(_components.a, {\n        href: \"http://matpalm.com/resemblance/simhash/\",\n        children: \"Similarity Hashing\"\n      }), \" widget computes similarity hashes for the given corpus, allowing the user to find duplicates, plagiarism or textual borrowing in the corpus. Here's an example from Wikipedia, which has a pre-defined structure of articles, making our corpus quite similar. We've used Wikipedia widget and retrieved 10 articles for the query 'Slovenia'. Then we've used Similarity Hashing to compute hashes for our text. What we got on the output is a table of 64 binary features (predefined in the SimHash widget), which denote a 64-bit hash size. Then we computed similarities in text by sending Similarity Hashing to Distances. Here we've selected cosine row distances and sent the output to Hierarchical Clustering. We can see that we have some similar documents, so we can select and inspect them in Corpus Viewer.\"]\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/blog/2016-09-text-mining-version-0-2-0/__webp-images__/SimHash1.webp\",\n      alt: \"\",\n      width: \"956\",\n      height: \"406\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Output of Similarity Hashing widget.\"\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/blog/2016-09-text-mining-version-0-2-0/__webp-images__/SimHash.webp\",\n      alt: \"\",\n      width: \"1616\",\n      height: \"912\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We've selected the two most similar documents in Hierarchical Clustering and displayed them in Corpus Viewer.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Topic Modeling now includes three modeling algorithms, namely \", _jsx(_components.a, {\n        href: \"https://en.wikipedia.org/wiki/Latent_semantic_analysis\",\n        children: \"Latent Semantic Indexing (LSP)\"\n      }), \", \", _jsx(_components.a, {\n        href: \"https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\",\n        children: \"Latent Dirichlet Allocation (LDA)\"\n      }), \", and \", _jsx(_components.a, {\n        href: \"https://en.wikipedia.org/wiki/Hierarchical_Dirichlet_process\",\n        children: \"Hierarchical Dirichlet Process (HDP)\"\n      }), \". Let's query Twitter for the latest tweets from Hillary Clinton and Donald Trump. First we preprocess the data and send the output to Topic Modeling. The widget suggests 10 topics, with the most significant words denoting each topic, and outputs topic probabilities for each document.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We can inspect distances between the topics with Distances (cosine) and Hierarchical Clustering. Seems like topics are not extremely author specific, since Hierarchical Clustering often puts Trump and Clinton in the same cluster. We've used Average linkage, but you can play around with different linkages and see if you can get better results.\"\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/blog/2016-09-text-mining-version-0-2-0/__webp-images__/Topic-Modelling.webp\",\n      alt: \"\",\n      width: \"1609\",\n      height: \"906\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Example of comparing text by topics.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now we connect Corpus Viewer to Preprocess Text. This is nothing new, but Corpus Viewer now displays also tokens and POS tags. Enable POS Tagger in Preprocess Text. Now open Corpus Viewer and tick the checkbox Show Tokens \u0026 Tags. This will display tagged token at the bottom of each document.\"\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/blog/2016-09-text-mining-version-0-2-0/__webp-images__/CorpusViewer.webp\",\n      alt: \"\",\n      width: \"869\",\n      height: \"542\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Corpus Viewer can now display tokens and POS tags below each document.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This is just a brief overview of what one can do with the new Orange text mining functionalities. Of course, these are just exemplary workflows. If you did textual analysis with great results using any of these widgets, feel free to share it with us! :)\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"text-mining-version-020"},"buildId":"t3FkTJiCx_jp9wzVJgeAY","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>