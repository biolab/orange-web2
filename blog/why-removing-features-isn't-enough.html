<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Orange Data Mining</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/media/0ac14a3c407fb3c4-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/media/fc6fba7ce0876fef-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/media/3cc61a2a1d48cb85-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/b49f75c2c957040b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b49f75c2c957040b.css" data-n-g=""/><link rel="preload" href="/_next/static/css/4ac7ed34d61ef456.css" as="style"/><link rel="stylesheet" href="/_next/static/css/4ac7ed34d61ef456.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-2df7a8d27de1794c.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-321f842b3cd96ae0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-998391557caf4c39.js" defer=""></script><script src="/_next/static/chunks/815-30d73934dc079aab.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-317e2716681b8fa3.js" defer=""></script><script src="/_next/static/g8uyptsXFwawFuOf8D4F-/_buildManifest.js" defer=""></script><script src="/_next/static/g8uyptsXFwawFuOf8D4F-/_ssgManifest.js" defer=""></script><style data-styled="" data-styled-version="5.3.6">.iSNtvJ{position:relative;max-width:1296px;margin-right:auto;margin-left:auto;padding-left:72px;padding-right:72px;height:100%;box-sizing:content-box;}/*!sc*/
@media (max-width:1130px){.iSNtvJ{padding-left:30px;padding-right:30px;}}/*!sc*/
@media (max-width:720px){.iSNtvJ{padding-left:15px;padding-right:15px;}}/*!sc*/
.fJLXpS{position:relative;max-width:1296px;margin-right:auto;margin-left:auto;padding-left:72px;padding-right:72px;height:100%;box-sizing:content-box;max-width:714px;}/*!sc*/
@media (max-width:1130px){.fJLXpS{padding-left:30px;padding-right:30px;}}/*!sc*/
@media (max-width:720px){.fJLXpS{padding-left:15px;padding-right:15px;}}/*!sc*/
data-styled.g1[id="sc-ef15f394-0"]{content:"iSNtvJ,fJLXpS,"}/*!sc*/
.fYoBKL{position:relative;display:inline-block;max-width:100%;font-size:20px;line-height:1.25;font-weight:600;-webkit-text-decoration:none;text-decoration:none;color:#fff;padding:10px 15px;border-radius:5px;background-image:linear-gradient(180deg,#FE7A00 74.93%,#F65D18 100%);box-shadow:0px 10px 10px rgba(0,0,0,0.03);background-size:100%;cursor:pointer;z-index:2;}/*!sc*/
.fYoBKL:before{content:"";display:block;position:absolute;top:0;left:0;width:100%;height:100%;border-radius:inherit;background-image:linear-gradient(180deg,#ffbc44 100%,#FE7A00 100%);opacity:0;z-index:-100;-webkit-transition:opacity 0.45s;transition:opacity 0.45s;}/*!sc*/
.fYoBKL:hover{color:#fff;}/*!sc*/
.fYoBKL:hover:before{opacity:1;}/*!sc*/
data-styled.g2[id="sc-e429b959-0"]{content:"fYoBKL,"}/*!sc*/
.ikuqzB{background:#1F1F1F;padding:38px 0 25px;color:#fff;}/*!sc*/
data-styled.g3[id="sc-7afe78da-0"]{content:"ikuqzB,"}/*!sc*/
.hrSlzr{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-column-gap:80px;column-gap:80px;row-gap:40px;margin-bottom:40px;}/*!sc*/
.hrSlzr h3{font-size:16px;font-weight:600;margin-bottom:12px;}/*!sc*/
.hrSlzr li + li{margin-top:12px;}/*!sc*/
data-styled.g4[id="sc-7afe78da-1"]{content:"hrSlzr,"}/*!sc*/
.eWdALT{display:none;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:space-around;-webkit-justify-content:space-around;-ms-flex-pack:space-around;justify-content:space-around;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:44px;height:34px;padding:6px;border-radius:5px;border:none;background:linear-gradient(180deg,#FE7A00 74.93%,#F65D18 100%);box-shadow:0px 10px 10px rgba(0,0,0,0.03);cursor:pointer;margin-left:auto;}/*!sc*/
@media (max-width:920px){.eWdALT{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}/*!sc*/
.eWdALT div{width:22px;height:2px;background:#fff;-webkit-transform-origin:1px;-ms-transform-origin:1px;transform-origin:1px;-webkit-transition:all 0.3s linear;transition:all 0.3s linear;}/*!sc*/
data-styled.g5[id="sc-46d6b9d9-0"]{content:"eWdALT,"}/*!sc*/
.jTcDmc{-webkit-clip:rect(0 0 0 0);clip:rect(0 0 0 0);-webkit-clip-path:inset(50%);clip-path:inset(50%);height:1px;overflow:hidden;position:absolute;white-space:nowrap;width:1px;}/*!sc*/
data-styled.g6[id="sc-cdea7863-0"]{content:"jTcDmc,"}/*!sc*/
.gPgccM{position:fixed;top:0;left:0;width:100%;height:80px;z-index:5;background:#fff;box-shadow:0px 4px 10px 4px rgba(0,0,0,0.04);}/*!sc*/
data-styled.g7[id="sc-6852f5f6-0"]{content:"gPgccM,"}/*!sc*/
.jrLPNu{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:100%;}/*!sc*/
.jrLPNu .img-logo{width:115px;}/*!sc*/
data-styled.g8[id="sc-6852f5f6-1"]{content:"jrLPNu,"}/*!sc*/
.hzGhxs{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media (max-width:920px){.hzGhxs{display:block;position:absolute;top:80px;left:0;width:100%;z-index:1;padding:0 30px 30px;background-color:#fff;opacity:0;z-index:-9999;pointer-events:none;-webkit-transition:opacity 0.3s ease;transition:opacity 0.3s ease;box-shadow:0 4px 10px -1px rgba(0,0,0,0.04);}}/*!sc*/
@media (max-width:720px){.hzGhxs{padding:0 15px 15px;}}/*!sc*/
data-styled.g9[id="sc-6852f5f6-2"]{content:"hzGhxs,"}/*!sc*/
.chSgxF{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media (max-width:920px){.chSgxF{display:block;margin-bottom:15px;}}/*!sc*/
.chSgxF li + li{margin-left:26px;}/*!sc*/
@media (max-width:920px){.chSgxF li + li{margin-left:0;}}/*!sc*/
.chSgxF a{display:inline-block;font-size:1.25rem;line-height:1;color:#000000;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:color 0.3s;transition:color 0.3s;}/*!sc*/
.chSgxF a:hover{color:#FE7A00;}/*!sc*/
@media (max-width:920px){.chSgxF a{padding:8px 0;}}/*!sc*/
data-styled.g10[id="sc-6852f5f6-3"]{content:"chSgxF,"}/*!sc*/
.hTbjmy{position:relative;width:160px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-transition:width 0.3s ease-in-out;transition:width 0.3s ease-in-out;margin-left:26px;}/*!sc*/
@media (max-width:920px){.hTbjmy{width:auto;margin-left:0;}}/*!sc*/
data-styled.g11[id="sc-6852f5f6-4"]{content:"hTbjmy,"}/*!sc*/
.kzOElI{display:inline-block;position:absolute;top:0;right:42px;width:calc(100% - 42px);height:100%;font-size:16px;line-height:1.25;padding:10px 13px;background:#fff;border-radius:5px 0px 0px 5px;border:1px solid #D9D9D9;border-right:none;-webkit-transition:width 0.3s ease-in-out;transition:width 0.3s ease-in-out;}/*!sc*/
@media (max-width:920px){.kzOElI{display:none;}}/*!sc*/
.kzOElI::-webkit-input-placeholder{color:#D9D9D9;}/*!sc*/
.kzOElI::-moz-placeholder{color:#D9D9D9;}/*!sc*/
.kzOElI:-ms-input-placeholder{color:#D9D9D9;}/*!sc*/
.kzOElI::placeholder{color:#D9D9D9;}/*!sc*/
data-styled.g12[id="sc-6852f5f6-5"]{content:"kzOElI,"}/*!sc*/
.dSfmcG{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;color:#fff;margin-left:auto;-webkit-flex:0 0 42px;-ms-flex:0 0 42px;flex:0 0 42px;height:41px;border:1px solid #474747;border-radius:0px 5px 5px 0px;background-color:#474747;}/*!sc*/
@media (max-width:920px){.dSfmcG{margin-left:0;}}/*!sc*/
data-styled.g13[id="sc-6852f5f6-6"]{content:"dSfmcG,"}/*!sc*/
*,*:before,*:after{box-sizing:border-box;-webkit-font-smoothing:antialiased;}/*!sc*/
html,body{height:100%;margin:0;}/*!sc*/
html{font-family:'Source Sans Pro',sans-serif;font-weight:400;color:#000000;}/*!sc*/
body{background:#fff;}/*!sc*/
body,h1,h2,h3,h4,h5,h6,p,ol,ul{margin:0;padding:0;font-weight:normal;}/*!sc*/
main{padding-top:80px;}/*!sc*/
ol,ul{list-style:none;}/*!sc*/
figure{margin:0;}/*!sc*/
img,video{display:block;max-width:100%;height:auto;}/*!sc*/
a{color:unset;-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
a:hover{color:unset;}/*!sc*/
strong{font-weight:600;}/*!sc*/
pre{display:block;padding:10px 14px;margin:0 0 10px;font-size:18px;line-height:1.42857143;word-break:break-all;word-wrap:break-word;color:#000;background-color:#f5f5f5;border-radius:5px;overflow-x:auto;word-wrap:normal;}/*!sc*/
pre code{padding:0;font-size:inherit;color:inherit;white-space:pre-wrap;background-color:transparent;border-radius:0;}/*!sc*/
code{font-family:Menlo,Monaco,Consolas,"Courier New",monospace;padding:2px 4px;font-size:90%;color:#c7254e;background-color:#f9f2f4;white-space:nowrap;border-radius:4px;white-space:pre-wrap;}/*!sc*/
::selection{background-color:#FE7A00;color:#fff;}/*!sc*/
.lg-container .lg-backdrop.in{opacity:0.75;}/*!sc*/
.lg-container .lg-toolbar.lg-group,.lg-container .lg-outer .lg-thumb-outer{background:rgba(0,0,0,0.45);}/*!sc*/
data-styled.g18[id="sc-global-exMkeD1"]{content:"sc-global-exMkeD1,"}/*!sc*/
.cXdfZy{font-size:62px;line-height:1.04;font-weight:700;color:#fff;color:#1F1F1F;}/*!sc*/
@media (max-width:920px){.cXdfZy{font-size:50px;}}/*!sc*/
@media (max-width:720px){.cXdfZy{font-size:42px;}}/*!sc*/
data-styled.g19[id="sc-4d34df46-0"]{content:"cXdfZy,"}/*!sc*/
.htQmXI h1{font-size:44px;line-height:1.13;font-weight:700;color:#1F1F1F;}/*!sc*/
@media (max-width:920px){.htQmXI h1{font-size:38px;}}/*!sc*/
@media (max-width:720px){.htQmXI h1{font-size:32px;}}/*!sc*/
.htQmXI h2{font-size:33px;line-height:1.13;font-weight:600;color:#1F1F1F;}/*!sc*/
@media (max-width:920px){.htQmXI h2{font-size:30px;}}/*!sc*/
@media (max-width:720px){.htQmXI h2{font-size:26px;}}/*!sc*/
.htQmXI h3{font-size:28px;line-height:1.18;font-weight:600;color:#1F1F1F;}/*!sc*/
@media (max-width:920px){.htQmXI h3{font-size:26px;}}/*!sc*/
@media (max-width:720px){.htQmXI h3{font-size:24px;}}/*!sc*/
.htQmXI h4{font-size:22px;line-height:1.18;font-weight:600;color:#1F1F1F;}/*!sc*/
@media (max-width:720px){.htQmXI h4{font-size:20px;}}/*!sc*/
.htQmXI p{font-size:22px;line-height:1.36;color:#1F1F1F;}/*!sc*/
@media (max-width:1130px){.htQmXI p{font-size:20px;}}/*!sc*/
@media (max-width:920px){.htQmXI p{font-size:18px;}}/*!sc*/
.htQmXI ul,.htQmXI ol{padding-left:40px;}/*!sc*/
.htQmXI ul li,.htQmXI ol li{font-size:22px;line-height:1.36;color:#1F1F1F;}/*!sc*/
@media (max-width:1130px){.htQmXI ul li,.htQmXI ol li{font-size:20px;}}/*!sc*/
@media (max-width:920px){.htQmXI ul li,.htQmXI ol li{font-size:18px;}}/*!sc*/
.htQmXI ul li + li,.htQmXI ol li + li{margin-top:4px;}/*!sc*/
.htQmXI ul{list-style:disc;}/*!sc*/
.htQmXI ol{list-style:decimal;}/*!sc*/
.htQmXI p a,.htQmXI li a{color:#FE7A00;}/*!sc*/
.htQmXI p a:hover,.htQmXI li a:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.htQmXI * + *:not(li,a){margin-top:15px;}/*!sc*/
.htQmXI * + a[data-gallery],.htQmXI * + video{margin-top:40px;}/*!sc*/
.htQmXI a[data-gallery]{display:block;margin-bottom:40px;}/*!sc*/
.htQmXI iframe,.htQmXI video{margin-bottom:40px;}/*!sc*/
data-styled.g22[id="sc-b0f3732d-0"]{content:"htQmXI,"}/*!sc*/
.fdCCNC{display:block;margin-left:auto;margin-right:auto;}/*!sc*/
data-styled.g23[id="sc-b0f3732d-1"]{content:"fdCCNC,"}/*!sc*/
.jiUdjB{padding:80px 0;}/*!sc*/
@media (max-width:920px){.jiUdjB{padding:60px 0;}}/*!sc*/
@media (max-width:720px){.jiUdjB{padding:40px 0;}}/*!sc*/
data-styled.g25[id="sc-18ad0c3-0"]{content:"jiUdjB,"}/*!sc*/
.farhVs{margin-bottom:30px;}/*!sc*/
data-styled.g26[id="sc-18ad0c3-1"]{content:"farhVs,"}/*!sc*/
.evzqvZ{position:relative;}/*!sc*/
data-styled.g27[id="sc-18ad0c3-2"]{content:"evzqvZ,"}/*!sc*/
.kwRhph{position:absolute;top:0;right:100%;width:200px;margin-right:50px;text-align:right;}/*!sc*/
@media (max-width:1130px){.kwRhph{position:unset;width:auto;text-align:left;margin:0 0 30px 0;}}/*!sc*/
data-styled.g28[id="sc-18ad0c3-3"]{content:"kwRhph,"}/*!sc*/
.jMsRbj{font-size:20px;line-height:1.25;color:#1F1F1F;text-transform:capitalize;color:#5651EC;}/*!sc*/
@media (max-width:920px){.jMsRbj{font-size:18px;}}/*!sc*/
.cwWTdU{font-size:20px;line-height:1.25;color:#1F1F1F;}/*!sc*/
@media (max-width:920px){.cwWTdU{font-size:18px;}}/*!sc*/
data-styled.g29[id="sc-18ad0c3-4"]{content:"jMsRbj,cwWTdU,"}/*!sc*/
</style></head><body><div id="__next"><div class="__className_ac89ad"><nav class="sc-6852f5f6-0 gPgccM"><div class="sc-ef15f394-0 iSNtvJ"><div class="sc-6852f5f6-1 jrLPNu"><a href="/"><img alt="Orange Logo" loading="lazy" width="120" height="35" decoding="async" data-nimg="1" class="img-logo" style="color:transparent" src="/_next/static/media/logo-orange.faff1861.svg"/></a><div><button aria-label="Toggle navigation" class="sc-46d6b9d9-0 eWdALT"><div></div><div></div><div></div></button><div class="sc-6852f5f6-2 hzGhxs"><ul class="sc-6852f5f6-3 chSgxF"><li><a href="/examples">Examples</a></li><li><a href="/download">Download</a></li><li><a href="/blog">Blog</a></li><li><a href="/docs">Docs</a></li><li><a href="/workshops">Workshops</a></li></ul><form class="sc-6852f5f6-4 hTbjmy"><div class="sc-6852f5f6-5 kzOElI">Search</div><button class="sc-6852f5f6-6 dSfmcG"><img alt="Icon for search" loading="lazy" width="18" height="17" decoding="async" data-nimg="1" style="color:transparent" src="/_next/static/media/icon-search.459b2665.svg"/><span class="sc-cdea7863-0 jTcDmc">Search through page</span></button></form></div></div></div></div></nav><main><div class="sc-18ad0c3-0 jiUdjB"><div class="sc-ef15f394-0 fJLXpS"><div class="sc-18ad0c3-1 farhVs"><p class="sc-18ad0c3-4 jMsRbj"><strong>fairness</strong></p><h1 class="sc-4d34df46-0 cXdfZy">Why Removing Features Isn&#x27;t Enough</h1></div><div class="sc-18ad0c3-2 evzqvZ"><div class="sc-18ad0c3-3 kwRhph"><p class="sc-18ad0c3-4 cwWTdU"><strong>Žan Mervič</strong></p><p class="sc-18ad0c3-4 cwWTdU">Sep 19, 2023</p></div><div class="sc-b0f3732d-0 htQmXI"><div class="lg-react-element "><p>Previously, we introduced and explained different fairness algorithms that can be used to mitigate bias in a dataset or model predictions. Here, we will discuss a common misconception: removing the protected attribute from the dataset will remove bias. We show why this is not the case and why it is essential to use fairness algorithms.</p>
<h2>Hiding Protected Attribute:</h2>
<p>Our setup is the following: we have two workflows, and both are using the adult data set. In the first workflow, we will train a logistic regression model using Reweighing as a preprocessor and a regular logistic regression model as a baseline on data that has not been modified. The second workflow uses the same dataset but with the protected attribute removed. We will then compare the predictions of the two workflows using a Box Plot.</p>
<a href="/blog/2023-09-fairness-hiding-protected-attribute/__webp-images__/2023-09-19-fairness-hiding-protected-attribute-use-case-1.webp" data-gallery="true"><img alt="" loading="lazy" width="968" height="412" decoding="async" data-nimg="1" class="sc-b0f3732d-1 fdCCNC" style="color:transparent" src="/blog/2023-09-fairness-hiding-protected-attribute/__webp-images__/2023-09-19-fairness-hiding-protected-attribute-use-case-1.webp"/></a>
<a href="/blog/2023-09-fairness-hiding-protected-attribute/__webp-images__/2023-09-19-fairness-hiding-protected-attribute-use-case-2.webp" data-gallery="true"><img alt="" loading="lazy" width="957" height="301" decoding="async" data-nimg="1" class="sc-b0f3732d-1 fdCCNC" style="color:transparent" src="/blog/2023-09-fairness-hiding-protected-attribute/__webp-images__/2023-09-19-fairness-hiding-protected-attribute-use-case-2.webp"/></a>
<p>The two workflows are very similar, the only difference being the extra learner used in the first workflow, the reweighted learner, and two Select Columns widgets used in the second workflow. The first Select Columns widget is used to remove the protected attribute from the data, and the second one is used to add it back after the predictions are made so that we can compare the predictions of the two workflows using a Box Plot widget.</p>
<p>In this example, we will not be able to use the scores to compare the fairness metrics of the models because we could not calculate the fairness metrics for the workflow with the hidden protected attribute. This is because the protected attribute is required to calculate the fairness metrics. Saying that we can still compare the accuracy scores of the models. Here are the scores for the reweighted model, the baseline model, and the model learning on altered data:</p>
<a href="/blog/2023-09-fairness-hiding-protected-attribute/__webp-images__/2023-09-19-fairness-hiding-protected-attribute-scores.webp" data-gallery="true"><img alt="" loading="lazy" width="815" height="196" decoding="async" data-nimg="1" class="sc-b0f3732d-1 fdCCNC" style="color:transparent" src="/blog/2023-09-fairness-hiding-protected-attribute/__webp-images__/2023-09-19-fairness-hiding-protected-attribute-scores.webp"/></a>
<p>The baseline model and the model learning on altered data have very similar accuracy scores, while the reweighted model has a slight dip in accuracy.</p>
<p>Instead of using the scores to compare the models, we will visualize the fairness Disparate Impact and Statistical Parity Difference fairness metrics using the Box Plot widget, just like we did in many previous blogs. Here we will show three different box plots, the first one will be from the reweighted model, the second one from the baseline, and the third from the model learning on data with the protected attribute removed:</p>
<a href="/blog/2023-09-fairness-hiding-protected-attribute/__webp-images__/2023-09-19-fairness-hiding-protected-attribute-box-plot.webp" data-gallery="true"><img alt="" loading="lazy" width="796" height="693" decoding="async" data-nimg="1" class="sc-b0f3732d-1 fdCCNC" style="color:transparent" src="/blog/2023-09-fairness-hiding-protected-attribute/__webp-images__/2023-09-19-fairness-hiding-protected-attribute-box-plot.webp"/></a>
<p>The box plots show that the baseline model results are very similar to the model learning on data with the protected attribute removed. In contrast, the reweighted model results are very different. The ratio of favorable prediction is much more similar when using debiasing compared to the baseline model and the model learning on data with the protected attribute removed.</p>
<p>A similar observation can be made for the Equalized Odds Difference and Average Odds Difference fairness metrics. Using a similar setup as in the <a href="/blog/2023-09-19-fairness-equal-odds-postprocessing/">previous blog post</a>, we visualized the True Positive Rate for each group using the mosaic display widget. We trained one model using the Equalized Odds Postprocessing widget and one on data with the protected attribute &quot;age&quot; hidden. Here are the results:</p>
<a href="/blog/2023-09-fairness-hiding-protected-attribute/__webp-images__/2023-09-19-fairness-hiding-protected-attribute-mosaic.webp" data-gallery="true"><img alt="" loading="lazy" width="800" height="378" decoding="async" data-nimg="1" class="sc-b0f3732d-1 fdCCNC" style="color:transparent" src="/blog/2023-09-fairness-hiding-protected-attribute/__webp-images__/2023-09-19-fairness-hiding-protected-attribute-mosaic.webp"/></a>
<p>In the visualizations, the red part of each column represents the true positive rate for each group; you can ignore the width of the columns as that represents the number of instances in each group, which is irrelevant to us. We can see that the difference in True Positive Rate between the two groups is much smaller when using the Equal Odds Postprocessing widget.</p>
<h2>Why and how?</h2>
<p>If we removed the protected attribute from the dataset on which we train and test the model, how is it that the predictions are still biased towards one of the groups?</p>
<p>The answer is that features in a dataset are often correlated. This means that even if we remove the protected attribute, other features can still be used to infer the protected attribute. For example, suppose the protected attribute is race, and we have a feature like zip code in the data. Since certain races might be predominant in particular zip codes, the model may still indirectly learn the bias. We can test this correlation by predicting the protected attribute from the other features. If we can predict the protected attribute with high accuracy, we know that the other features are correlated with the protected attribute. Here is an example of such a workflow:</p>
<a href="/blog/2023-09-fairness-hiding-protected-attribute/__webp-images__/2023-09-19-fairness-hiding-protected-attribute-sex.webp" data-gallery="true"><img alt="" loading="lazy" width="547" height="248" decoding="async" data-nimg="1" class="sc-b0f3732d-1 fdCCNC" style="color:transparent" src="/blog/2023-09-fairness-hiding-protected-attribute/__webp-images__/2023-09-19-fairness-hiding-protected-attribute-sex.webp"/></a>
<p>In this workflow, we set the &quot;sex&quot; attribute as the target variable and use a Logistic Regression model to try and predict it.</p>
<a href="/blog/2023-09-fairness-hiding-protected-attribute/__webp-images__/2023-09-19-fairness-hiding-protected-attribute-sex-scores.webp" data-gallery="true"><img alt="" loading="lazy" width="521" height="217" decoding="async" data-nimg="1" class="sc-b0f3732d-1 fdCCNC" style="color:transparent" src="/blog/2023-09-fairness-hiding-protected-attribute/__webp-images__/2023-09-19-fairness-hiding-protected-attribute-sex-scores.webp"/></a>
<p>We can see that the model has a very high accuracy score, which means that the other features in the dataset are correlated with the protected attribute. This means that even if we remove the protected attribute from the dataset, the model can still infer it from the other features. This is why it is crucial to use fairness algorithms instead of simply removing the protected attribute.</p></div></div></div></div></div></main><footer class="sc-7afe78da-0 ikuqzB"><div class="sc-ef15f394-0 iSNtvJ"><div class="sc-7afe78da-1 hrSlzr"><div><h3>Orange</h3><ul><li><a href="/faq">FAQ</a></li><li><a href="/license">License</a></li><li><a href="/privacy">Privacy</a></li><li><a href="/citation">Citation</a></li><li><a href="/contact">Contact</a></li></ul></div><div><h3>Download</h3><ul><li><a href="/download#win">Windows</a></li><li><a href="/download#mac">Mac OS</a></li></ul></div><div><h3>Community</h3><ul><li><a href="https://twitter.com/OrangeDataMiner">Twitter</a></li><li><a href="https://www.facebook.com/orangedatamining">Facebook</a></li><li><a href="https://datascience.stackexchange.com/questions/tagged/orange">Stack Exchange</a></li><li><a href="https://www.youtube.com/channel/UClKKWBe2SCAEyv7ZNGhIe4g">YouTube</a></li><li><a href="https://discord.com/invite/FWrfeXV">Discord</a></li></ul></div><div><h3>Documentation</h3><ul><li><a href="/getting-started">Get started</a></li><li><a href="/widget-catalogue">Widgets</a></li><li><a href="https://orange3.readthedocs.io/projects/orange-data-mining-library/en/latest/">Scripting</a></li></ul></div><div><h3>Developers</h3><ul><li><a href="https://github.com/biolab/orange3">GitHub</a></li><li><a href="http://docs.biolab.si/3/development/">Getting started</a></li></ul></div><div><a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=A76TAX87ZVR3J" target="_blank" rel="noreferrer" class="sc-e429b959-0 fYoBKL">Donate to Orange</a></div></div><p>Copyright © University of Ljubljana</p></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"author":"Žan Mervič","date":"2023-09-19 05:00:00+00:00","draft":false,"title":"Why Removing Features Isn't Enough","thumbImage":"2023-09-19-fairness-hiding-protected-attribute-thumb.png","frontPageImage":"2023-09-19-fairness-hiding-protected-attribute-thumb.png","blog":["fairness"],"shortExcerpt":"Find out why merely removing protected attributes will not fix bias. Features often correlate, letting models infer biases. Fairness algorithms are key for genuine bias mitigation.","longExcerpt":"In this blog, we confront the common misconception that merely removing a protected attribute from a dataset eliminates bias in model predictions. Our case study reveals that models trained without these attributes still produce biased results. This is due to feature correlations that indirectly capture the protected information. Our conclusion? You cannot sidestep the need for specialized fairness algorithms.","oldUrl":"/blog/2023/2023-09-19-fairness-hiding-protected-attribute/"},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    h2: \"h2\",\n    a: \"a\"\n  }, _provideComponents(), props.components), {WindowScreenshot} = _components;\n  if (!WindowScreenshot) _missingMdxReference(\"WindowScreenshot\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"Previously, we introduced and explained different fairness algorithms that can be used to mitigate bias in a dataset or model predictions. Here, we will discuss a common misconception: removing the protected attribute from the dataset will remove bias. We show why this is not the case and why it is essential to use fairness algorithms.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Hiding Protected Attribute:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Our setup is the following: we have two workflows, and both are using the adult data set. In the first workflow, we will train a logistic regression model using Reweighing as a preprocessor and a regular logistic regression model as a baseline on data that has not been modified. The second workflow uses the same dataset but with the protected attribute removed. We will then compare the predictions of the two workflows using a Box Plot.\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2023-09-fairness-hiding-protected-attribute/2023-09-19-fairness-hiding-protected-attribute-use-case-1.png\",\n      width: \"968\",\n      height: \"412\",\n      src: \"/blog/2023-09-fairness-hiding-protected-attribute/__webp-images__/2023-09-19-fairness-hiding-protected-attribute-use-case-1.webp\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2023-09-fairness-hiding-protected-attribute/2023-09-19-fairness-hiding-protected-attribute-use-case-2.png\",\n      width: \"957\",\n      height: \"301\",\n      src: \"/blog/2023-09-fairness-hiding-protected-attribute/__webp-images__/2023-09-19-fairness-hiding-protected-attribute-use-case-2.webp\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The two workflows are very similar, the only difference being the extra learner used in the first workflow, the reweighted learner, and two Select Columns widgets used in the second workflow. The first Select Columns widget is used to remove the protected attribute from the data, and the second one is used to add it back after the predictions are made so that we can compare the predictions of the two workflows using a Box Plot widget.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"In this example, we will not be able to use the scores to compare the fairness metrics of the models because we could not calculate the fairness metrics for the workflow with the hidden protected attribute. This is because the protected attribute is required to calculate the fairness metrics. Saying that we can still compare the accuracy scores of the models. Here are the scores for the reweighted model, the baseline model, and the model learning on altered data:\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2023-09-fairness-hiding-protected-attribute/2023-09-19-fairness-hiding-protected-attribute-scores.png\",\n      width: \"815\",\n      height: \"196\",\n      src: \"/blog/2023-09-fairness-hiding-protected-attribute/__webp-images__/2023-09-19-fairness-hiding-protected-attribute-scores.webp\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The baseline model and the model learning on altered data have very similar accuracy scores, while the reweighted model has a slight dip in accuracy.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Instead of using the scores to compare the models, we will visualize the fairness Disparate Impact and Statistical Parity Difference fairness metrics using the Box Plot widget, just like we did in many previous blogs. Here we will show three different box plots, the first one will be from the reweighted model, the second one from the baseline, and the third from the model learning on data with the protected attribute removed:\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2023-09-fairness-hiding-protected-attribute/2023-09-19-fairness-hiding-protected-attribute-box-plot.png\",\n      width: \"796\",\n      height: \"693\",\n      src: \"/blog/2023-09-fairness-hiding-protected-attribute/__webp-images__/2023-09-19-fairness-hiding-protected-attribute-box-plot.webp\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The box plots show that the baseline model results are very similar to the model learning on data with the protected attribute removed. In contrast, the reweighted model results are very different. The ratio of favorable prediction is much more similar when using debiasing compared to the baseline model and the model learning on data with the protected attribute removed.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"A similar observation can be made for the Equalized Odds Difference and Average Odds Difference fairness metrics. Using a similar setup as in the \", _jsx(_components.a, {\n        href: \"/blog/2023-09-19-fairness-equal-odds-postprocessing/\",\n        children: \"previous blog post\"\n      }), \", we visualized the True Positive Rate for each group using the mosaic display widget. We trained one model using the Equalized Odds Postprocessing widget and one on data with the protected attribute \\\"age\\\" hidden. Here are the results:\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2023-09-fairness-hiding-protected-attribute/2023-09-19-fairness-hiding-protected-attribute-mosaic.png\",\n      width: \"800\",\n      height: \"378\",\n      src: \"/blog/2023-09-fairness-hiding-protected-attribute/__webp-images__/2023-09-19-fairness-hiding-protected-attribute-mosaic.webp\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"In the visualizations, the red part of each column represents the true positive rate for each group; you can ignore the width of the columns as that represents the number of instances in each group, which is irrelevant to us. We can see that the difference in True Positive Rate between the two groups is much smaller when using the Equal Odds Postprocessing widget.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Why and how?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"If we removed the protected attribute from the dataset on which we train and test the model, how is it that the predictions are still biased towards one of the groups?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The answer is that features in a dataset are often correlated. This means that even if we remove the protected attribute, other features can still be used to infer the protected attribute. For example, suppose the protected attribute is race, and we have a feature like zip code in the data. Since certain races might be predominant in particular zip codes, the model may still indirectly learn the bias. We can test this correlation by predicting the protected attribute from the other features. If we can predict the protected attribute with high accuracy, we know that the other features are correlated with the protected attribute. Here is an example of such a workflow:\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2023-09-fairness-hiding-protected-attribute/2023-09-19-fairness-hiding-protected-attribute-sex.png\",\n      width: \"547\",\n      height: \"248\",\n      src: \"/blog/2023-09-fairness-hiding-protected-attribute/__webp-images__/2023-09-19-fairness-hiding-protected-attribute-sex.webp\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"In this workflow, we set the \\\"sex\\\" attribute as the target variable and use a Logistic Regression model to try and predict it.\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2023-09-fairness-hiding-protected-attribute/2023-09-19-fairness-hiding-protected-attribute-sex-scores.png\",\n      width: \"521\",\n      height: \"217\",\n      src: \"/blog/2023-09-fairness-hiding-protected-attribute/__webp-images__/2023-09-19-fairness-hiding-protected-attribute-sex-scores.webp\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We can see that the model has a very high accuracy score, which means that the other features in the dataset are correlated with the protected attribute. This means that even if we remove the protected attribute from the dataset, the model can still infer it from the other features. This is why it is crucial to use fairness algorithms instead of simply removing the protected attribute.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"why-removing-features-isn't-enough"},"buildId":"g8uyptsXFwawFuOf8D4F-","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>