<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>My new cool app</title><meta name="next-head-count" content="3"/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-d38be8d96a62f950.js" defer=""></script><script src="/_next/static/chunks/framework-3b5a00d5d7e8d93b.js" defer=""></script><script src="/_next/static/chunks/main-1b5205ad35b92557.js" defer=""></script><script src="/_next/static/chunks/pages/_app-72ed113f22a36849.js" defer=""></script><script src="/_next/static/chunks/675-44addd24cc62f3a3.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-6a5b6173e98f113c.js" defer=""></script><script src="/_next/static/CX8-OElU8TqTkW2zxyeBX/_buildManifest.js" defer=""></script><script src="/_next/static/CX8-OElU8TqTkW2zxyeBX/_ssgManifest.js" defer=""></script><style data-styled="" data-styled-version="5.3.6">.iBQlkL{background:coral;height:60px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media (max-width:920px){.iBQlkL ul{display:none;}}/*!sc*/
data-styled.g1[id="sc-7295e4c-0"]{content:"iBQlkL,"}/*!sc*/
.bDMZDz{display:none;font-size:22px;margin-left:auto;}/*!sc*/
@media (max-width:920px){.bDMZDz{display:block;}}/*!sc*/
data-styled.g2[id="sc-7295e4c-1"]{content:"bDMZDz,"}/*!sc*/
html,body{height:100%;margin:0;}/*!sc*/
body{background:#fff;}/*!sc*/
html{box-sizing:border-box;font-size:16px;}/*!sc*/
*,*:before,*:after{box-sizing:inherit;}/*!sc*/
body,h1,h2,h3,h4,h5,h6,p,ol,ul{margin:0;padding:0;font-weight:normal;}/*!sc*/
ol,ul{list-style:none;}/*!sc*/
img,video{max-width:100%;height:auto;}/*!sc*/
data-styled.g3[id="sc-global-eYmBoQ1"]{content:"sc-global-eYmBoQ1,"}/*!sc*/
.ufdhg{max-width:800px;margin:0 auto;}/*!sc*/
data-styled.g4[id="sc-dab4a0e4-0"]{content:"ufdhg,"}/*!sc*/
.fGzDrQ{display:block;margin-left:auto;margin-right:auto;}/*!sc*/
data-styled.g5[id="sc-dab4a0e4-1"]{content:"fGzDrQ,"}/*!sc*/
</style></head><body><div id="__next"><nav class="sc-7295e4c-0 iBQlkL"><ul><a href="/blog">Blog</a></ul><button class="sc-7295e4c-1 bDMZDz">X</button></nav><main><div class="sc-dab4a0e4-0 ufdhg"><h1>How to identify fake news with document embeddings</h1><p>Text is described by the sequence of character. Since every machine learning algorithm needs numbers, we need to transform it into vectors of real numbers before we can continue with the analysis. To do this, we can use various approaches. Orange currently offers bag-of-words approach and now also Document embedding by fastText.
In this post, we explain what document embedding is, why it is useful, and show its usage on the classification example.</p>
<h2>Word embedding and document embedding</h2>
<p>Before we understand document embeddings, we need to understand the concept of word embeddings. Word embedding is a representation of a word in multidimensional spaces such that words with similar meanings have similar embedding. It means that each word is mapped to the vector of real numbers that represents the word.
Embedding models are mostly based on neural networks.</p>
<p>Document embedding is computed in two steps. First, each word is embedded with the word embedding then word embeddings are aggregated. The most common type of aggregation is the average over each dimension.</p>
<h2>Why and when should we use embedders?</h2>
<p>Compared to <a href="/widget-catalog/text-mining/bagofwords-widget">bag-of-words</a>, which counts the number of appearances of each token in the document, embeddings have two main advantages:</p>
<ul>
<li><strong>They do not have a dimensionality problem</strong> The result of bag-of-words is a table which has the number of features equal to the number of unique tokens in all documents in a corpus. Large corpora with long texts result in a large number of unique tokens. It results in huge tables which can exceed memory in the computer. Huge tables also increase the learning and evaluation time of machine learning models. Embedders have constant dimensionality of the vector, which is 300 for fastText embeddings that Orange uses.</li>
<li><strong>Most of the preprocessing is not required</strong> In case of bag-of-words approach, we solve the dimensionality problem with the text preprocessing where we remove tokens (e.g. words) that seem to be less important for the analysis. It can also cause the removal of some important tokens. When using embedders, we do not need to remove tokens, so we are not losing the accuracy this way. Also most of the basic preprocessing can be omitted (such as normalization) in case of fastText embeddings.</li>
<li><strong>They can be pretrained</strong> Word embedding models can be pretrained on large corpora with billions of tokens. That way, they capture the significant characteristics of the language and produce the embeddings of high quality. Pretrained models are then used to obtain embeddings of smaller datasets. Our Document Embedding widget uses pretrained fastText models and is suitable for corpora of any size.</li>
</ul>
<p>The shortcoming of the embedders is that they are difficult to understand. For example, when we use a bag-of-words, we can easily observe which tokens are important for classification with <a href="/widget-catalog/visualize/nomogram/">Nomogram widget</a> since tokens themselves are features. In the case of document embeddings, features are numbers which are not understandable to human by themselves.</p>
<h2>Document Embedding widget</h2>
<p>Orange now offers document embedders through Document Embedding widget. We decided to use <a href="https://fasttext.cc/docs/en/crawl-vectors.html">fastText pretrained embedders</a>, which support 157 languages. Orange&#x27;s Document Embedding widget currently supports 31 most common languages.</p>
<img srcSet="/?url=%2Fblog%2F2020-10-document-embedders%2F2020-10-15-document-embedding-widget.png&amp;w=640&amp;q=75 1x, /?url=%2Fblog%2F2020-10-document-embedders%2F2020-10-15-document-embedding-widget.png&amp;w=1920&amp;q=75 2x" src="/?url=%2Fblog%2F2020-10-document-embedders%2F2020-10-15-document-embedding-widget.png&amp;w=1920&amp;q=75" width="608" height="486" decoding="async" data-nimg="1" class="sc-dab4a0e4-1 fGzDrQ" loading="lazy" style="color:transparent"/>
<p>In the widget, the user sets the language of documents and the aggregation method -- it is how embeddings for each word in a document are aggregated into one document embedding.</p>
<h2>The Fake News dataset</h2>
<p>For this tutorial, we use the sample of <a href="https://www.kaggle.com/c/fake-news/data">Fake News dataset</a>. The dataset sample is available at <a href="http://file.biolab.si/datasets/fake.zip">Orange&#x27;s file repository</a>. It is a zip archive containing two datasets: training set including 2725 text items and testing set with 275 items. Each item is an article which is labelled as a real or fake.</p>
<h2>Fake news identification</h2>
<p>Here we present a fake news identification. First, we will load a training part of the dataset with the Corpus widget.</p>
<img srcSet="/?url=%2Fblog%2F2020-10-document-embedders%2F2020-10-15-corpus.png&amp;w=1080&amp;q=75 1x, /?url=%2Fblog%2F2020-10-document-embedders%2F2020-10-15-corpus.png&amp;w=2048&amp;q=75 2x" src="/?url=%2Fblog%2F2020-10-document-embedders%2F2020-10-15-corpus.png&amp;w=2048&amp;q=75" width="982" height="728" decoding="async" data-nimg="1" class="sc-dab4a0e4-1 fGzDrQ" loading="lazy" style="color:transparent"/>
<p>After the dataset is loaded, we make sure that the <code>text</code> variable is selected in the Used text features field. It means that the text in this variable is used in the text analysis. When the dataset is loaded we connect the Corpus widget to the Document embedder widget which will compute text embeddings. Our workflow should look like this now:</p>
<img srcSet="/?url=%2Fblog%2F2020-10-document-embedders%2F2020-10-15-workflow1.png&amp;w=750&amp;q=75 1x, /?url=%2Fblog%2F2020-10-document-embedders%2F2020-10-15-workflow1.png&amp;w=1920&amp;q=75 2x" src="/?url=%2Fblog%2F2020-10-document-embedders%2F2020-10-15-workflow1.png&amp;w=1920&amp;q=75" width="642" height="762" decoding="async" data-nimg="1" class="sc-dab4a0e4-1 fGzDrQ" loading="lazy" style="color:transparent"/>
<p>In the document embeddings widget, we check that language is set to English since texts in this dataset are English. We will use mean (average) aggregation in this experiment -- it is the most standard one. After minute documents are embedded -- embedding progress is shown with the bar around the widget.</p>
<p>When embeddings are ready, we can train models. In this tutorial, we train two models -- Logistic regression and Random forest. We will use default settings for both learners.</p>
<img srcSet="/?url=%2Fblog%2F2020-10-document-embedders%2F2020-10-15-workflow2.png&amp;w=828&amp;q=75 1x, /?url=%2Fblog%2F2020-10-document-embedders%2F2020-10-15-workflow2.png&amp;w=1920&amp;q=75 2x" src="/?url=%2Fblog%2F2020-10-document-embedders%2F2020-10-15-workflow2.png&amp;w=1920&amp;q=75" width="756" height="396" decoding="async" data-nimg="1" class="sc-dab4a0e4-1 fGzDrQ" loading="lazy" style="color:transparent"/>
<p>When our models are trained, we prepare the testing data. To load testing data, we use another Corpus widget and connect it to the Document embedder widget. Settings are the same as before. The only difference is that this time we load testing part of the dataset in the second Corpus widget. To make predictions and inspect the prediction results on the testing dataset, we use the prediction widget.</p>
<img srcSet="/?url=%2Fblog%2F2020-10-document-embedders%2F2020-10-15-workflow3.png&amp;w=1920&amp;q=75 1x, /?url=%2Fblog%2F2020-10-document-embedders%2F2020-10-15-workflow3.png&amp;w=3840&amp;q=75 2x" src="/?url=%2Fblog%2F2020-10-document-embedders%2F2020-10-15-workflow3.png&amp;w=3840&amp;q=75" width="1378" height="1272" decoding="async" data-nimg="1" class="sc-dab4a0e4-1 fGzDrQ" loading="lazy" style="color:transparent"/>
<p>In the bottom part of the widget, we inspect the accuracies. In the column with name CA (classification accuracy), we can see that both models perform with around 80 % accuracy. In the table above, we can find cases where models made mistakes. If we select rows, we can check them in the Corpus Viewer widget which is connected to the Predictions widget. We have also connected the confusion matrix widget to our workflow, which shows the proportions between the predicted and actual classes.</p>
<img srcSet="/?url=%2Fblog%2F2020-10-document-embedders%2F2020-10-15-confusion.png&amp;w=3840&amp;q=75 1x" src="/?url=%2Fblog%2F2020-10-document-embedders%2F2020-10-15-confusion.png&amp;w=3840&amp;q=75" width="2760" height="690" decoding="async" data-nimg="1" class="sc-dab4a0e4-1 fGzDrQ" loading="lazy" style="color:transparent"/>
<p>We can see that Logistic regression is slightly more accurate in cases of real news while Random forest model is better for predicting fake news.</p>
<p>It is just one example which shows how to use document embeddings.  You can also use them for other tasks such as clustering, regression or other types of analysis.</p></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"author":"Primož Godec and Nikola Đukić","date":"2020-10-15","draft":false,"title":"How to identify fake news with document embeddings","type":"blog","thumbImage":"2020-10-15-document-embedding-widget.png","frontPageImage":"2020-10-15-document-embedding-title.png","blog":["text mining","corpus","classification"],"shortExcerpt":"New Document embedder widget and its use for classification","longExcerpt":"Presenting document embeddings widget and how to identify fake news.","x2images":true},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    h2: \"h2\",\n    a: \"a\",\n    ul: \"ul\",\n    li: \"li\",\n    strong: \"strong\",\n    code: \"code\"\n  }, _provideComponents(), props.components), {WorkflowScreenshot} = _components;\n  if (!WorkflowScreenshot) _missingMdxReference(\"WorkflowScreenshot\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"Text is described by the sequence of character. Since every machine learning algorithm needs numbers, we need to transform it into vectors of real numbers before we can continue with the analysis. To do this, we can use various approaches. Orange currently offers bag-of-words approach and now also Document embedding by fastText.\\nIn this post, we explain what document embedding is, why it is useful, and show its usage on the classification example.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Word embedding and document embedding\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Before we understand document embeddings, we need to understand the concept of word embeddings. Word embedding is a representation of a word in multidimensional spaces such that words with similar meanings have similar embedding. It means that each word is mapped to the vector of real numbers that represents the word.\\nEmbedding models are mostly based on neural networks.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Document embedding is computed in two steps. First, each word is embedded with the word embedding then word embeddings are aggregated. The most common type of aggregation is the average over each dimension.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Why and when should we use embedders?\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Compared to \", _jsx(_components.a, {\n        href: \"/widget-catalog/text-mining/bagofwords-widget\",\n        children: \"bag-of-words\"\n      }), \", which counts the number of appearances of each token in the document, embeddings have two main advantages:\"]\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"They do not have a dimensionality problem\"\n        }), \" The result of bag-of-words is a table which has the number of features equal to the number of unique tokens in all documents in a corpus. Large corpora with long texts result in a large number of unique tokens. It results in huge tables which can exceed memory in the computer. Huge tables also increase the learning and evaluation time of machine learning models. Embedders have constant dimensionality of the vector, which is 300 for fastText embeddings that Orange uses.\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Most of the preprocessing is not required\"\n        }), \" In case of bag-of-words approach, we solve the dimensionality problem with the text preprocessing where we remove tokens (e.g. words) that seem to be less important for the analysis. It can also cause the removal of some important tokens. When using embedders, we do not need to remove tokens, so we are not losing the accuracy this way. Also most of the basic preprocessing can be omitted (such as normalization) in case of fastText embeddings.\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"They can be pretrained\"\n        }), \" Word embedding models can be pretrained on large corpora with billions of tokens. That way, they capture the significant characteristics of the language and produce the embeddings of high quality. Pretrained models are then used to obtain embeddings of smaller datasets. Our Document Embedding widget uses pretrained fastText models and is suitable for corpora of any size.\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"The shortcoming of the embedders is that they are difficult to understand. For example, when we use a bag-of-words, we can easily observe which tokens are important for classification with \", _jsx(_components.a, {\n        href: \"/widget-catalog/visualize/nomogram/\",\n        children: \"Nomogram widget\"\n      }), \" since tokens themselves are features. In the case of document embeddings, features are numbers which are not understandable to human by themselves.\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Document Embedding widget\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Orange now offers document embedders through Document Embedding widget. We decided to use \", _jsx(_components.a, {\n        href: \"https://fasttext.cc/docs/en/crawl-vectors.html\",\n        children: \"fastText pretrained embedders\"\n      }), \", which support 157 languages. Orange's Document Embedding widget currently supports 31 most common languages.\"]\n    }), \"\\n\", _jsx(WorkflowScreenshot, {\n      src: \"/blog/2020-10-document-embedders/2020-10-15-document-embedding-widget.png\",\n      width: \"608\",\n      height: \"486\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"In the widget, the user sets the language of documents and the aggregation method -- it is how embeddings for each word in a document are aggregated into one document embedding.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"The Fake News dataset\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"For this tutorial, we use the sample of \", _jsx(_components.a, {\n        href: \"https://www.kaggle.com/c/fake-news/data\",\n        children: \"Fake News dataset\"\n      }), \". The dataset sample is available at \", _jsx(_components.a, {\n        href: \"http://file.biolab.si/datasets/fake.zip\",\n        children: \"Orange's file repository\"\n      }), \". It is a zip archive containing two datasets: training set including 2725 text items and testing set with 275 items. Each item is an article which is labelled as a real or fake.\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Fake news identification\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Here we present a fake news identification. First, we will load a training part of the dataset with the Corpus widget.\"\n    }), \"\\n\", _jsx(WorkflowScreenshot, {\n      src: \"/blog/2020-10-document-embedders/2020-10-15-corpus.png\",\n      width: \"982\",\n      height: \"728\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"After the dataset is loaded, we make sure that the \", _jsx(_components.code, {\n        children: \"text\"\n      }), \" variable is selected in the Used text features field. It means that the text in this variable is used in the text analysis. When the dataset is loaded we connect the Corpus widget to the Document embedder widget which will compute text embeddings. Our workflow should look like this now:\"]\n    }), \"\\n\", _jsx(WorkflowScreenshot, {\n      src: \"/blog/2020-10-document-embedders/2020-10-15-workflow1.png\",\n      width: \"642\",\n      height: \"762\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"In the document embeddings widget, we check that language is set to English since texts in this dataset are English. We will use mean (average) aggregation in this experiment -- it is the most standard one. After minute documents are embedded -- embedding progress is shown with the bar around the widget.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When embeddings are ready, we can train models. In this tutorial, we train two models -- Logistic regression and Random forest. We will use default settings for both learners.\"\n    }), \"\\n\", _jsx(WorkflowScreenshot, {\n      src: \"/blog/2020-10-document-embedders/2020-10-15-workflow2.png\",\n      width: \"756\",\n      height: \"396\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When our models are trained, we prepare the testing data. To load testing data, we use another Corpus widget and connect it to the Document embedder widget. Settings are the same as before. The only difference is that this time we load testing part of the dataset in the second Corpus widget. To make predictions and inspect the prediction results on the testing dataset, we use the prediction widget.\"\n    }), \"\\n\", _jsx(WorkflowScreenshot, {\n      src: \"/blog/2020-10-document-embedders/2020-10-15-workflow3.png\",\n      width: \"1378\",\n      height: \"1272\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"In the bottom part of the widget, we inspect the accuracies. In the column with name CA (classification accuracy), we can see that both models perform with around 80 % accuracy. In the table above, we can find cases where models made mistakes. If we select rows, we can check them in the Corpus Viewer widget which is connected to the Predictions widget. We have also connected the confusion matrix widget to our workflow, which shows the proportions between the predicted and actual classes.\"\n    }), \"\\n\", _jsx(WorkflowScreenshot, {\n      src: \"/blog/2020-10-document-embedders/2020-10-15-confusion.png\",\n      width: \"2760\",\n      height: \"690\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We can see that Logistic regression is slightly more accurate in cases of real news while Random forest model is better for predicting fake news.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"It is just one example which shows how to use document embeddings.  You can also use them for other tasks such as clustering, regression or other types of analysis.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"how-to-identify-fake-news-with-document-embeddings"},"buildId":"CX8-OElU8TqTkW2zxyeBX","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>