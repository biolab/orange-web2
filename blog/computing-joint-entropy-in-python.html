<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>My new cool app</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/4ac7ed34d61ef456.css" as="style"/><link rel="stylesheet" href="/_next/static/css/4ac7ed34d61ef456.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-d38be8d96a62f950.js" defer=""></script><script src="/_next/static/chunks/framework-3b5a00d5d7e8d93b.js" defer=""></script><script src="/_next/static/chunks/main-9f90a364d66949ce.js" defer=""></script><script src="/_next/static/chunks/pages/_app-8cfbd06f8ddd0b49.js" defer=""></script><script src="/_next/static/chunks/675-262430aa11afdf01.js" defer=""></script><script src="/_next/static/chunks/9-9d15d34c7affe00b.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-ba4a3f289f8ef3c5.js" defer=""></script><script src="/_next/static/GAVvi42gDEHMzYTCLyMnv/_buildManifest.js" defer=""></script><script src="/_next/static/GAVvi42gDEHMzYTCLyMnv/_ssgManifest.js" defer=""></script><style data-styled="" data-styled-version="5.3.6">.iBQlkL{background:coral;height:60px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media (max-width:920px){.iBQlkL ul{display:none;}}/*!sc*/
data-styled.g1[id="sc-7295e4c-0"]{content:"iBQlkL,"}/*!sc*/
.bDMZDz{display:none;font-size:22px;margin-left:auto;}/*!sc*/
@media (max-width:920px){.bDMZDz{display:block;}}/*!sc*/
data-styled.g2[id="sc-7295e4c-1"]{content:"bDMZDz,"}/*!sc*/
html,body{height:100%;margin:0;}/*!sc*/
body{background:#fff;}/*!sc*/
html{box-sizing:border-box;font-size:16px;}/*!sc*/
*,*:before,*:after{box-sizing:inherit;}/*!sc*/
body,h1,h2,h3,h4,h5,h6,p,ol,ul{margin:0;padding:0;font-weight:normal;}/*!sc*/
ol,ul{list-style:none;}/*!sc*/
img,video{max-width:100%;height:auto;}/*!sc*/
.lg-container .lg-backdrop.in{opacity:0.75;}/*!sc*/
.lg-container .lg-toolbar.lg-group,.lg-container .lg-outer .lg-thumb-outer{background:rgba(0,0,0,0.45);}/*!sc*/
data-styled.g3[id="sc-global-cqyaiF1"]{content:"sc-global-cqyaiF1,"}/*!sc*/
.cGDPDa{max-width:800px;margin:0 auto;}/*!sc*/
data-styled.g6[id="sc-f0e62130-0"]{content:"cGDPDa,"}/*!sc*/
</style></head><body><div id="__next"><nav class="sc-7295e4c-0 iBQlkL"><ul><a href="/blog">Blog</a></ul><button class="sc-7295e4c-1 bDMZDz">X</button></nav><main><div class="sc-f0e62130-0 cGDPDa"><h1>Computing joint entropy (in Python)</h1><div class="lg-react-element "><p>How I wrote a beautiful, general, and super fast joint entropy method (in Python).</p>
<pre><code>    def entropy(*X):
    return = np.sum(-p * np.log2(p) if p &gt; 0 else 0 for p in
        (np.mean(reduce(np.logical_and, (predictions == c for predictions, c in zip(X, classes))))
            for classes in itertools.product(*[set(x) for x in X])))
</code></pre>
<p>I started with the method to compute the entropy of a single variable. Input is a numpy array with discrete values (either integers or strings).</p>
<pre><code>    import numpy as np

    def entropy(X):
        probs = [np.mean(X == c) for c in set(X)]
        return np.sum(-p * np.log2(p) for p in probs)
</code></pre>
<p>In my next version I extended it to compute the joint entropy of two variables:</p>
<pre><code>    def entropy(X, Y):
    probs = []
    for c1 in set(X):
        for c2 in set(Y):
            probs.append(np.mean(np.logical_and(X == c1, Y == c2)))

    return np.sum(-p * np.log2(p) for p in probs)
</code></pre>
<p>Now wait a minute, it looks like we have a recursion here. I couldn&#x27;t stop myself of writing en extended general function to compute the joint entropy of n variables.</p>
<pre><code>    def entropy(*X, **kwargs):
        predictions = parse_arg(X[0])
        H = kwargs[&quot;H&quot;] if &quot;H&quot; in kwargs else 0
        v = kwargs[&quot;v&quot;] if &quot;v&quot; in kwargs else np.array([True] * len(predictions))

        for c in set(predictions):
            if len(X) &gt; 1:
                H = entropy(*X[1:], v=np.logical_and(v, predictions == c), H=H)
            else:
                p = np.mean(np.logical_and(v, predictions == c))
                H += -p * np.log2(p) if p &gt; 0 else 0
        return H
</code></pre>
<p>It was the ugliest recursive function I&#x27;ve ever written. I couldn&#x27;t stop coding, I was hooked. Besides, this method was slow as hell and I need a faster version for my reasearch. I need my data tommorow, not next month. I googled if Python has something that would help me deal with the recursive part. I fould this great method: itertools.product, I&#x27;s just what we need. It takes lists and returns a cartesian product of their values. It&#x27;s the &quot;nested for loops&quot; in one function.</p>
<pre><code>    def entropy(*X):
        n_insctances = len(X[0])
        H = 0
        for classes in itertools.product(*[set(x) for x in X]):
            v = np.array([True] * n_insctances)
            for predictions, c in zip(X, classes):
                v = np.logical_and(v, predictions == c)
            p = np.mean(v)
            H += -p * np.log2(p) if p &gt; 0 else 0
        return H
</code></pre>
<p>No resursion, but still slow. It&#x27;s time to rewrite loops to the Python-like style. As a sharp eye has already noticed, the second for loop with the np.logical_and inside is perfect for the reduce method.</p>
<pre><code>    def entropy(*X):
        n_insctances = len(X[0])
        H = 0
        for classes in itertools.product(*[set(x) for x in X]):
            v = reduce(np.logical_and, (predictions, c for predictions, c in zip(X, classes)))
            p = np.mean(v)
            H += -p * np.log2(p) if p &gt; 0 else 0
        return H
</code></pre>
<p>Now, we have to remove just one more list comprehension and we have a beautiful, general, and super fast joint etropy method.</p>
<pre><code>    def entropy(*X):
        return = np.sum(-p * np.log2(p) if p &gt; 0 else 0 for p in
            (np.mean(reduce(np.logical_and, (predictions == c for predictions, c in zip(X, classes))))
                for classes in itertools.product(*[set(x) for x in X])))
</code></pre></div></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"author":"BIOLAB","date":"2012-06-15 13:21:00+00:00","draft":false,"title":"Computing joint entropy (in Python)","type":"blog","blog":["orange3","python"]},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    pre: \"pre\",\n    code: \"code\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"How I wrote a beautiful, general, and super fast joint entropy method (in Python).\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        children: \"    def entropy(*X):\\n    return = np.sum(-p * np.log2(p) if p \u003e 0 else 0 for p in\\n        (np.mean(reduce(np.logical_and, (predictions == c for predictions, c in zip(X, classes))))\\n            for classes in itertools.product(*[set(x) for x in X])))\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I started with the method to compute the entropy of a single variable. Input is a numpy array with discrete values (either integers or strings).\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        children: \"    import numpy as np\\n\\n    def entropy(X):\\n        probs = [np.mean(X == c) for c in set(X)]\\n        return np.sum(-p * np.log2(p) for p in probs)\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"In my next version I extended it to compute the joint entropy of two variables:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        children: \"    def entropy(X, Y):\\n    probs = []\\n    for c1 in set(X):\\n        for c2 in set(Y):\\n            probs.append(np.mean(np.logical_and(X == c1, Y == c2)))\\n\\n    return np.sum(-p * np.log2(p) for p in probs)\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now wait a minute, it looks like we have a recursion here. I couldn't stop myself of writing en extended general function to compute the joint entropy of n variables.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        children: \"    def entropy(*X, **kwargs):\\n        predictions = parse_arg(X[0])\\n        H = kwargs[\\\"H\\\"] if \\\"H\\\" in kwargs else 0\\n        v = kwargs[\\\"v\\\"] if \\\"v\\\" in kwargs else np.array([True] * len(predictions))\\n\\n        for c in set(predictions):\\n            if len(X) \u003e 1:\\n                H = entropy(*X[1:], v=np.logical_and(v, predictions == c), H=H)\\n            else:\\n                p = np.mean(np.logical_and(v, predictions == c))\\n                H += -p * np.log2(p) if p \u003e 0 else 0\\n        return H\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"It was the ugliest recursive function I've ever written. I couldn't stop coding, I was hooked. Besides, this method was slow as hell and I need a faster version for my reasearch. I need my data tommorow, not next month. I googled if Python has something that would help me deal with the recursive part. I fould this great method: itertools.product, I's just what we need. It takes lists and returns a cartesian product of their values. It's the \\\"nested for loops\\\" in one function.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        children: \"    def entropy(*X):\\n        n_insctances = len(X[0])\\n        H = 0\\n        for classes in itertools.product(*[set(x) for x in X]):\\n            v = np.array([True] * n_insctances)\\n            for predictions, c in zip(X, classes):\\n                v = np.logical_and(v, predictions == c)\\n            p = np.mean(v)\\n            H += -p * np.log2(p) if p \u003e 0 else 0\\n        return H\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"No resursion, but still slow. It's time to rewrite loops to the Python-like style. As a sharp eye has already noticed, the second for loop with the np.logical_and inside is perfect for the reduce method.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        children: \"    def entropy(*X):\\n        n_insctances = len(X[0])\\n        H = 0\\n        for classes in itertools.product(*[set(x) for x in X]):\\n            v = reduce(np.logical_and, (predictions, c for predictions, c in zip(X, classes)))\\n            p = np.mean(v)\\n            H += -p * np.log2(p) if p \u003e 0 else 0\\n        return H\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now, we have to remove just one more list comprehension and we have a beautiful, general, and super fast joint etropy method.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        children: \"    def entropy(*X):\\n        return = np.sum(-p * np.log2(p) if p \u003e 0 else 0 for p in\\n            (np.mean(reduce(np.logical_and, (predictions == c for predictions, c in zip(X, classes))))\\n                for classes in itertools.product(*[set(x) for x in X])))\\n\"\n      })\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"computing-joint-entropy-in-python"},"buildId":"GAVvi42gDEHMzYTCLyMnv","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>