{"pageProps":{"frontmatter":{"author":"Ajda Pretnar Å½agar","date":"2024-05-30","draft":false,"title":"Document embeddings vs. Bag of words","url":"embedding-vs-bow","thumbImage":"embed.png","frontPageImage":"embed.png","blog":["embedding","text mining","bag of words"],"shortExcerpt":"When to use document embeddings instead of a bag of words.","longExcerpt":"In the new text mining video, we present document embeddings, a popular text vectorisation technique. We describe when to use document embeddings in favour of a bag of words."},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    a: \"a\",\n    h3: \"h3\"\n  }, _provideComponents(), props.components), {WindowScreenshot} = _components;\n  if (!WindowScreenshot) _missingMdxReference(\"WindowScreenshot\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.p, {\n      children: [\"A \", _jsx(_components.a, {\n        href: \"https://youtu.be/QQqaWZEdE58?si=REvx2DN6FvwmSbZL\",\n        children: \"new video\"\n      }), \" in our Text Mining series describes document embeddings, a text vectorisation technique that captures the semantic meaning of words. Let us see how document embedding differs from a bag of words approach.\"]\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"1. Context\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Document embedding considers context, particularly the SBERT model. For example, a bag of words approach cannot distinguish between the word \\\"bear\\\" in the following sentence: \\\"A bear struggles to bear its weight.\\\" We might solve this in a bag of words by adding part-of-speech tags to the two words, i.e. bear_NOUN and bear_VERB. How about this one: \\\"The orange was orange.\\\" The first \\\"orange\\\" refers to a fruit, while the second orange refers to a colour. Document embedding can consider context and distinguish between words with the same spelling (polysemy). Bag of words also does not consider word order, while document embedding does.\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2024-05-document-embedding/embeddings.png\",\n      width: \"1469\",\n      height: \"1178\",\n      src: \"/blog/2024-05-document-embedding/__optimized-images__/embeddings.png\"\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"2. Fixed vector size\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"A bag of words can get prohibitively big when the vocabulary is large since each word is a feature. We can solve this by lemmatisation and filtering, but we might lose relevant details. Document embedding provides vectors of fixed length, making downstream analyses quicker.\"\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"3. Preprocessing\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Preprocessing is key to control the vector size and to limit the noise in the data. Preprocessing before document embedding can be beneficial, but it isn't strictly necessary. In Orange, where we use pre-trained embedders, preprocessing is less important than if the embedding is trained from scratch.\"\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"4. Out-of-vocabulary words\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"A bag of words handles out-of-vocabulary words by constructing a column with the new words. However, since the word doesn't occur in the training data, the model won't consider it. On the other hand, embeddings typically rely on the subword information, which can infer information from out-of-vocabulary (OOV) words. FastText, for example, uses character n-grams to compute word embeddings, while SBERT uses WordPiece information. Both handle OOV words.\"\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"5. Interpretability\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The biggest flaw of document embeddings is that they are not interpretable, while the bag of words features are. If interpretability is important, bag of words is the right approach. Document embeddings can become interpretable in Orange by using t-SNE for document maps and adding an Annotated Corpus Map to find keywords in document clusters.\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2024-05-document-embedding/workflow.png\",\n      width: \"1060\",\n      height: \"244\",\n      src: \"/blog/2024-05-document-embedding/__optimized-images__/workflow.png\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{}},"thumbImage":{"width":600,"height":400,"src":"/blog/2024-05-document-embedding/__optimized-images__/embed.png"}},"__N_SSG":true}