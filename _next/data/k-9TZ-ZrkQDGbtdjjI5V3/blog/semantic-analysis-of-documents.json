{"pageProps":{"frontmatter":{"author":"Ajda Pretnar","date":"2021-09-17","draft":false,"title":"Semantic Analysis of Documents","type":"blog","thumbImage":"2021-09-17-seman.png","frontPageImage":"2021-09-17-seman.png","blog":["semantic analysis","text mining","corpus","keywords"],"shortExcerpt":"How to use Text add-on for semantic analysis of documents.","longExcerpt":"How to use Text add-on to extract keywords from documents, score documents on keywords, and display semantic content in a map.","x2images":true},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    a: \"a\",\n    code: \"code\",\n    strong: \"strong\",\n    em: \"em\"\n  }, _provideComponents(), props.components), {WindowScreenshot} = _components;\n  if (!WindowScreenshot) _missingMdxReference(\"WindowScreenshot\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.p, {\n      children: [\"Our recent project with the Ministry of Public Administration comprises building a \", _jsx(_components.a, {\n        href: \"https://nio.gov.si/nio/asset/semanticni+analizator+besedil?lang=en\",\n        children: \"semantic analysis pipeline in Orange\"\n      }), \", enabling the users to quickly and efficiently explore the content of documents, compare a subset against the corpus, extract keywords, and semantically explore document maps. If this sounds too vague, don't worry, here's a quick demo on how to perform semantic analysis in Orange.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"First, we will use the pre-prepared corpus of \", _jsx(_components.a, {\n        href: \"https://predlagam.vladi.si/\",\n        children: \"proposals to the government\"\n      }), \", which you can download \", _jsx(_components.a, {\n        href: \"http://file.biolab.si/text-semantics/data/predlogi-vladi-1k.tab\",\n        children: \"here\"\n      }), \". These are the initiatives which the citizens of Slovenia propose to the current government for consideration. The present corpus contains 1093 such proposals.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-wf1.png\",\n      width: \"256\",\n      height: \"113\",\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-wf1.png\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-corpus.png\",\n      width: \"605\",\n      height: \"431\",\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-corpus.png\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Each proposal contains a title, the content of the proposal, the author, the date when it was published, number of upvotes, and so on. But for a thousand proposals, it would take a long time to read all of them and see which policy areas they cover. Instead, we will use two new Orange widgets to determine the content (main keywords) of a subset of documents.\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-corpus-viewer.png\",\n      width: \"1207\",\n      height: \"628\",\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-corpus-viewer.png\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"As always, we will first preprocess the corpus to create tokens, the core units of our analysis. The preprocessing pipeline is sequential; first, we lowercase the text, then we split the text into words (this is what the regular expression \", _jsx(_components.code, {\n        children: \"\\\\w+.\"\n      }), \" does), transform the words into lemmas with UDPipe, and finally remove stopwords from the list (such as \\\"in\\\", \\\"da\\\", \\\"če\\\"). Since we are working with a Slovenian language text, we have to select the corresponding models and stopword lists.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-wf2.png\",\n      width: \"254\",\n      height: \"114\",\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-wf2.png\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-preprocess-text.png\",\n      width: \"984\",\n      height: \"1030\",\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-preprocess-text.png\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"After preprocessing, we build a document-term matrix using the \", _jsx(_components.strong, {\n        children: \"Bag of Words\"\n      }), \" widget with the Count+IDF setting. Next, we pass the data to \", _jsx(_components.strong, {\n        children: \"t-SNE\"\n      }), \" to observe the document map. t-SNE takes the document-term matrix and finds a 2D projection, where similar documents lie close together.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-wf3.png\",\n      width: \"282\",\n      height: \"112\",\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-wf3.png\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-tsne1.png\",\n      width: \"2240\",\n      height: \"1456\",\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-tsne1.png\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now we will select a small subset of the document, say in the lower right corner, where we have a nice cluster of points. The question is, what are these documents talking about?\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"We will use \", _jsx(_components.strong, {\n        children: \"Extract Keywords\"\n      }), \" widget to find the most significant keywords in the selection. There are several methods we can use, even the popuar \", _jsx(_components.a, {\n        href: \"https://repositorio.inesctec.pt/bitstream/123456789/7623/1/P-00N-NF5.pdf\",\n        children: \"YAKE!\"\n      }), \", but we will go with a simple TF-IDF method, which takes the words with the highest TF-IDF score. Note that the vectorizer uses the default sklearn's \", _jsx(_components.a, {\n        href: \"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\",\n        children: \"TfidfVectorizer\"\n      }), \" settings, that is the tf-idf transform with L2 norm, keeping the passed tokens as they are.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-wf4.png\",\n      width: \"288\",\n      height: \"118\",\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-wf4.png\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-extract-keywords.png\",\n      width: \"643\",\n      height: \"548\",\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-extract-keywords.png\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"It looks like the top words characterizing the selected subcorpus are \\\"študent\\\" (\", _jsx(_components.em, {\n        children: \"student\"\n      }), \"), \\\"delati\\\" (\", _jsx(_components.em, {\n        children: \"to work\"\n      }), \"), and \\\"delo\\\" (\", _jsx(_components.em, {\n        children: \"the work\"\n      }), \"). Apparently, the documents talk mostly about student work. Let us explore this a bit further. It would be nice to have a certain score attached to the documents, which would correspond to how much a document talks about student work. In other words, we would like to score the documents based on how many of the selected words they contain (and in what proportion).\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"To achieve this, we will use \", _jsx(_components.strong, {\n        children: \"Score Documents\"\n      }), \". We will pass it the document-term matrix and the list of selected keywords from \", _jsx(_components.strong, {\n        children: \"Extract Keywords\"\n      }), \". The widget again offers several different ways of scoring documents. A simple way to score them is to compute how often selected words appear in each document, which corresponds to the \\\"Word Count\\\" method.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-wf5.png\",\n      width: \"566\",\n      height: \"218\",\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-wf5.png\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-score-documents.png\",\n      width: \"1007\",\n      height: \"634\",\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-score-documents.png\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [_jsx(_components.strong, {\n        children: \"Score Documents\"\n      }), \" returns keyword scores for each document. Let us pass the scored documents to another \", _jsx(_components.strong, {\n        children: \"t-SNE\"\n      }), \" widget. If we set the color and the size of the points to \\\"Word Count\\\" variable, t-SNE plot will expose the documents with the highest scores. These documents talk the most about students and work. A great thing is that we can see documents with high scores that were not a part of our selection, which means the general bottom-right area contains documents relating to this topic.\"]\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-tsne2.png\",\n      width: \"1832\",\n      height: \"1160\",\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-tsne2.png\"\n    }), \"\\n\", _jsx(WindowScreenshot, {\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-workflow.png\",\n      width: \"1734\",\n      height: \"562\",\n      src: \"/blog/2021-09-semantic-analysis/2021-09-17-workflow.png\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now try selecting a different subset yourself and see what the documents are about. You can use any corpus you want, even the ones that come with Orange.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{}}},"__N_SSG":true}