{"pageProps":{"frontmatter":{"author":"AJDA","date":"2018-10-05 10:36:13+00:00","draft":false,"title":"Orange Now Speaks 50 Languages","type":"blog","blog":["preprocessing","text mining"],"shortExcerpt":"In the past couple of weeks we have been working hard on introducing a better language support for the Text add-on. Until recently, Orange supported only a limited number of languages, mostly English and some bigger languages, such as Spanish, German, Arabic, Russian... "},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    strong: \"strong\",\n    a: \"a\",\n    em: \"em\",\n    img: \"img\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"In the past couple of weeks we have been working hard on introducing a better language support for the Text add-on. Until recently, Orange supported only a limited number of languages, mostly English and some bigger languages, such as Spanish, German, Arabic, Russian... Language support was most evident in the list of stopwords, normalization and POS tagging.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [_jsx(_components.strong, {\n        children: \"Related:\"\n      }), \" \", _jsx(_components.a, {\n        href: \"/blog/2018/09/11/text-workshops-in-ljubljana/\",\n        children: \"Text Workshops in Ljubljana\"\n      })]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Stopwords come from \", _jsx(_components.a, {\n        href: \"https://www.nltk.org/\",\n        children: \"NLTK library\"\n      }), \", so we can only offer whatever is available there. However, \", _jsx(_components.a, {\n        href: \"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\",\n        children: \"TF-IDF\"\n      }), \" already implicitly considers stopwords, so the functionality is already implemented. For POS tagging, we would rely on Stanford POS tagger, that already has pre-trained models available.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"The main issue was with normalization. While English can do without lemmatization and stemming for simple tasks, morphologically rich languages, such as Slovenian, perform poorly on un-normalized tokens. Cases and declensions present a problem for natural language processing, so we wanted to provide a tool for normalization in many different languages. Luckily, we found \", _jsx(_components.a, {\n        href: \"http://ufal.mff.cuni.cz/udpipe\",\n        children: \"UDPipe\"\n      }), \", a Czech initiative that offers trained lemmatization models for 50 languages. UDPipe is actually a preprocessing pipeline and we are already thinking about how to bring all of its functionality to Orange, but let us talk a bit about the recent improvements for normalization.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Let us load a simple corpus from Corpus widget, say \", _jsx(_components.em, {\n        children: \"grimm-tales-selected.tab\"\n      }), \" that contain 44 tales from the Grimm Brothers. Now, pass them through Preprocess Text and keep just the defaults, namely lowercase transformation, tokenization by words, and removal of stopwords. Here we see that we have \", _jsx(_components.em, {\n        children: \"came\"\n      }), \" as quite a frequent word and \", _jsx(_components.em, {\n        children: \"come\"\n      }), \" as a bit less frequent. But semantically, they are the same word from the verb \", _jsx(_components.em, {\n        children: \"to come\"\n      }), \". Shouldn't we consider them as one word?\"]\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/blog/2018-10-orange-now-speaks-50-languages/Screen-Shot-2018-10-04-at-13.28.50.webp\",\n      alt: \"\",\n      width: \"1600\",\n      height: \"911\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"We can. This is what normalization does - it transforms all words into their lemmas or basic grammatical form. \", _jsx(_components.em, {\n        children: \"Came\"\n      }), \" and \", _jsx(_components.em, {\n        children: \"come\"\n      }), \" will become \", _jsx(_components.em, {\n        children: \"come\"\n      }), \", \", _jsx(_components.em, {\n        children: \"sons\"\n      }), \" and \", _jsx(_components.em, {\n        children: \"son\"\n      }), \" will become \", _jsx(_components.em, {\n        children: \"son\"\n      }), \", \", _jsx(_components.em, {\n        children: \"pretty\"\n      }), \" and \", _jsx(_components.em, {\n        children: \"prettier\"\n      }), \" will become \", _jsx(_components.em, {\n        children: \"pretty\"\n      }), \". This will result in less tokens that capture the text better, semantically speaking.\"]\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/blog/2018-10-orange-now-speaks-50-languages/Screen-Shot-2018-10-04-at-13.27.03.webp\",\n      alt: \"\",\n      width: \"1600\",\n      height: \"1156\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"We can see that \", _jsx(_components.em, {\n        children: \"came\"\n      }), \" became \", _jsx(_components.em, {\n        children: \"come\"\n      }), \" with 435 counts. \", _jsx(_components.em, {\n        children: \"Went\"\n      }), \" became \", _jsx(_components.em, {\n        children: \"go\"\n      }), \". \", _jsx(_components.em, {\n        children: \"Said\"\n      }), \" became \", _jsx(_components.em, {\n        children: \"say\"\n      }), \". And so on. As we said, this doesn't work only on verbs, but on all word forms.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"One thing to note here. UDPipe has an internal tokenizer, that works with sentences instead of tokens. You can enable it by selecting UDPipe tokenizer option. What is the difference? A quicker version would be to tokenize all the words and just look up their lemma. But sometimes this can be wrong. Consider the sentence:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"I am wearing a tie to work.\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Now the word \", _jsx(_components.em, {\n        children: \"tie\"\n      }), \" is obviously a piece of clothing, which is indicated by the word wearing before it. But \", _jsx(_components.em, {\n        children: \"tie\"\n      }), \" alone can also be the verb \", _jsx(_components.em, {\n        children: \"to tie\"\n      }), \". So the UDPipe tokenizer will consider the entire sentence and correctly lemmatize this word, while lemmatization on regular tokens might not. While UDPipe works better, it is also slower, so you might want to work with regular tokenization to speed up the analysis.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [_jsx(_components.img, {\n        src: \"/blog/2018-10-orange-now-speaks-50-languages/Screen-Shot-2018-10-04-at-13.42.25.webp\",\n        alt: \"\",\n        width: \"1600\",\n        height: \"1366\"\n      }), \"\\nIn Preprocess Text, you turn on the Normalization button on the right, then select UDPipe Lemmatizer and select the language you wish to use. Finally, if you wish to go with the better albeit slower UDPipe tokenizer, tick the UDPipe tokenizer box.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Finally, UDPipe does not remove punctuation, so you might end up with words like \", _jsx(_components.em, {\n        children: \"rose.\"\n      }), \" and \", _jsx(_components.em, {\n        children: \"away.\"\n      }), \", with the full stop at the end. This you can fix with using regular tokenization and also by select the Regex option in Filtering, which will remove pure punctuation.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [_jsx(_components.img, {\n        src: \"/blog/2018-10-orange-now-speaks-50-languages/Screen-Shot-2018-10-04-at-13.26.31.webp\",\n        alt: \"\",\n        width: \"838\",\n        height: \"444\"\n      }), \"\\nFinal workflow, where we compared the results of no normalization and UDPipe normalization in a word cloud.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This is it. UDPipe contains lemmatization models for 50 languages and only when you click on a particular language in the Language option, will the resource be loaded, so your computer won't be flooded with models for languages you won't ever use. The installation of UDPipe could also be a little tricky, but after some initial obstacles, we have managed to prepare packages for both pip (OSX and Linux) and conda (Windows).\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We hope you enjoy the new possibilities of a freshly multilingual Orange!\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true}