{"pageProps":{"frontmatter":{"author":"BLAZ","date":"2017-08-11 08:59:54+00:00","draft":false,"title":"It's Sailing Time (Again)","blog":["classification","tree"]},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    a: \"a\",\n    strong: \"strong\",\n    img: \"img\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.p, {\n      children: [\"Every fall I teach a course on Introduction to Data Mining. And while the course is really on statistical learning and its applications, I also venture into \", _jsx(_components.a, {\n        href: \"https://en.wikipedia.org/wiki/Decision_tree_learning\",\n        children: \"classification trees\"\n      }), \". For several reasons. First, I can introduce \", _jsx(_components.a, {\n        href: \"https://en.wikipedia.org/wiki/Information_gain_ratio\",\n        children: \"information gain\"\n      }), \" and with it feature scoring and ranking. Second, classification trees are one of the first machine learning approaches co-invented by engineers (\", _jsx(_components.a, {\n        href: \"https://en.wikipedia.org/wiki/Ross_Quinlan\",\n        children: \"Ross Quinlan\"\n      }), \") and statisticians (\", _jsx(_components.a, {\n        href: \"https://www.amazon.com/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418/ref=sr_1_1?ie=UTF8&qid=1501848607&sr=8-1&keywords=classification+and+regression+trees\",\n        children: \"Leo Breiman, Jerome Friedman, Charles J. Stone, Richard A. Olshen\"\n      }), \"). And finally, because they make the base of random forests, one of the most accurate machine learning models for smaller and mid-size data sets.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [_jsx(_components.strong, {\n        children: \"Related:\"\n      }), \" \", _jsx(_components.a, {\n        href: \"/blog/2016/09/15/data-mining-in-houston-2/\",\n        children: \"Introduction to Data Mining Course in Houston\"\n      })]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Lecture on classification trees has to start with the data. Years back I have crafted a data set on sailing. Every data set has to have a story. Here is one:\"\n    }), \"\\n\", _jsx(\"blockquote\", {\n      children: \"Sara likes weekend sailing. Though, not under any condition. Past\\ntwenty Wednesdays I have asked her if she will have any company, what\\nkind of boat she can rent, and I have checked the weather\\nforecast. Then, on Saturday, I wrote down if she actually went to the Sea.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Data on Sara's sailing contains three attributes (Outlook, Company, Sailboat) and a class (Sail).\"\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/blog/2017-08-its-sailing-time-again/__webp-images__/sailing-data.webp\",\n      alt: \"\",\n      width: \"453\",\n      height: \"481\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"The data comes with \", _jsx(_components.a, {\n        href: \"http://orange.biolab.si\",\n        children: \"Orange\"\n      }), \" and you can get them from Data Sets widget (currently in Prototypes Add-On, but soon to be moved to core Orange). It takes time, usually two lecture hours, to go through probabilities, entropy and information gain, but at the end, the data analysis workflow we develop with students looks something like this:\"]\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/blog/2017-08-its-sailing-time-again/__webp-images__/sailing-tree-model.webp\",\n      alt: \"\",\n      width: \"428\",\n      height: \"270\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"And here is the classification tree:\"\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/blog/2017-08-its-sailing-time-again/__webp-images__/sailing-classification-tree.webp\",\n      alt: \"\",\n      width: \"705\",\n      height: \"471\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Turns out that Sara is a social person. When the company is big, she goes sailing no matter what. When the company is smaller, she would not go sailing if the weather is bad. But when it is sunny, sailing is fun, even when being alone.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [_jsx(_components.strong, {\n        children: \"Related:\"\n      }), \" \", _jsx(_components.a, {\n        href: \"/blog/2016/07/29/pythagorean-trees-and-forests/\",\n        children: \"Pythagorean Trees and Forests\"\n      })]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Classification trees are not very stable classifiers. Even with small changes in the data, the trees can change substantially. This is an important concept that leads to the use of ensembles like random forests. It is also here, during my lecture, that I need to demonstrate this instability. I use Data Sampler and show a classification tree under the current sampling. Pressing on Sample Data button the tree changes every time. The workflow I use is below, but if you really want to see this in action, well, try it in Orange.\"\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/blog/2017-08-its-sailing-time-again/__webp-images__/sailing-sampled-trees.webp\",\n      alt: \"\",\n      width: \"1005\",\n      height: \"716\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true}