{"pageProps":{"frontmatter":{"author":"BIOLAB","date":"2012-06-15 13:21:00+00:00","draft":false,"title":"Computing joint entropy (in Python)","blog":["orange3","python"],"oldUrl":"/blog/2012/06/15/computing-joint-entropy-in-python/"},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    pre: \"pre\",\n    code: \"code\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"How I wrote a beautiful, general, and super fast joint entropy method (in Python).\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        children: \"    def entropy(*X):\\n    return = np.sum(-p * np.log2(p) if p > 0 else 0 for p in\\n        (np.mean(reduce(np.logical_and, (predictions == c for predictions, c in zip(X, classes))))\\n            for classes in itertools.product(*[set(x) for x in X])))\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I started with the method to compute the entropy of a single variable. Input is a numpy array with discrete values (either integers or strings).\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        children: \"    import numpy as np\\n\\n    def entropy(X):\\n        probs = [np.mean(X == c) for c in set(X)]\\n        return np.sum(-p * np.log2(p) for p in probs)\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"In my next version I extended it to compute the joint entropy of two variables:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        children: \"    def entropy(X, Y):\\n    probs = []\\n    for c1 in set(X):\\n        for c2 in set(Y):\\n            probs.append(np.mean(np.logical_and(X == c1, Y == c2)))\\n\\n    return np.sum(-p * np.log2(p) for p in probs)\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now wait a minute, it looks like we have a recursion here. I couldn't stop myself of writing en extended general function to compute the joint entropy of n variables.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        children: \"    def entropy(*X, **kwargs):\\n        predictions = parse_arg(X[0])\\n        H = kwargs[\\\"H\\\"] if \\\"H\\\" in kwargs else 0\\n        v = kwargs[\\\"v\\\"] if \\\"v\\\" in kwargs else np.array([True] * len(predictions))\\n\\n        for c in set(predictions):\\n            if len(X) > 1:\\n                H = entropy(*X[1:], v=np.logical_and(v, predictions == c), H=H)\\n            else:\\n                p = np.mean(np.logical_and(v, predictions == c))\\n                H += -p * np.log2(p) if p > 0 else 0\\n        return H\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"It was the ugliest recursive function I've ever written. I couldn't stop coding, I was hooked. Besides, this method was slow as hell and I need a faster version for my reasearch. I need my data tommorow, not next month. I googled if Python has something that would help me deal with the recursive part. I fould this great method: itertools.product, I's just what we need. It takes lists and returns a cartesian product of their values. It's the \\\"nested for loops\\\" in one function.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        children: \"    def entropy(*X):\\n        n_insctances = len(X[0])\\n        H = 0\\n        for classes in itertools.product(*[set(x) for x in X]):\\n            v = np.array([True] * n_insctances)\\n            for predictions, c in zip(X, classes):\\n                v = np.logical_and(v, predictions == c)\\n            p = np.mean(v)\\n            H += -p * np.log2(p) if p > 0 else 0\\n        return H\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"No resursion, but still slow. It's time to rewrite loops to the Python-like style. As a sharp eye has already noticed, the second for loop with the np.logical_and inside is perfect for the reduce method.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        children: \"    def entropy(*X):\\n        n_insctances = len(X[0])\\n        H = 0\\n        for classes in itertools.product(*[set(x) for x in X]):\\n            v = reduce(np.logical_and, (predictions, c for predictions, c in zip(X, classes)))\\n            p = np.mean(v)\\n            H += -p * np.log2(p) if p > 0 else 0\\n        return H\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now, we have to remove just one more list comprehension and we have a beautiful, general, and super fast joint etropy method.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        children: \"    def entropy(*X):\\n        return = np.sum(-p * np.log2(p) if p > 0 else 0 for p in\\n            (np.mean(reduce(np.logical_and, (predictions == c for predictions, c in zip(X, classes))))\\n                for classes in itertools.product(*[set(x) for x in X])))\\n\"\n      })\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true}